<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CDH中禁用kerberos</title>
    <url>/2020/05/28/cdh-zhong-jin-yong-kerberos/</url>
    <content><![CDATA[<p>免费版的CDH关闭kerberos会相对复杂。</p>
<ol>
<li>关闭所有组件。</li>
</ol>
<h2 id="一、操作zookeeper"><a href="#一、操作zookeeper" class="headerlink" title="一、操作zookeeper"></a>一、操作zookeeper</h2><ol>
<li>关闭zk的认证配置：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095701.png"></p>
<ol start="2">
<li>找到zk的数据目录：<br>[][1]</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528094904.png"></p>
<ol start="3">
<li>删除目录下的内容：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095524.png"></p>
<ol start="4">
<li>初始化zk：</li>
</ol>
<img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095741.png" style="zoom:50%;" />

<ol start="5">
<li>确认初始化成功：</li>
</ol>
<h2 id="HDFS篇"><a href="#HDFS篇" class="headerlink" title="HDFS篇"></a>HDFS篇</h2><ol>
<li><p>关闭认证相关配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop.security.authentication</span><br><span class="line">hadoop.security.authorization</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100134.png"></p>
</li>
<li><p>修改权限及其他配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dfs.datanode.data.dir.perm =&gt; 755</span><br><span class="line">dfs.datanode.address =&gt; 50010</span><br><span class="line">dfs.datanode.http.address =&gt; 50070</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100400.png"></p>
</li>
</ol>
<h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><ol>
<li>搜索auth选项，关闭即可。一般来说还需要关HBase</li>
</ol>
<h2 id="重启、核查："><a href="#重启、核查：" class="headerlink" title="重启、核查："></a>重启、核查：</h2><p>重启，</p>
<ol>
<li>核查可发现kerberos已关闭：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101020.png"></p>
<ol start="2">
<li><p>核查hdfs上数据，发现都在：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101222.png"></p>
</li>
<li><p>核查zk数据，kafka数据，一切正常：</p>
<p>![image-20200528101328031](/Users/liushengwei/Library/Application Support/typora-user-images/image-20200528101328031.png)</p>
</li>
</ol>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink1.13中的SQL优化</title>
    <url>/2021/07/15/flink1-13-zhong-de-sql-you-hua/</url>
    <content><![CDATA[<h2 id="通过-Table-valued-函数来定义时间窗口"><a href="#通过-Table-valued-函数来定义时间窗口" class="headerlink" title="通过 Table-valued 函数来定义时间窗口"></a>通过 Table-valued 函数来定义时间窗口</h2><h3 id="新引入的CUMLATE函数"><a href="#新引入的CUMLATE函数" class="headerlink" title="新引入的CUMLATE函数"></a>新引入的CUMLATE函数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> window_time, window_start, window_end, <span class="built_in">SUM</span>(price) <span class="keyword">AS</span> total_price</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(CUMULATE(<span class="keyword">TABLE</span> Bid, DESCRIPTOR(bidtime), <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> MINUTES, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">MINUTES))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end, window_time;</span><br></pre></td></tr></table></figure>

<ul>
<li>通过CUMULATE构建一个表。需要4个参数<ul>
<li>Table名。</li>
<li></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink中使用cep</title>
    <url>/2020/05/18/flink-zhong-shi-yong-cep/</url>
    <content><![CDATA[<h1 id="Flink中使用CEP进行复杂计算"><a href="#Flink中使用CEP进行复杂计算" class="headerlink" title="Flink中使用CEP进行复杂计算"></a>Flink中使用CEP进行复杂计算</h1><h2 id="CEP简介："><a href="#CEP简介：" class="headerlink" title="CEP简介："></a>CEP简介：</h2><p>官网介绍: <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/cep.html">Complex event processing for Flink</a><br>CEP主要用于流式处理过程中的数据状态转移，我们假设一个场景，我们接受的流数据有2两个字段：（id，name），利用cep，我们就可以监控上下文。</p>
<p>即在流数据场景中，是否存在先达成状态1，后达成状态2的情景。</p>
<p>简单来说，最初我们在监控id这个字段，并希望得到id=1的数据。这个时候称之为状态1。当我们确实收到了id=1的数据时，就发生了状态转移，这时候我们开始关注name=zhangsan的数据。这称为状态2。在状态2下，我们已经不关心id这个字段是几，只有当name=zhangsan时，才会出发状态2的结束，进入状态3或者完成CEP。<br>当然这是最简单的情景，更加完善的解释可以参见官网和其他博文。</p>
<h2 id="Flink使用CEP："><a href="#Flink使用CEP：" class="headerlink" title="Flink使用CEP："></a>Flink使用CEP：</h2><h3 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h3><ol>
<li>引入依赖： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;flink-cep-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">	&lt;scope&gt;compile&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
</li>
<li>简单的CEP程序：</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[<span class="type">Event</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> pattern = <span class="type">Pattern</span>.begin[<span class="type">Event</span>](<span class="string">&quot;start&quot;</span>).where(_.getId == <span class="number">42</span>).optional.times(<span class="number">5</span>).greedy</span><br><span class="line">	.next(<span class="string">&quot;middle&quot;</span>).subtype(classOf[<span class="type">SubEvent</span>]).where(_.name.equals(<span class="string">&quot;zhangsan&quot;</span>))</span><br><span class="line">	.within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataStream</span>[<span class="type">Alert</span>] = patternStream.process(</span><br><span class="line">	<span class="keyword">new</span> <span class="type">PatternProcessFunction</span>[<span class="type">Event</span>, <span class="type">Alert</span>]() &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processMatch</span></span>(</span><br><span class="line">          `<span class="keyword">match</span>`: util.<span class="type">Map</span>[<span class="type">String</span>, util.<span class="type">List</span>[<span class="type">Event</span>]],</span><br><span class="line">          ctx: <span class="type">PatternProcessFunction</span>.<span class="type">Context</span>,</span><br><span class="line">          out: <span class="type">Collector</span>[<span class="type">Alert</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          	out.collect(createAlertFrom(pattern))</span><br><span class="line">          &#125;</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure>

<p>  pattern是CEP最重要的地方，这里定义了如何对Event进行处理。示例中，定义了第一个状态为start，匹配条件是id=42。其中：</p>
<ul>
<li>Next/followedBy表示转换成下一状态。</li>
<li>where表示状态判定</li>
<li>subtype的作用是进行类型校验。SubEvent需要时Event的子类。</li>
<li>times表示需要匹配上n次才进入下一阶段。</li>
<li>within表示需要在n时间内完成所有状态，否则是无效的。</li>
<li>optional表示这个匹配条件是可选的。即便没匹配也会进入下一阶段的匹配。</li>
<li>greedy表示贪婪，即可以无视当前状态进行匹配。greedy需要跟在times后面，表示取最后n次匹配到的。</li>
</ul>
<p>  实际计算时，我们将input输入和pattern捆绑在一起。</p>
<p>  最终，在processMatch中就是在我们完成所有状态匹配后出发，match: util.Map[String, util.List[Event]]里保存了所有匹配上的结果，其中key是state的名字，value是匹配上的值。需要注意，如果启用了optional，没匹配的情况下不会存在value中。</p>
<h3 id="定义模式"><a href="#定义模式" class="headerlink" title="定义模式"></a>定义模式</h3><ol>
<li><p>模式间的联系：</p>
<p>主要分为三种：严格连续性（next/notNext），宽松连续性（followedBy/notFollowedBy），和非确定宽松连续性（followedByAny）。</p>
<ul>
<li>严格连续性：需要消息的顺序到达与模式完全一致。</li>
<li>宽松连续性：允许忽略不匹配的事件。</li>
<li>非确定宽松连性：不仅可以忽略不匹配的事件，也可以忽略已经匹配的事件。</li>
</ul>
</li>
<li><p>模式属性：</p>
<p>正如示例中，分为循环模式times和可选属性。</p>
<p>循环属性可以定义模式匹配发生固定次数（<strong>times</strong>），匹配发生一次以上（<strong>oneOrMore</strong>），匹配发生多次以上。(<strong>timesOrMore</strong>)。</p>
<p>可选属性可以设置模式是贪婪的（<strong>greedy</strong>），即匹配最长的串，或设置为可选的（<strong>optional</strong>），有则匹配，无则忽略。</p>
</li>
<li><p>模式有效期：</p>
<p>通过within设置一个全局有效区，防止数据过度缓存。</p>
</li>
<li><p>多模式组合：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> start: <span class="type">Pattern</span>[<span class="type">Event</span>, _] = <span class="type">Pattern</span>.begin(</span><br><span class="line">    <span class="type">Pattern</span>.begin[<span class="type">Event</span>](<span class="string">&quot;start&quot;</span>).where(...).followedBy(<span class="string">&quot;start_middle&quot;</span>).where(...)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>可以将pattern如此进行复用</p>
</li>
<li><p>处理结果：</p>
<p>处理匹配的结果主要有四个接口： PatternFlatSelectFunction，PatternSelectFunction，PatternFlatTimeoutFunction和PatternTimeoutFunction。</p>
<p>输出可以分为两类：</p>
<ul>
<li>select和flatSelect指定输出一条还是多条</li>
<li>timeoutFunction和不带timeout的Function指定可不可以对超时事件进行旁路输出。</li>
<li></li>
</ul>
</li>
<li><p>关于PatternStream</p>
<p>Stream和Pattern组合会得到PatternStream，其下主要有3类方法：process，select，flatSelect。以接受函数适应不同的处理需求，所有函数会在完成匹配后触发。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink中时间服务</title>
    <url>/2021/07/25/flink-zhong-shi-jian-fu-wu/</url>
    <content><![CDATA[<p>一般情况下，在<code>KeyedProcessFunction#processElement()</code>方法中会用到Timer，注册Timer然后重写其<code>onTimer()</code>方法。在Watermark超过Timer的时间点之后，触发<code>onTimer()</code>方法。包含事件时间和处理时间两种Timer。</p>
<p>Timer在在注册过程中会使用到定时器服务<code>TimerService</code>，该服务提供了定时器的管理行为。包括定时器的删除和注册。</p>
<p>简单来讲，在算子中使用<code>TimerService</code>来创建<code>Timer</code>，并且在<code>Timer</code>触发的时候进行回调，进行相应的业务处理。</p>
<span id="more"></span>

<p>Flink在开发层面，会在<code>KeyedProcessFunction</code>和<code>Window</code>中使用到时间概念。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>一般情况下，在<code>KeyedProcessFunction#processElement()</code>方法中会用到Timer，注册Timer然后重写其<code>onTimer()</code>方法。在Watermark超过Timer的时间点之后，触发<code>onTimer()</code>方法。包含事件时间和处理时间两种Timer。</p>
<p>Timer在在注册过程中会使用到定时器服务<code>TimerService</code>，该服务提供了定时器的管理行为。包括定时器的删除和注册。</p>
<p>简单来讲，在算子中使用<code>TimerService</code>来创建<code>Timer</code>，并且在<code>Timer</code>触发的时候进行回调，进行相应的业务处理。</p>
<h1 id="定时器服务"><a href="#定时器服务" class="headerlink" title="定时器服务"></a>定时器服务</h1><p>定时器服务在Flink中叫做<code>TimerService</code>，窗口算子<code>WindowOperator</code>中使用了<code>InternalTimerService</code>来管理定时器<code>Timer</code>，其初始化是在<code>WindowOperator#open()</code>中实现的。</p>
<blockquote>
<p><code>InternalTimerService</code>并不是<code>TimerService</code>继承/实现关系。仅仅是<code>org.apache.flink.streaming.api.TimerService</code>的内部版本。两者是独立的接口。</p>
<p><code>TimerService</code>有一个实现类为<code>SimpleTimerService</code>，而**<code>SimpleTimerService</code>主要是委托给<code>InternalTimerService</code>来实现。**</p>
<p><code>InternalTimerService</code>是<code>TimerService</code>的internal版本的接口，比起<code>TimerService</code>它定义了<code>namespace</code>，</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WindowOperator.java</span></span><br><span class="line">internalTimerService = getInternalTimerService(<span class="string">&quot;window-timers&quot;</span>, windowSerializer, <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// AbstractStreamOperator.java</span></span><br><span class="line"><span class="keyword">public</span> &lt;K, N&gt; <span class="function">InternalTimerService&lt;N&gt; <span class="title">getInternalTimerService</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        String name, TypeSerializer&lt;N&gt; namespaceSerializer, Triggerable&lt;K, N&gt; triggerable)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (timeServiceManager == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;The timer service has not been initialized.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    InternalTimeServiceManager&lt;K&gt; keyedTimeServiceHandler =</span><br><span class="line">            (InternalTimeServiceManager&lt;K&gt;) timeServiceManager;</span><br><span class="line">    KeyedStateBackend&lt;K&gt; keyedStateBackend = getKeyedStateBackend();</span><br><span class="line">    checkState(keyedStateBackend != <span class="keyword">null</span>, <span class="string">&quot;Timers can only be used on keyed operators.&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> keyedTimeServiceHandler.getInternalTimerService(</span><br><span class="line">            name, keyedStateBackend.getKeySerializer(), namespaceSerializer, triggerable);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于InternalTimerService而言，有几个元素比较重要：名称、命名空间类型N（及其序列化器）、键类型K（及其序列化器）和Triggerable对象（支持延时计算的算子，继承了<code>Triggerable</code>接口来实现回调）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Registers a timer to be fired when event time watermark passes the given time. The namespace</span></span><br><span class="line"><span class="comment"> * you pass here will be provided when the timer fires.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(N namespace, <span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Deletes the timer for the given key and namespace. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(N namespace, <span class="keyword">long</span> time)</span></span>;</span><br></pre></td></tr></table></figure>

<h1 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h1><p>对于事件时间，会根据Watermark的时间，从事件时间的定时器队列中，找到比给定时间小的所有定时器，触发该Timer所在的算子，然后由算子去调用UDF中的onTime方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// InternalTimerServiceImpl.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">advanceWatermark</span><span class="params">(<span class="keyword">long</span> time)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    currentWatermark = time;</span><br><span class="line"></span><br><span class="line">    InternalTimer&lt;K, N&gt; timer;</span><br><span class="line">		</span><br><span class="line">  	<span class="comment">// 取出一个定时器，并判断其中的时间</span></span><br><span class="line">    <span class="keyword">while</span> ((timer = eventTimeTimersQueue.peek()) != <span class="keyword">null</span> &amp;&amp; timer.getTimestamp() &lt;= time) &#123;</span><br><span class="line">        eventTimeTimersQueue.poll();</span><br><span class="line">        keyContext.setCurrentKey(timer.getKey());</span><br><span class="line">        triggerTarget.onEventTime(timer);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Triggerable.java</span></span><br><span class="line"><span class="comment">/** Invoked when an event-time timer fires. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">onEventTime</span><span class="params">(InternalTimer&lt;K, N&gt; timer)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 某一个实现:KeyedCoProcessOperator.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEventTime</span><span class="params">(InternalTimer&lt;K, VoidNamespace&gt; timer)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    collector.setAbsoluteTimestamp(timer.getTimestamp());</span><br><span class="line">    onTimerContext.timeDomain = TimeDomain.EVENT_TIME;</span><br><span class="line">    onTimerContext.timer = timer;</span><br><span class="line">  	<span class="comment">// userFunction即是某一个Function</span></span><br><span class="line">    userFunction.onTimer(timer.getTimestamp(), onTimerContext, collector);</span><br><span class="line">    onTimerContext.timeDomain = <span class="keyword">null</span>;</span><br><span class="line">    onTimerContext.timer = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h1 id="优先级队列"><a href="#优先级队列" class="headerlink" title="优先级队列"></a>优先级队列</h1><p>Flink自己实现了优先级队列来管理Timer，共有两种实现：</p>
<ol>
<li>基于堆内存的优先队列<code>HeapPriorityQueueSet</code>：基于Java堆内存的优先级队列。使用二叉树实现。</li>
<li>基于RocksDB的优先级队列。</li>
</ol>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Time</tag>
        <tag>Window</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink中水印的使用</title>
    <url>/2021/07/24/flink-zhong-shui-yin-de-shi-yong/</url>
    <content><![CDATA[<p>之前大概介绍过在简单的水印使用。时间作为流计算中最重要的部分，Flink对于其中关键的Watermark提供了比较丰富的支持。可以在多个地方生成、分配和使用Watermark</p>
<span id="more"></span>

<blockquote>
<p>内容基于1.13</p>
</blockquote>
<h1 id="Watermark的生成"><a href="#Watermark的生成" class="headerlink" title="Watermark的生成"></a>Watermark的生成</h1><h2 id="在DataStream中生成"><a href="#在DataStream中生成" class="headerlink" title="在DataStream中生成"></a>在DataStream中生成</h2><p>通常水位线会在source中生成，不过Flink能够根据业务需要，在对DataStream进行API操作后，使用时间戳和watermark生成器修改数据记录的时间戳和watermark</p>
<h3 id="SourceFunction中生成Watermark"><a href="#SourceFunction中生成Watermark" class="headerlink" title="SourceFunction中生成Watermark"></a>SourceFunction中生成Watermark</h3><ul>
<li><p>为记录分配时间戳，需要调用<code>SourceContext</code>的<code>collectWithTimestamp</code>方法。</p>
<ul>
<li>```java<br>/**<ul>
<li>提交从数据源获取的记录，并附加给记录时间戳。该方法与使用事件时间息息相关。</li>
<li>使用事件时间需要通过记录内容获取时间戳。而不是依赖于流上的TimestampAssigner</li>
<li></li>
<li>On ProcessingTime, 时间戳会被忽略</li>
<li>On IngestionTime, 时间戳会被覆盖为系统当前时间</li>
<li>On EventTime, 时间戳会被使用</li>
<li></li>
<li>@param element The element to emit</li>
<li>@param timestamp The timestamp in milliseconds since the Epoch</li>
<li>/<br>@PublicEvolving<br>void collectWithTimestamp(T element, long timestamp);<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">- 为生成Watermark，需要调用`SourceContext`的`emitWatermark`方法。</span><br><span class="line"></span><br><span class="line">  - ```java</span><br><span class="line">    /**</span><br><span class="line">     * 提交一个给定的Watermark</span><br><span class="line">     * On ProcessingTime, 该Watermark会被忽略</span><br><span class="line">     * On IngestionTime, 该Watermark会被覆盖为系统自动生成的IngestionTime Watermark</span><br><span class="line">     * On EventTime, 该Watermark会被使用</span><br><span class="line">     *</span><br><span class="line">     * @param mark The Watermark to emit</span><br><span class="line">     */</span><br><span class="line">    @PublicEvolving</span><br><span class="line">    void emitWatermark(Watermark mark);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="DataStream-API中生成Watermark"><a href="#DataStream-API中生成Watermark" class="headerlink" title="DataStream API中生成Watermark"></a>DataStream API中生成Watermark</h3><p>DataSream API中使用的TimestampAssigner接口定义了时间戳的提取行为。其有两个子接口，定义了不同的Watermark生成策略。</p>
<h4 id="AssignerWithPeriodicWatermarks"><a href="#AssignerWithPeriodicWatermarks" class="headerlink" title="AssignerWithPeriodicWatermarks"></a>AssignerWithPeriodicWatermarks</h4><p>该接口的实现类会周期性的生成Watermark，而不会针对每个事件都生成。</p>
<p>####AssignerWithPunctuatedWatermarks</p>
<p>该接口的实现类对每个事件都会尝试生成Watermark，不过如果生成的Watermark为null或者小于之前的Watermark，则不会像下游发送。</p>
<p>通过调用<code>assignTimestampsAndWatermarks</code>方法。可以为流数据分配一个Assigner。</p>
<blockquote>
<p>在新版本的Flink中。TimestampAssigner已经被标记为过时，总的来说建议通过WatermarkStrategy来进行上述操作。</p>
</blockquote>
<p>####WatermarkStrategy</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WatermarkStrategy&lt;Event&gt; eventWatermarkStrategy = WatermarkStrategy</span><br><span class="line">      .&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">      .withTimestampAssigner((event, timestamp) -&gt; event.getTimestamp());</span><br></pre></td></tr></table></figure>

<p>同样也是通过调用<code>DataStream</code>的<code>assignTimestampsAndWatermarks</code>方法。</p>
<h2 id="在Flink-SQL中生成"><a href="#在Flink-SQL中生成" class="headerlink" title="在Flink SQL中生成"></a>在Flink SQL中生成</h2><h3 id="TableSource定义"><a href="#TableSource定义" class="headerlink" title="TableSource定义"></a>TableSource定义</h3><p>可以在实现了 <code>DefinedRowTimeAttributes</code> 的 <code>TableSource</code> 中定义。<code>getRowtimeAttributeDescriptors()</code> 方法返回 <code>RowtimeAttributeDescriptor</code> 的列表，包含了描述事件时间属性的字段名字、如何计算事件时间、以及 watermark 生成策略等信息。</p>
<p>同时需要确保 <code>getDataStream</code> 返回的 <code>DataStream</code> 已经定义好了时间属性。 只有在定义了 <code>StreamRecordTimestamp</code> 时间戳分配器的时候，才认为 <code>DataStream</code> 是有时间戳信息的。 只有定义了 <code>PreserveWatermarks</code> watermark 生成策略的 <code>DataStream</code> 的 watermark 才会被保留。反之，则只有时间字段的值是生效的。</p>
<blockquote>
<p>TableSource及相关使用在新版本中已经被标记为过时，而改用新的<code>DynamicTableSource</code>接口。新的数据源需要实现<code>SupportsSourceWatermark</code>接口或<code>SupportsWatermarkPushDown</code>接口，获得相关的水位线能力。这部分会在自定义Table-Connector中提及。</p>
<p>可参考<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-95%3A+New+TableSource+and+TableSink+interfaces">FLIP-95: Add new table source and sink interfaces</a>。主要应该还是为了统一批流一体。</p>
</blockquote>
<h3 id="在DataStream中定义"><a href="#在DataStream中定义" class="headerlink" title="在DataStream中定义"></a>在DataStream中定义</h3><p><em>事件时间属性可以用 .rowtime 后缀在定义 DataStream schema 的时候来定义。</em>其时间属性与stream保持一致。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, $<span class="string">&quot;sid&quot;</span>, $<span class="string">&quot;callOut&quot;</span>, $<span class="string">&quot;callIn&quot;</span>, $<span class="string">&quot;callType&quot;</span>, $<span class="string">&quot;callTime&quot;</span>.rowtime(), $<span class="string">&quot;duration&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="在DDL中定义"><a href="#在DDL中定义" class="headerlink" title="在DDL中定义"></a>在DDL中定义</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name STRING,</span><br><span class="line">  data STRING,</span><br><span class="line">  user_action_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  <span class="comment">-- 声明 user_action_time 是事件时间属性，并且用 延迟 5 秒的策略来生成 watermark</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> user_action_time <span class="keyword">AS</span> user_action_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure>



<h1 id="多流的Watermark"><a href="#多流的Watermark" class="headerlink" title="多流的Watermark"></a>多流的Watermark</h1><p>Flink内部实现每一个边上只能有一个递增的Watermark，当出现多个汇聚在一起时，Flink会选择流入的EventTime中最小的一个向下游流出。保证Watermark的单调递增性。</p>
<p>在Flink的底层模型上，多流输入会被分解为多个双流输入。无论哪一个流的Watermark进入算子，都需要跟<strong>另一个流</strong>的当前算子进行比较，选择较小的Watermark，再与算子当前的Watermark进行比较，如果大于算子当前的Watermark，则更新。处理逻辑如下图所示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210725105916.png" alt="image-20210725105916565"></p>
<p>具体的代码逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AbstractStreamOperator.java</span></span><br><span class="line"><span class="comment">// 处理source1的watermark</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark1</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    processWatermark(mark, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理source2的watermark</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark2</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    processWatermark(mark, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理watermark时会调用combinedWatermark.updateWatermark</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">processWatermark</span><span class="params">(Watermark mark, <span class="keyword">int</span> index)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (combinedWatermark.updateWatermark(index, mark.getTimestamp())) &#123;</span><br><span class="line">      <span class="comment">// 如果确实需要更新，再更新</span></span><br><span class="line">        processWatermark(<span class="keyword">new</span> Watermark(combinedWatermark.getCombinedWatermark()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 尝试更新当前算子的watermark</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">updateWatermark</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    checkArgument(index &lt; partialWatermarks.length);</span><br><span class="line">  	<span class="comment">// 设置某条流的Watermark为新的watermark</span></span><br><span class="line">    partialWatermarks[index].setWatermark(timestamp);</span><br><span class="line">    <span class="comment">// 尝试更新合并</span></span><br><span class="line">    <span class="keyword">return</span> combinedWatermarkStatus.updateCombinedWatermark();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置某条流的Watermark为新的watermark。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">setWatermark</span><span class="params">(<span class="keyword">long</span> watermark)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.idle = <span class="keyword">false</span>;</span><br><span class="line">    <span class="comment">// 设置时会进行大小判断，保证watermark比之前的大</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">boolean</span> updated = watermark &gt; <span class="keyword">this</span>.watermark;</span><br><span class="line">    <span class="keyword">this</span>.watermark = Math.max(watermark, <span class="keyword">this</span>.watermark);</span><br><span class="line">    <span class="keyword">return</span> updated;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新当前算子的watermark</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">updateCombinedWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> minimumOverAllOutputs = Long.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 仅当有水位线时才更新</span></span><br><span class="line">    <span class="keyword">if</span> (partialWatermarks.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">this</span>.idle = combinedWatermark &gt; Long.MIN_VALUE;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> allIdle = <span class="keyword">true</span>;</span><br><span class="line">    <span class="comment">// 遍历所有水位线</span></span><br><span class="line">    <span class="keyword">for</span> (PartialWatermark partialWatermark : partialWatermarks) &#123;</span><br><span class="line">      	<span class="comment">// 当一条线不是空闲时进行判断</span></span><br><span class="line">        <span class="keyword">if</span> (!partialWatermark.isIdle()) &#123;</span><br><span class="line">          	<span class="comment">// 将partialWatermark与minimumOverAllOutputs比较，取得最小值。</span></span><br><span class="line">          	<span class="comment">// 这里出来的是所有partialWatermark最小的watermark</span></span><br><span class="line">            minimumOverAllOutputs =</span><br><span class="line">                    Math.min(minimumOverAllOutputs, partialWatermark.getWatermark());</span><br><span class="line">            allIdle = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.idle = allIdle;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!allIdle &amp;&amp; minimumOverAllOutputs &gt; combinedWatermark) &#123;</span><br><span class="line">        combinedWatermark = minimumOverAllOutputs;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (timeServiceManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">        timeServiceManager.advanceWatermark(mark);</span><br><span class="line">    &#125;</span><br><span class="line">    output.emitWatermark(mark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其简单的调用路径如下：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210725113741.png" alt="image-20210725113741307"></p>
<p>在官方的例子中：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223730.png" alt="Kafka in WaterMark"></p>
<p>Source算子各自产生Watermark（黄色部分），并随着数据流流向下游map算子。由于map是无状态算子，并不需要使用watermark，会讲其透传至后续算子。window接收到后，会选择其中较小的一个使用。如图所示选择了watermark14。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Time</tag>
        <tag>Window</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的Runtime核心机制</title>
    <url>/2021/07/28/flink-de-runtime-he-xin-ji-zhi/</url>
    <content><![CDATA[<p>Flink 可以运行在多种不同的环境中，它可以通过单进程多线程的方式直接运行，从而提供调试的能力。它也可以运行在 Yarn 或者 K8S 这种资源管理系统上面，也可以在各种云环境中执行。对于不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。</p>
<span id="more"></span>

<h1 id="Flink的整体架构"><a href="#Flink的整体架构" class="headerlink" title="Flink的整体架构"></a>Flink的整体架构</h1><p>Flink 可以运行在多种不同的环境中，它可以通过单进程多线程的方式直接运行，从而提供调试的能力。它也可以运行在 Yarn 或者 K8S 这种资源管理系统上面，也可以在各种云环境中执行。对于不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210728172903.png" alt="图片-1"></p>
<p>Flink Runtime 层的整个架构主要是在 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077">FLIP-6</a> 中实现的。是一个标准的Master-slave架构，master负责管理集群的资源与调度，slave负责具体作业的执行。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210728173245.png" alt="图片-2"></p>
<p>Master即图中的AppMaster进程，由3个组件组成：Dispatcher、ResourceManager 和 JobManager。</p>
<ul>
<li>Dispatcher<ul>
<li>负责接收用户从Client submit上来的作业。</li>
<li>负责为新提交的作业拉起一个新的 JobManager 组件。</li>
</ul>
</li>
<li>ResourceManager<ul>
<li>负责资源的管理</li>
<li><strong>整个 Flink 集群中只有一个 ResourceManager</strong></li>
</ul>
</li>
<li>JobManager<ul>
<li>负责管理作业的执行。</li>
<li>在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。</li>
</ul>
</li>
</ul>
<p>故当用户提交作业时，其逻辑流程可以简化为：</p>
<ol>
<li>提交脚本会首先启动一个 Client进程负责作业的编译与提交。它会将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。</li>
<li> Client将JobGraph 提交到集群中执行。</li>
<li>对于Yarn，由于AM 不会预先启动，此时 Client 将首先向资源管理系统申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。</li>
<li>作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件。然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。<ol>
<li>对于Yarn，ResourceManager 需要首先向外部资源管理系统申请资源来启动 TaskExecutor。然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配。</li>
<li>目前 Flink 中 TaskExecutor 的资源是通过 Slot 来描述的，一个 Slot 一般可以执行一个具体的 Task，但在一些情况下也可以执行多个相关联的 Task。</li>
</ol>
</li>
<li>ResourceManager 选择到空闲的 Slot 之后，就会通知相应的 TM “将该 Slot 分配分 JobManager XX ”，然后 TaskExecutor 进行相应的记录后，会向 JobManager 进行注册。</li>
<li>JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。</li>
<li>TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</li>
</ol>
<h2 id="关于第一步"><a href="#关于第一步" class="headerlink" title="关于第一步"></a>关于第一步</h2><blockquote>
<p>参考的代码版本：1.09 、1.10、 1.13</p>
</blockquote>
<p>我们这里主要讨论提交到Yarn的Per-Job的逻辑，通过观察flink的提交脚本<code>flink</code>，可以看到最后实际是调用了<code>org.apache.flink.client.cli.CliFrontend</code>这个类。</p>
<blockquote>
<p>在1.9中，会通过run这个分支去调用：<code>main-&gt;parseParameters-&gt;run-&gt;runProgram</code></p>
<p>在1.10中，会通过run这个分支去调用：<code>main-&gt;parseParameters-&gt;run-&gt;executeProgram</code></p>
<p>在最新1.13中，如果使用run-application这种模式，则提交会一路按照<code>main-&gt;parseAndRun-&gt;runApplication-&gt;deployer.run</code>进行方法调用。最终Job的提交会来到<code>YarnClusterDescriptor#deployApplicationCluster</code>。</p>
<p>所以这部分提交其实发生了数次变化。</p>
</blockquote>
<h3 id="1-9"><a href="#1-9" class="headerlink" title="1.9"></a>1.9</h3><p>其中最关键的是在run方法中，通过<code>validateAndGetActiveCommandLine</code>方法，得到一个<code>activeCommandLine</code>，这个其实是在main函数中就完成了初始化的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> CustomCommandLine&lt;?&gt; customCommandLine = getActiveCustomCommandLine(commandLine);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实是在这里初始化</span></span><br><span class="line"><span class="comment">// CliFrontend#loadCustomCommandLines</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;CustomCommandLine&gt; <span class="title">loadCustomCommandLines</span><span class="params">(Configuration configuration, String configurationDirectory)</span> </span>&#123;</span><br><span class="line">		...</span><br><span class="line">		<span class="keyword">final</span> String flinkYarnSessionCLI = <span class="string">&quot;org.apache.flink.yarn.cli.FlinkYarnSessionCli&quot;</span>;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			customCommandLines.add(</span><br><span class="line">				loadCustomCommandLine(flinkYarnSessionCLI,</span><br><span class="line">					configuration,</span><br><span class="line">					configurationDirectory,</span><br><span class="line">					<span class="string">&quot;y&quot;</span>,</span><br><span class="line">					<span class="string">&quot;yarn&quot;</span>));</span><br><span class="line">		&#125;</span><br><span class="line">  	...</span><br><span class="line">		<span class="keyword">return</span> customCommandLines;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所以其本质是<code>flinkYarnSessionCLI</code>，在<code>runProgram</code>方法中，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 这里Flink会针对不同的提交模式有不同的分支，主要是判断是否为detached mode</span></span><br><span class="line"><span class="comment">// 如果是，则得到jobGraph，提交任务</span></span><br><span class="line"><span class="keyword">final</span> JobGraph jobGraph = PackagedProgramUtils.createJobGraph(program, configuration, parallelism);</span><br><span class="line">client = clusterDescriptor.deployJobCluster(</span><br><span class="line">					clusterSpecification,</span><br><span class="line">					jobGraph,</span><br><span class="line">					runOptions.getDetachedMode());</span><br><span class="line"><span class="comment">// 如果不是，则executeProgram</span></span><br><span class="line">executeProgram(program, client, userParallelism);</span><br><span class="line"><span class="comment">//// 如果prog.isUsingProgramEntryPoint()，则</span></span><br><span class="line"><span class="keyword">return</span> run(jobWithJars, parallelism, prog.getSavepointSettings());</span><br><span class="line"><span class="comment">////// 最终会调用到：</span></span><br><span class="line">JobGraph job = getJobGraph(flinkConfig, compiledPlan, libraries, classpaths, savepointSettings);</span><br><span class="line"><span class="keyword">return</span> submitJob(job, classLoader);</span><br><span class="line"><span class="comment">//// 否则，则直接调用main函数</span></span><br><span class="line">prog.invokeInteractiveModeForExecution();</span><br><span class="line"><span class="comment">////// PackageProgram#callMainMethod</span></span><br><span class="line">mainMethod = entryClass.getMethod(<span class="string">&quot;main&quot;</span>, String[].class);</span><br><span class="line">mainMethod.invoke(<span class="keyword">null</span>, (Object) args);</span><br></pre></td></tr></table></figure>

<p>执行用户的main方法后，就是flink的标准流程了。创建env、构建StreamDAG、生成Pipeline、提交到集群、阻塞运行。当main程序执行完毕，整个run脚本程序也就退出了。</p>
<h3 id="1-10"><a href="#1-10" class="headerlink" title="1.10"></a>1.10</h3><p>前半部分基本都一样，不过run方法中的最终调用由<code>runProgram</code>修改为<code>executeProgram</code>，相关修改对应的是<a href="https://issues.apache.org/jira/browse/FLINK-14851">FLINK-14851</a>和<a href="https://issues.apache.org/jira/browse/FLINK-14130">FLINK-14130</a>。即删除了之前runProgram及run相关的代码。简单来讲社区认为不应该将这部分工作放在<code>run</code>中。</p>
<blockquote>
<p>The <code>run()</code> methods are concerned with unpacking programs or job-with-jars and at the end use <code>submitJob()</code> in some way, they should reside in some other component.</p>
</blockquote>
<p>最终会直接通过下属代码直接进行任务的调用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// PackageProgram.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invokeInteractiveModeForExecution</span><span class="params">()</span> <span class="keyword">throws</span> ProgramInvocationException </span>&#123;</span><br><span class="line">		callMainMethod(mainClass, args);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PackageProgram#callMainMethod</span></span><br><span class="line">mainMethod = entryClass.getMethod(<span class="string">&quot;main&quot;</span>, String[].class);</span><br><span class="line">mainMethod.invoke(<span class="keyword">null</span>, (Object) args);</span><br></pre></td></tr></table></figure>



<h3 id="1-13"><a href="#1-13" class="headerlink" title="1.13"></a>1.13</h3><p>新版本相比于1.10，如要是提供了<code>runApplication</code>方法，与run进行了区分。这涉及了在1.11引入的新的yarn部署模式。相比于之前，<strong>原本需要客户端做的三件事被转移到了JobManager里</strong>，也就是说main()方法在集群中执行，Deployer只需要负责发起部署请求了。另外，如果一个main()方法中有多个env.execute()/executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）。可见，Application模式本质上是Session和Per-Job模式的折衷。</p>
<p>// TODO</p>
<h1 id="资源管理与作业调度"><a href="#资源管理与作业调度" class="headerlink" title="资源管理与作业调度"></a>资源管理与作业调度</h1><p>作业调度可以看做是对资源和任务进行匹配的过程。如上节所述，在 Flink 中，资源是通过 Slot 来表示的，每个 Slot 可以用来执行不同的 Task。而在另一端，任务即 Job 中实际的 Task，它包含了待执行的用户逻辑。调度的主要目的就是为了给 Task 找到匹配的 Slot。</p>
<p>在 1.9 之前，Flink 是不支持细粒度的资源描述的，而是统一的认为每个 Slot 提供的资源和 Task 需要的资源都是相同的。</p>
<p>从 1.9 开始，Flink 开始增加对细粒度的资源匹配的支持的实现，每个 Slot 都有一个向量来描述它所能提供的各种资源的量，每个 Task 也需要相应的说明它所需要的各种资源的量。</p>
<blockquote>
<p>在 ResourceManager 中，有一个子组件叫做 SlotManager，它维护了当前集群中所有 TaskExecutor 上的 Slot 的信息与状态：如该 Slot 在哪个 TaskExecutor 中，该 Slot 当前是否空闲等。当 JobManger 来为特定 Task 申请资源的时候，ResourceManager 可能会去申请资源来启动新的 TaskExecutor。当 TaskExecutor 启动之后，它会通过服务发现找到当前活跃的 ResourceManager 并进行注册。在注册信息中，会包含该 TaskExecutor中所有 Slot 的信息。 ResourceManager 收到注册信息后，其中的 SlotManager 就会记录下相应的 Slot 信息。当 JobManager 为某个 Task 来申请资源时， SlotManager 就会从当前空闲的 Slot 中按一定规则选择一个空闲的 Slot 进行分配。当分配完成后，RM 会首先向 TaskManager 发送 RPC 要求将选定的 Slot 分配给特定的 JobManager。TaskManager 如果还没有执行过该 JobManager 的 Task 的话，它需要首先向相应的 JobManager 建立连接，然后发送提供 Slot 的 RPC 请求。在 JobManager 中，所有 Task 的请求会缓存到 SlotPool 中。当有 Slot 被提供之后，SlotPool 会从缓存的请求中选择相应的请求并结束相应的请求过程。</p>
<p>当 Task 结束之后，无论是正常结束还是异常结束，都会通知 JobManager 相应的结束状态，然后在 TaskManager 端将 Slot 标记为已占用但未执行任务的状态。JobManager 会首先将相应的 Slot 缓存到 SlotPool 中，但不会立即释放。这种方式避免了如果将 Slot 直接还给 ResourceManager，在任务异常结束之后需要重启时，需要立刻重新申请 Slot 的问题。通过延时释放，Failover 的 Task 可以尽快调度回原来的 TaskManager，从而加快 Failover 的速度。当 SlotPool 中缓存的 Slot 超过指定的时间仍未使用时，SlotPool 就会发起释放该 Slot 的过程。与申请 Slot 的过程对应，SlotPool 会首先通知 TaskManager 来释放该 Slot，然后 TaskExecutor 通知 ResourceManager 该 Slot 已经被释放，从而最终完成释放的逻辑。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210729135920.png" alt="图片-4"></p>
</blockquote>
<p>本质上来说，</p>
<ul>
<li>JobManager用于管理作业的执行进度。执行Task会找RM申请资源。相当于资源的申请者。</li>
<li>ResourceManager负责管理资源，主要通过SlotManager，维护者TaskManager上Slot与Slot的状态，在有资源申请时负责管理和分配slot资源。</li>
<li>TaskManager是资源的实际所有者。Slot都是TM提供。</li>
</ul>
<blockquote>
<p>注意这里的RM都是Flink内部的概念，在Yarn模式下，整个AM是运行在一个Container下的。</p>
</blockquote>
<p>1、当JM需要自信Task时。</p>
<p>2、对于per-job模式，如果没有启动过TM，RM则会通过2/3/4步骤启动一个TM。</p>
<p>5、TM启动会向RM进行注册，提供一些元信息。RM会通知TM分配了哪个Slot给JM。</p>
<p>6、TM就会通知JM分配给其某个Slot，JM会将该Slot缓存到Slotpool并与之前的Task申请进行匹配。匹配后就会将该Task调度到对应的slot之上。</p>
<p>7、Task执行成功或者失败后，会触发Slot的释放逻辑。</p>
<blockquote>
<p>关于Flink中Job和Task的概念。</p>
<p>一个大的任务是一个Job，即execute就会触发一个Job</p>
<p>一个Operator就是一个Task，不过Flink会在由StreamGraph生成JobGraph的过程中进行OperatorChain，将部分Operator合并为一个Task（类似于Spark的stage划分）</p>
<p>实际运行时，Task会按照并行度分成多个Subtask，Subtask是执行/调度的基本单元。subtask运行在slot中。</p>
</blockquote>
<p>// 进行batch模式时，类似于spark，会根据shuffle进行任务分割，shuffle产生的数据会存放在tm所在机器上。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的Window详解</title>
    <url>/2021/07/31/flink-de-window-xiang-jie/</url>
    <content><![CDATA[<p>Window是Flink中一类算子。一般会搭配时间来处理一个范围的数据</p>
<span id="more"></span>

<h1 id="Window的分类"><a href="#Window的分类" class="headerlink" title="Window的分类"></a>Window的分类</h1><p>Flink中Window可以有不同维度的多种分类，总的来说，Flink提供了三类默认窗口：计数窗口、时间窗口和会话窗口。而计数窗口和时间窗口根据Window的位移方式又分为了：滑动窗口和滚动窗口。两类。</p>
<h2 id="计数窗口"><a href="#计数窗口" class="headerlink" title="计数窗口"></a>计数窗口</h2><p>累计固定个数的元素视为一个窗口。</p>
<blockquote>
<p>不过计数窗口其实并不是一个单独的类别。不过实际使用上来说，</p>
<p>看源码可以知道，本质上是一种<strong>基于全局窗口实现的</strong>，故后续讨论的没注明的都是基于时间的窗口。</p>
</blockquote>
<h2 id="时间窗口"><a href="#时间窗口" class="headerlink" title="时间窗口"></a>时间窗口</h2><p>在时间上按照事先约定的窗口大小切分的窗口</p>
<h2 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h2><p>当超过一定时间后，该窗口没有新元素进入时，则窗口结束。故无法实现确定窗口的大小、元素个数。窗口间也不会重叠</p>
<h2 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h2><p>滚动窗口整体概念上是最简单的，即设定好窗口长度即可。下一个窗口的开始是上一个窗口的结束，每个窗口长度一致：</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.13/fig/tumbling-windows.svg" alt="Tumbling Windows"></p>
<h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h2><p>滑动窗口则按照划分方式是时间还是计数略有不同。</p>
<p>对于时间/计数，下一个窗口的开始，是上一个窗口的开始 + 设定的滑动时间/数量大小。</p>
<p>两个滑动窗口间可能会出现重叠部分，如图中的Window1和Window2。（图中的slide恰好是Window size的一半）</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.13/fig/sliding-windows.svg" alt="sliding windows"></p>
<h1 id="窗口原理与机制"><a href="#窗口原理与机制" class="headerlink" title="窗口原理与机制"></a>窗口原理与机制</h1><blockquote>
<p>之前在介绍时间语义时写过，这次换一个角度去说明。</p>
</blockquote>
<p>每一个数据元素进入窗口算子时，首先会被交给<code>WindowAssigner</code>。<code>WindowAssigner</code>决定元素被放到<em>哪个</em>或<em>哪些</em>窗口。在这个过程中可能会创建或者合并旧的窗口。<strong>WindowOperator中可能会存在多个窗口，一个元素也可能被放入多个窗口中</strong>。</p>
<p>Window本身只是一个ID标识符，内存可能存储了一些元数据，如TimeWindow会存放开始和结束时间，但<strong>窗口不会存储窗口中的元素</strong>。窗口的元素实际存储在Key/Value State中，其中Key为Window，Value为数据集合（某些函数为聚合后的值）。这部分实现依赖于Flink的State机制。</p>
<p>每个窗口都会拥有一个自己的<code>Trigger</code>，<code>Trigger</code>上有定时器，用来决定窗口何时能够被计算或清除。同时，<strong>每当有元素被分配到该窗口</strong>或者<strong>之前注册的定时器超时时</strong>，<code>Trigger</code>都会被调用。</p>
<p><code>Trigger</code>被触发后，元素集合会交给<code>Evictor</code>（如果没有指定<code>Evictor</code>则没有这一步）。<code>Evictor</code>主要用来从窗口中删除一些已收集元素。如果定义在具体的计算函数前，则会将删除后的元素再交给计算函数。如果没有指定<code>Evictor</code>，则会将所有元素直接交给执行函数。同样，<code>Evictor</code>也可以放在执行函数后，过滤计算结果。</p>
<p>最后元素就会交给具体的计算函数，Flink对一些聚合类函数进行了优化，如<code>sum</code>、<code>min</code>、<code>max</code>等，由于这些聚合函数不需要将所有元素存储下来，只需要存储每一次的聚合结果，能够大大降低内存消耗并提升性能。不过如果制定了<code>Evictor</code>，则不会启用这些优化。</p>
<blockquote>
<p>在API层面，WindowAssigner时必须要指定的，不过Flink会有一个默认的<code>Trigger</code>分配给窗口，故如果不需要自定义这部分逻辑且不是<code>Global Window</code>，可省去这部分：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream</span><br><span class="line">    .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">    .window(...)              &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">   [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">   [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">   [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">   [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">    .reduce/aggregate/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">   [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure>

<p><code>window</code>和<code>windowAll</code>这部分是一致的。而计数窗口的入口为<code>countWindow</code>。</p>
</blockquote>
<h2 id="WindowAssigner"><a href="#WindowAssigner" class="headerlink" title="WindowAssigner"></a>WindowAssigner</h2><p>用来决定元素被分配到哪个/哪些窗口中。<code>SessionWindow</code>此处较为特殊，因为其无法事先确定窗口的范围。一般来说使用内置的<code>WindowAssigner</code>就可以了。在其它情况也可以自定义<code>WindowAssigner</code>。</p>
<h3 id="内置窗口分配器"><a href="#内置窗口分配器" class="headerlink" title="内置窗口分配器"></a>内置窗口分配器</h3><ul>
<li><p>（滚动窗口）：下一个窗口的开始为：这个窗口的结束时间。</p>
<ul>
<li><p>提供两个内置实现（事件时间和处理时间各一个）：</p>
<ul>
<li><p><code>TumblingEventTimeWindows</code>：基于事件时间的滚动窗口。</p>
</li>
<li><p><code>of</code>方法接收参数：</p>
<ul>
<li><p>size : 窗口大小</p>
</li>
<li><p>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</p>
</li>
<li><p>windowStagger：交错策略。实际源码中表现为会影响offset的效果。</p>
<ul>
<li><blockquote>
<p>The utility that produces staggering offset in runtime. </p>
<p>有三类：</p>
<p>​    ALIGNED：默认。不产生任何影响。所有分区的窗口开始时间都是一样的</p>
<p>​    RANDOM：加入一个随机值作为交错参数。即每个分区的第一个窗口的开始时间会随机开始。</p>
<p>​    NATURAL：根据每个线程得到的第一个元素，来作为该分区的第一个窗口的开始时间。</p>
<p>（待验证）</p>
<p>Flink会在WindowOperator中的processElement方法中，对每个接收到的流元素进行窗口画风。而这个参数会影响到最后生成的elementWindows，windowStagger主要就是通过影响offset，来影响elementWindows的生成。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>窗口大小决定了多久会生成一个新的窗口。</p>
</li>
</ul>
</li>
<li><p>SlidingTimeWindow（滑动窗口）：下一个窗口开始时间为：这个窗口开始时间+滑动距离。</p>
<ul>
<li>提供两个内置实现（事件时间和处理时间各一个）：<ul>
<li><code>SlidingEventTimeWindows</code>：基于事件时间的滑动窗口。</li>
<li><code>of</code>方法接收参数：<ul>
<li>size : 窗口大小。</li>
<li>slide：滑动距离。</li>
<li>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</li>
</ul>
</li>
</ul>
</li>
<li>滑动距离决定了多久会生成一个新的窗口。</li>
</ul>
</li>
<li><p>sessionWindow（会话窗口）：下一个窗口开始时间为：新元素到达 且距离上一个元素到达时间超过阈值。</p>
<ul>
<li><p>提供两类(动态和固定)，供四个（事件时间和处理时间各两个）内置实现：</p>
<ul>
<li><p><code>EventTimeSessionWindows</code>：基于事件时间的会话窗口。</p>
<ul>
<li>方法：<ul>
<li><code>withGap(Time size)</code>：间隔阈值。</li>
</ul>
</li>
</ul>
</li>
<li><p><code>DynamicEventTimeSessionWindows</code>：基于事件时间的动态会话窗口。</p>
<ul>
<li><p>方法：</p>
<ul>
<li><p><code>withDynamicGap(SessionWindowTimeGapExtractor&lt;T&gt; sessionWindowTimeGapExtractor)</code>：</p>
<ul>
<li><blockquote>
<p>SessionWindowTimeGapExtractor能够从数据中提取超时间隔。</p>
<p>即数据中可以有一个字段，来作为这条数据距离上一条数据多久。<code>SessionWindowTimeGapExtractor</code>接口需要手动实现。且只有一个方法，从数据中拿到这他的间隔<code>long extract(T element);</code></p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>两者的主要区别：</p>
<ul>
<li>非动态：超时间隔是固定的。</li>
<li>动态：有数据自己决定是否划分新窗口。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>globalWindow（全局窗口）: 只会产生一个窗口</p>
<ul>
<li>不会自动触发，必须手动指定<em>触发器</em>。</li>
<li>必须指定<em>移除器</em>来移除窗口中的元素</li>
</ul>
</li>
</ul>
<h3 id="自定义窗口分配器"><a href="#自定义窗口分配器" class="headerlink" title="自定义窗口分配器"></a>自定义窗口分配器</h3><p><code>WindowAssigner</code>接口提供了四个方法需要实现：</p>
<ul>
<li><code>assignWindows(T element, long timestamp, WindowAssignerContext context)</code>：返回元素归属的窗口集合（这条记录归属哪些窗口）<ul>
<li>一般都是返回TimeWindow。基于数量的窗口可以使用<code>countWindow</code></li>
</ul>
</li>
<li><code>getDefaultTrigger(StreamExecutionEnvironment env)</code>：返回该分配器的默认触发器。<ul>
<li>由于触发器在API层面可以省略，故在省略时需要一个默认的触发器，来源就是这。</li>
<li>请参考自定义触发器，或者Trigger子类的内部实现。</li>
</ul>
</li>
<li><code>getWindowSerializer(ExecutionConfig executionConfig)</code>：返回窗口的TypeSerializer<ul>
<li>对于TimeWindow而言，则返回<code>new TimeWindow.Serializer()</code>即可。</li>
</ul>
</li>
<li><code>isEventTime()</code>：是否是事件时间窗口</li>
</ul>
<blockquote>
<p>需要注意的是，<code>SessionWindow</code>的<code>WindowAssigner</code>需要实现<code>MergingWindowAssigner</code>抽象类，因为对于<code>SessionWindow</code>来说，每个记录都会被创建一个单独的窗口，当Window间差距小于<code>Gap</code>时会触发<code>Merge</code>，以此达到<code>SessionWindow</code>的效果。在Flink使用``时，也会进行判断，只有实现了该抽象类，Flink才会认为是会话窗口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WindowOperator.java</span></span><br><span class="line"><span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner)&#123;...&#125;</span><br></pre></td></tr></table></figure>


</blockquote>
<h3 id="关于计数窗口"><a href="#关于计数窗口" class="headerlink" title="关于计数窗口"></a>关于计数窗口</h3><p>计数窗口的入口函数为<code>countWindow</code>，观察其源码可以看到都是基于全局窗口实现的：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows this &#123;<span class="doctag">@code</span> KeyedStream&#125; into tumbling count windows.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows this &#123;<span class="doctag">@code</span> KeyedStream&#125; into sliding count windows.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create())</span><br><span class="line">            .evictor(CountEvictor.of(size))</span><br><span class="line">            .trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其实也很好理解，计数窗口的触发条件就是数量达到多少个。滑动计数窗口的位移就是移除多少个老的元素。</p>
<h2 id="WindowTrigger"><a href="#WindowTrigger" class="headerlink" title="WindowTrigger"></a>WindowTrigger</h2><p>WindowTrigger决定了一个窗口何时能够被计算或清除。这个通过Trigger的触发结果来决定。每个窗口都有一个属于自己的Trigger。<strong>每当有元素被分配到该窗口</strong>或者<strong>之前注册的定时器超时时</strong>，<code>Trigger</code>都会被调用。</p>
<h3 id="Trigger的触发结果"><a href="#Trigger的触发结果" class="headerlink" title="Trigger的触发结果"></a>Trigger的触发结果</h3><p>Trigger有四种触发结果：</p>
<ul>
<li>Continue：继续，不做任何操作。</li>
<li>Fire：触发<strong>计算</strong>。处理窗口中的数据。并发出结果。</li>
<li>Purge：触发<strong>清理</strong>。清空窗口内容，并删除窗口、及窗口元数据。并调用<code>ProcessWindowFunction.clear()</code>方法。</li>
<li>Fire+Purge：触发<strong>计算</strong>和<strong>清理</strong>。并移除窗口中的数据。</li>
</ul>
<h3 id="Trigger的触发流程"><a href="#Trigger的触发流程" class="headerlink" title="Trigger的触发流程"></a>Trigger的触发流程</h3><ol>
<li>当Trigger Fire时，窗口中的元素会交给Evictor。数据经过Evictor后会交给指定的计算函数进行计算。</li>
<li>计算函数收到元素后，触发计算并得到计算结果。窗口计算的结果值可能是多个。</li>
<li>如果触发清理，则会移除窗口中的数据。</li>
</ol>
<h3 id="内置触发器"><a href="#内置触发器" class="headerlink" title="内置触发器"></a>内置触发器</h3><p>Flink提供一些内置的触发器，包括（只说明事件时间的，处理时间一样的）：</p>
<ul>
<li><p><code>EventTimeTrigger</code>：水位线大于窗口结束时间，则触发。</p>
</li>
<li><p><code>CountTrigger</code>：当窗口内元素数量大于阈值时则触发。<code>countWindow</code>的实现就使用的该触发器。</p>
<ul>
<li>所以官方并没有将计数窗口单独归为一类窗口，其和时间窗口是可以共存的。</li>
</ul>
</li>
<li><p><code>DeltaTrigger</code>：初始化是需要给定一个<code>DeltaFunction</code>，根据接入数据计算出来的Delta指标是否超过指定的Threshold去判断是否触发窗口计算。</p>
<ul>
<li>计算当前元素与上一个触发计算元素的Delta值来与阈值进行比较。</li>
</ul>
</li>
<li><p><code>ContinuousEventTimeTrigger</code>:</p>
<ul>
<li><p>水位线大于窗口结束时间，则触发。</p>
</li>
<li><p>定期设置处理时间计时器。（计时器触发时会调用触发器的<code>onEventTime</code>方法。详见自定义）</p>
<ul>
<li><blockquote>
<p>1、当第一个元素达到时，会根据时间间隔注册一个计时器。触发时间约为开始时间+时间间隔（<code>start = timestamp - (timestamp % interval)+interval</code>）</p>
<p>2、当计时器触发时（），进行判断。</p>
<p>​    2.1、如果触发事件 等于 窗口结束时间，则触发触发器。</p>
<p>​    2.2、其他情况，则判断计时器是否有效，若有效则触发触发器，并根据设定的间隔注册下一个计时器。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>PurgingTrigger</code>：是一个trigger的包装类。具体作用为：如果被包装的trigger触发返回FIRE，则PurgingTrigger将返回修改为FIRE_AND_PURGE，其他的返回值不做处理。</p>
<ul>
<li>通过<code>of(Trigger&lt;T, W&gt; nestedTrigger)</code>来包装其他<code>Trigger</code></li>
</ul>
</li>
<li><p><code>ProcessingTimeoutTrigger</code>是一个trigger的包装类，作用于处理时间。具体作用为：为元素配置处理时间超时。当到达超时时间时，清除触发器状态</p>
</li>
</ul>
<h2 id="WindowEvictor"><a href="#WindowEvictor" class="headerlink" title="WindowEvictor"></a>WindowEvictor</h2><h2 id="WindowFunction"><a href="#WindowFunction" class="headerlink" title="WindowFunction"></a>WindowFunction</h2><h2 id="Lateness-And-LateData"><a href="#Lateness-And-LateData" class="headerlink" title="Lateness And LateData"></a>Lateness And LateData</h2><h2 id="SideOutput"><a href="#SideOutput" class="headerlink" title="SideOutput"></a>SideOutput</h2>]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Window</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的水位线在单线程和并行处理时候的问题记录</title>
    <url>/2020/01/27/flink-de-shui-wei-xian-zai-dan-xian-cheng-he-bing-xing-chu-li-shi-hou-de-wen-ti-ji-lu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink对于eventTime的处理是很妙的一个地方，借助水位线机制，能够帮助Flink处理延迟数据。不过在测试途中却遇到了一些有些奇怪的现象，特此记录。</p>
<blockquote>
<p>Flink版本：1.9.x</p>
</blockquote>
<h2 id="单线程下的水位线："><a href="#单线程下的水位线：" class="headerlink" title="单线程下的水位线："></a>单线程下的水位线：</h2><p>这篇博客写的很好，基本把水位线的原理讲清楚了。<br>博客地址：<a href="https://blog.csdn.net/lmalds/article/details/52704170">Flink流计算编程–watermark（水位线）简介</a><br>总结一下：</p>
<blockquote>
<p>1、在Flink中，水位线 = 最新的事件时间 - 设定的时间间隔。<br>2、当Window结束时间 &lt;= 水位线时间，且该窗口内有数据，则触发计算.<br>3、水位线到底描述了什么：事件时间&lt;水位线的数据都已经到达。未到达的数据利用Window的allowedLateness算子来特殊处理。</p>
</blockquote>
<p>不过美中不足的是其并没有提及他的测试环境是在单线程下进行的。在并行处理的情况下，水位线的表现可能和想的会不太一样。</p>
<h2 id="并行数据中的水位线："><a href="#并行数据中的水位线：" class="headerlink" title="并行数据中的水位线："></a>并行数据中的水位线：</h2><p>直接说结论：<br>Flink会生成多个水位线，取决于并行度。每个水位线会根据发往该分区的数据来独立维护。Window算子的计算触发只关心Long值最小的那个水位线。</p>
<ul>
<li>每个并行实例都会有单独的各自的水位线。<ul>
<li>对于单Source，多分区的数据流，则是每个分区都有独立的水位线。</li>
<li>对于多Source，则是每个Source的每个分区都有一个独立的水位线。</li>
</ul>
</li>
<li>当某个算子操作作用于两个流时，取决于最小的水位线。</li>
</ul>
<blockquote>
<p>简单来说，并行度3的情况下，水位线A=1581507300000，水位线B=1581507301000，水位线C=1581507302000。此时Window结束时间1581507301000&gt;水位线A，故不会触发计算，只有当水位线A&gt;Window结束时间1581507301000，此时所有线程水位线都&gt;Window的结束时间，该Window才会触发计算。</p>
</blockquote>
<blockquote>
<p>从下图的黄色标记也可以看出，并行（Parallel）情况下，取决于小的那个水位线。</p>
</blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223730.png" alt="Kafka in WaterMark"></p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Time</tag>
        <tag>Window</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的流式Join</title>
    <url>/2021/01/30/flink-de-liu-shi-join/</url>
    <content><![CDATA[<blockquote>
<p>本文Flink API版本：java-1.12</p>
</blockquote>
<p>#Flink的流式Join</p>
<p>流式Join一般来说分为两类：基于间隔的Join和基于窗口的的Join</p>
<h2 id="基于间隔的Join"><a href="#基于间隔的Join" class="headerlink" title="基于间隔的Join"></a>基于间隔的Join</h2><p>概念： 对两条流中，拥有相同键值，且彼此时间戳不超过某一指定间隔的事件进行Join。</p>
<blockquote>
<p>间隔是基于左侧流事件决定的（即B事件的事件在 A时间+设定的时间范围内时触发）</p>
<p>这类Join只支持Inner Join</p>
</blockquote>
<p>以官网的图为例：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210130154332.svg" alt="interval-join"></p>
<p>对于上述两条流，当<code>green.timestamp ∈ [orange.timestamp + lowerBound; orange.timestamp + upperBound]</code>时，就能够符合关联所需的时间范围。</p>
<p>使用方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">orangeStream</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">  	<span class="comment">// 指定使用间隔Join</span></span><br><span class="line">    .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">  	<span class="comment">// 设置间隔范围</span></span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(first + <span class="string">&quot;,&quot;</span> + second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>

<p>关键就在于使用<code>intervalJoin</code>指定为间隔Join，同时使用<code>between</code>设置间隔范围。Join成功的事件会发送给process，一般来说需要实现<code>ProcessJoinFunction</code>：</p>
<ul>
<li>left对应的途中的橙色数据流，其数据类型就对应<code>orangeStream</code>的数据类型。同理right对应于绿色数据流。</li>
<li>第三个参数对应于输出事件的数据类型。</li>
</ul>
<p>对于间隔Join，由于这个算子的水位线总是由 <em>较慢</em> 的那条流决定。所以需要注意防止两条流相差过大。</p>
<p>对于第一个输入而言：</p>
<ul>
<li>所有时间戳大于 <em>水位线 - 间隔上界</em> 的数据都会被缓存起来。<ul>
<li>水位线触发计算时，为了保证<strong>第二条</strong>流在水位线上的数据被正确参与计算，这条数据最早能够匹配到的数据是 【水位线 - 上限】的【第一条】数据中的数据。</li>
</ul>
</li>
</ul>
<p>对于第二个输入而言：</p>
<ul>
<li>所有时间戳大雨 <em>水位线 - 间隔下界</em> 的数据都会被缓存起来。<ul>
<li>水位线触发计算时，为了保证<strong>第一条</strong>流在水位线上的数据被正确参与计算，这条数据最早能够匹配到的数据是【水位线 - 下限】的【第二条】数据中的数据。</li>
</ul>
</li>
</ul>
<h2 id="基于窗口的Join"><a href="#基于窗口的Join" class="headerlink" title="基于窗口的Join"></a>基于窗口的Join</h2><p>基于窗口的Join又可以根据窗口的定义，分为三类：基于滚动窗口、基于滑动窗口、基于事件窗口。无论具体是哪种窗口，使用方式都差不多：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    <span class="comment">// 为orangeStream指定键值属性。</span></span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">  	<span class="comment">// 为greenStream指定键值属性。</span></span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">  	<span class="comment">// 选择一类Window</span></span><br><span class="line">    .window(...choose one window...)</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">&quot;,&quot;</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>

<p>其中，</p>
<ul>
<li><code>join</code>紧跟的<code>where</code>用于为左侧流指定键值属性，<code>equalTo</code>为右侧流指定键值属性。<ul>
<li>即选择那个key（可以指定一个元组，注意位置即可）作为join匹配。</li>
</ul>
</li>
</ul>
<p>基于窗口的Join工作原理如图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210130165328.png" alt="窗口Join原理"></p>
<p>触发Join需要满足下述三个条件：</p>
<ul>
<li><p>想要Join的元素位于一个窗口中。</p>
</li>
<li><p><code>where</code>中指定的左侧key与<code>equalTo</code>中指定的右侧key相等。</p>
</li>
<li><p>触发窗口计算（即水位线来到窗口的结束时间）。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的状态管理</title>
    <url>/2021/02/03/flink-de-zhuang-tai-guan-li/</url>
    <content><![CDATA[<h2 id="状态管理的逻辑"><a href="#状态管理的逻辑" class="headerlink" title="状态管理的逻辑"></a>状态管理的逻辑</h2><p>状态在Flink中的工作逻辑可以简单的理解为：业务需要访问的本地或实例变量。一般来说工作逻辑如下图所示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203145253.png" alt="状态获取和更新示意图"></p>
<ol>
<li>任务接受输入数据。并进行处理。</li>
<li>处理过程中对状态进行读取或更新。</li>
<li>处理完的结果进行输出。</li>
</ol>
<h2 id="状态的分类"><a href="#状态的分类" class="headerlink" title="状态的分类"></a>状态的分类</h2><p>在Flink中，状态主要可以分为两大类：算子状态(Operator State) 和 键值分区状态(Keyed State)</p>
<h3 id="算子状态"><a href="#算子状态" class="headerlink" title="算子状态"></a>算子状态</h3><p>Operator State可以用在所有<strong>算子</strong>上，每个算子子任务或者说每个算子实例共享一个状态。流入这个算子子任务的数据可以访问和更新这个状态。反过来说，算子状态不能通过其他任务访问，无论该任务是否来自相同算子。</p>
<p>算子状态一般用于输入/输出和少数没有键值的情况下。一般来说有如下特点：</p>
<ul>
<li><p>状态对于同一个任务而言是共享的。（每一个并行子任务共享一个状态）</p>
</li>
<li><p>相同算子的不同任务之间也不能访问。（如何理解相同算子的不同任务？）</p>
</li>
<li><blockquote>
<p>Flink概念：</p>
<p>算子：一个数据处理单元。体现在代码中就是stream的处理方法。</p>
<p>任务：每当并行度发生变化或者数据需要分组（keyBy）时（还可通过API明确设置），就会产生任务。和spark一样，一般伴随着shuffle。</p>
<p>子任务：当某个任务并行度为4时，就拥有4个子任务。</p>
</blockquote>
</li>
</ul>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203152734.png" alt="Operator State示意图"></p>
<p>针对状态算子，Flink提供了三类原语：</p>
<ul>
<li>列表状态<ul>
<li>将状态表示为一个条目列表。</li>
</ul>
</li>
<li>联合列表状态<ul>
<li>类似于列表状态，将状态表示为一个条目列表。</li>
<li>故障恢复/重启时会有所不同。扩容时有所不同。</li>
</ul>
</li>
<li>广播状态<ul>
<li>保证算子的每个任务状态都相同。</li>
</ul>
</li>
</ul>
<p>###键值分区状态</p>
<p>键值状态作用域键值上。所有键值相同的记录能够访问到一样的状态。</p>
<p>Keyed State是<code>KeyedStream</code>上的状态。假如输入流按照id为Key进行了<code>keyBy</code>分组，形成一个<code>KeyedStream</code>，数据流中所有id为1的数据共享一个状态，可以访问和更新这个状态。因为一个算子子任务可以处理一到多个Key，算子子任务1处理了两种Key，两种Key分别对应自己的状态</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203160816.png" alt="Keyed State示意图"></p>
<p>Flink为Keyed State提供了几类原语，下述为三类常见的：</p>
<ul>
<li>单值状态（value state）<ul>
<li>每个键对应存储着一个任意类型的值。该值可以是某种复杂的数据结构。</li>
</ul>
</li>
<li>列表状态（list state）<ul>
<li>每个键对应存储着一个值的列表。</li>
</ul>
</li>
<li>映射状态（map state）<ul>
<li>每个键对应存储着一个键值映射Map。</li>
</ul>
</li>
</ul>
<h2 id="有状态算子的缩扩容"><a href="#有状态算子的缩扩容" class="headerlink" title="有状态算子的缩扩容"></a>有状态算子的缩扩容</h2><p>当算子并行度发生变化时，就会涉及到有状态算子的缩扩容问题，Flink准备了总共4种策略</p>
<h3 id="算子状态-1"><a href="#算子状态-1" class="headerlink" title="算子状态"></a>算子状态</h3><p>算子状态根据状态原语不同，会有不同的缩扩容方式。</p>
<h4 id="普通"><a href="#普通" class="headerlink" title="普通"></a>普通</h4><p>会对列表中的条目进行重新分配：</p>
<ol>
<li>收集所有并行算子任务的列表条目。</li>
<li>均匀分配到更多或更少的任务上。</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203163257.png" alt="算子状态普通"></p>
<h4 id="联合列表状态"><a href="#联合列表状态" class="headerlink" title="联合列表状态"></a>联合列表状态</h4><ol>
<li>将任务的列表条目广播到全部下游任务上。</li>
<li>由下游任务决定哪些条目保留/丢弃。</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203163423.png" alt="联合列表状态缩扩容"></p>
<h4 id="广播状态"><a href="#广播状态" class="headerlink" title="广播状态"></a>广播状态</h4><p>广播状态需要与广播流一起使用。</p>
<ol>
<li>算子会把状态拷贝到下游任务上。（不同于联合列表，上下游任务是一一对应的）</li>
<li>当缩容时，也是拷贝到下游任务。停掉多出的任务。</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203170326.png" alt="广播状态缩扩容"></p>
<h3 id="键值分区状态"><a href="#键值分区状态" class="headerlink" title="键值分区状态"></a>键值分区状态</h3><p>对于键值分区缩扩容时会相对简单。Flink会通过下述步骤完成：</p>
<ol>
<li>将统一任务中的键值分为不同的键值组。</li>
<li>对键值组进行重新分区。（一个键值组只会发往一个下游任务，但上游一个任务中的不同键值组可能发往不同下游任务）</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210203162854.png" alt="键值分区缩扩容"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论是Keyed State还是Operator State，Flink的状态都是基于本地的，即每个算子子任务维护着这个算子子任务对应的状态存储，算子子任务之间的状态不能相互访问。</p>
<p>使用Keyed State一般需要在<code>RichFunction</code>中的<code>open</code>方法中初始化。而对于Operator State，还需进一步实现<code>CheckpointedFunction</code>接口。</p>
<table>
<thead>
<tr>
<th></th>
<th>Keyed State</th>
<th>Operator State</th>
</tr>
</thead>
<tbody><tr>
<td>适用算子类型</td>
<td>只适用于<code>KeyedStream</code>上的算子</td>
<td>可以用于所有算子</td>
</tr>
<tr>
<td>状态分配</td>
<td>每个Key对应一个状态</td>
<td>一个算子子任务对应一个状态</td>
</tr>
<tr>
<td>创建和访问方式</td>
<td>重写Rich Function，通过里面的RuntimeContext访问</td>
<td>实现<code>CheckpointedFunction</code>等接口</td>
</tr>
<tr>
<td>缩扩容</td>
<td>状态随着Key自动在多个算子子任务上迁移</td>
<td>根据状态类型不同而不同</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>State</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase关键参数调优及建议</title>
    <url>/2019/08/17/hbase-guan-jian-can-shu-diao-you/</url>
    <content><![CDATA[<h3 id="HBase在哪些地方调优"><a href="#HBase在哪些地方调优" class="headerlink" title="HBase在哪些地方调优"></a>HBase在哪些地方调优</h3><p>HBase用下来主要有两类需要调优的地方，是最容易影响性能，最容易出问题的步骤：Flush和Compaction。</p>
<p>查资料时也发现很多文章并没有给这两方面的建议，有些参数还已经过时了，所以才写了这篇文章。</p>
<p>其他的比如网络超时时间等参数，GC选择可以参考其他文章。</p>
<h3 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h3><h4 id="MemStore级别限制：（主要优化项）"><a href="#MemStore级别限制：（主要优化项）" class="headerlink" title="MemStore级别限制：（主要优化项）"></a>MemStore级别限制：（主要优化项）</h4><p>当Region的任意一个memstore的size 达到阈值时触发。这个是最正常的Flush，建议将参数调大，防止频繁的Flush操作。</p>
<p>涉及参数：hbase.hregion.memstore.flush.size(默认128M) </p>
<p>建议：调至256~512M左右。</p>
<h4 id="Region级别限制："><a href="#Region级别限制：" class="headerlink" title="Region级别限制："></a>Region级别限制：</h4><p>当一个Region所有的memstore的size的和达到阈值时，会触发。</p>
<p>涉及参数：hbase.hregion.memstore.block.multipiler(默认2) * hbase.hregion.memstore.flush.size(上一项中的参数)。</p>
<p>建议：（前者）设置得略大于 列簇数（假如设置为4，则建表时列族不要超过3）</p>
<h4 id="regionServer级别限制"><a href="#regionServer级别限制" class="headerlink" title="regionServer级别限制"></a>regionServer级别限制</h4><p>这个灾难的(<strong>需要调优避免</strong>) (会<strong>影响这台机器上的所有表</strong>)</p>
<p>当RegionServer所有的memstore的size之和，超过低水位线。<strong>RS强制Flush</strong>，先从MemStore最大的开始，直到<strong>总的大小下降到低水位线以下</strong>。如果此时吞吐量依然很高，达到了高水位线，会触发阻塞Flush。直到大小降低到低水位线。</p>
<p>这里的参数也和hbase.hregion.memstore.flush.size惜惜相关，所以具体设置时需要一起算一下。</p>
<p>涉及参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>JVM堆内存 -Xmx ， -Xms</td>
<td>(默认50M),RS进程的堆内存.建议物理机32G。非物理机20~30G。<br />如果内存吃紧也可调低。重点调整。</td>
</tr>
<tr>
<td>hbase.regionserver.global.memstore.size<br />或<br />hbase.regionserver.global.memstore.upperlimit（已过期）</td>
<td>(默认0.4)不调<br />RS进程的堆内存*本参数 ==&gt; 高水位线</td>
</tr>
<tr>
<td>hbase.regionserver.global.memstore.size.lower.limit<br />或<br />hbase.regionserver.global.memstore.lowerLimit（已过期）</td>
<td>(默认0.95)，设置必须要&gt;=0.9。一般不用管。<br />高水位线*本参数 ==&gt; 低水位线</td>
</tr>
</tbody></table>
<h4 id="HLog级别限制：（重点）"><a href="#HLog级别限制：（重点）" class="headerlink" title="HLog级别限制：（重点）"></a>HLog级别限制：（重点）</h4><p>一个RegionServer上HLog总的大小达到会触发。系统会选取最早的HLog对应的一个或多个Region进行Flush。</p>
<p>涉及参数：hbase.regionserver.maxlogs（默认32）（一个Region Server中HLog数量）</p>
<p>建议：当我们调整了hbase.hregion.memstore.flush.size后，也需要调整这个参数，否则这个参数一样会频繁的触发Flush。本参数建议根据hbase.hregion.memstore.flush.size的调整来设置。具体设置可以参考<a href="https://issues.apache.org/jira/browse/HBASE-14951">HBASE-14951</a></p>
<h4 id="定期Flush"><a href="#定期Flush" class="headerlink" title="定期Flush"></a>定期Flush</h4><p>如果我们很久没有对 HBase 的数据进行更新，hbase会起一个线程flush所有memstore。默认周期为1h。这里的定期有一定的随机延迟（20000左右）。</p>
<p>涉及参数：hbase.regionserver.optionalcacheflushinterval（默认3600000）（单位ms）</p>
<p>建议：建议调大，但这个根据集群的压力来判断，没有一个合适的值，过短的Flush可能会造成小文件问题。</p>
<h4 id="手动触发flush"><a href="#手动触发flush" class="headerlink" title="手动触发flush"></a>手动触发flush</h4><p>flush ‘table_name’ 刷写单个表</p>
<p>flush ‘region_name’ 刷写单个region</p>
<h4 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h4><ol>
<li>hbase.hregion.memstore.flush.size</li>
<li>HBase的堆内存</li>
<li>HLog级别限制。</li>
</ol>
<h3 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h3><p>合并在HBase里分为大合并和小合并：</p>
<ul>
<li>小合并容易发生，且允许发生。小合并主要用来将数量过多的HFile进行合并。（仅仅合并而已）</li>
<li>大文件尽量不要发生。大合并主要是用来处理update(超过版本数)，delete，ddl产生的多余文件。</li>
</ul>
<p>讨论这块的调优还是跟着触发时机来看：</p>
<h4 id="小合并"><a href="#小合并" class="headerlink" title="小合并"></a>小合并</h4><p>触发及处理：同一个Region下，HFile数量过多，读取效率低。类似于小文件合并。选择相邻的一部分HFile文件，合成一个文件。(仅仅合并)</p>
<p>触发条件：</p>
<ul>
<li><p>数量出发</p>
<p>每次memstore级别的flush之后，都要对当前的HFile的<strong>文件数量</strong>进行判断，一旦大于就会触发合并。</p>
<p>涉及参数：hbase.hstore.compactionThreshold(默认3)</p>
<p>建议：一般不用调整，如果写入qps较高比较大，可以略微调高至5左右。调整该参数建议同时调整<strong>hbase.hstore.compaction.max</strong>至同样的倍数</p>
</li>
<li><p>时间触发</p>
<p>到达参数设置的时间后，会进行检查。一旦达到要求。就会触发合并。</p>
<p>涉及参数：hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multiplier</p>
<p>建议：一般不用调整。</p>
</li>
</ul>
<p>其他参数：</p>
<p>hbase.hstore.blockingStoreFiles：默认为10，表示一旦某个store中文件数大于该阈值，就会导致所有更新阻塞。建议逐步调大至100，特别是出现“Too many HFiles, delaying flush”时。</p>
<p>hbase.regionserver.thread.compaction.small：小合并的线程数。默认为1。建议调整为3/5。但不能过大。</p>
<h4 id="大合并"><a href="#大合并" class="headerlink" title="大合并"></a>大合并</h4><p>一句话：避免自动大合并，所以把<strong>hbase.hregion.majorcompaction</strong>设置为0。写个脚本在业务低谷期去定期大合并。</p>
<p>大合并主要清除3类数据：</p>
<ul>
<li>清理TTL：假如table设置了过期时间，大合并会清理已过期的数据</li>
<li>清理put造成的多版本。（版本号超过设定）</li>
<li>清理delete操作造成的多版本。</li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase概念及架构</title>
    <url>/2019/05/01/hbase-gai-nian-ji-jia-gou/</url>
    <content><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><blockquote>
<p>HBase版本：1.2.x</p>
</blockquote>
<h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><p>HBase也是主从结构，本身主要由两部分组成：</p>
<ul>
<li>HMaster：主节点，表相关的工作都会经过Hmaster，包括表的管理、Region的管理。</li>
<li>HRegionServer：从节点，主要关注数据的读写操作</li>
</ul>
<h3 id="表概念"><a href="#表概念" class="headerlink" title="表概念"></a>表概念</h3><blockquote>
<p>HBase中表会进行分布式存储，决定因素主要是：RowKey、Column Family</p>
</blockquote>
<p>HBase中的表中一条数据主要由5部分组成：RowKey、Column Family、Column、Version Number、Value</p>
<ul>
<li>RowKey：主键。将表按照行进行切分。</li>
<li>Column Family：列族。将表进行切割 简称CF。一般会将一类信息存放在一个列族里。（如订单id，订单名称放在订单列族里）</li>
<li>Column：字段名。列。</li>
<li>Version Number：类型为Long，默认值为系统时间戳。表示这条数据的版本。</li>
<li>Value：这条数据的value</li>
</ul>
<h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p>Hbase中的最小单元，是一段数据的集合，类似于HDFS上的Block。Region存放在RegionServer节点上。</p>
<blockquote>
<p>一般RegionSrver要和DataNode在一个节点上。</p>
</blockquote>
<h4 id="物理层面的Region"><a href="#物理层面的Region" class="headerlink" title="物理层面的Region"></a>物理层面的Region</h4><p>一个RegionServer管理1<del>n个Region。一个Region管理者1</del>n个FC。</p>
<h4 id="逻辑层面的Region"><a href="#逻辑层面的Region" class="headerlink" title="逻辑层面的Region"></a>逻辑层面的Region</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194401.png" alt="逻辑层面的Region"></p>
<p>对于一张表：</p>
<ul>
<li>按照rowKey范围（Range），将一张表划分成多个Region。(Region一般会保存在不同的机器上)</li>
<li>Region再按照列族，分成多个Store。MemStore是Flush入文件的一个缓存区域。</li>
</ul>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><h4 id="逻辑视图"><a href="#逻辑视图" class="headerlink" title="逻辑视图"></a>逻辑视图</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194541.png" alt="逻辑视图"></p>
<p>整个数据可以看作一张表，数据按照RowKey，字典排序。HBase不存储null值，整个表是以稀疏表的方式存储。</p>
<h4 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194602.png" alt="物理视图"></p>
<p>每个数据其实本质都是按照k-v进行存储的，不同的CF存储在不同的文件中。（图中左右是两个文件，但其实是一张表里。）</p>
<h5 id="多版本"><a href="#多版本" class="headerlink" title="多版本"></a>多版本</h5><p>上图中，row2存在多版本数据。小王是新的数据。</p>
<p>Hbase中一般只有insert。</p>
<ul>
<li><p>put：insert+update</p>
<ul>
<li><p>rowKey不存在：插入一条新数据</p>
</li>
<li><p>rowKey存在：插入新数据，但保留原来的数据，两者主要是TimeStamp和Value不同。TimeStamp一定程度反映了数据的版本。</p>
<p>hbase会默认返回新版本的数据，当然，老版本的数据也能查得到</p>
</li>
</ul>
</li>
<li><p>delete：插入一条打上的delete标签的数据。并将之前版本的数据都置为不可见。合并时才真正删除数据。</p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Hbase适合写多读少的业务，适合update少的业务（因为update其实是插入等量的数据）</li>
<li>Hbase中，同一张表的数据往往会存放在不同的节点上。</li>
</ul>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217200948.png" alt="HBase架构"></p>
<p>HBase的架构中，有一个至关重要的角色：Zookeeper。</p>
<h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><p><strong>Client的所有操作第一步都是Zookeeper</strong></p>
<p>功能：</p>
<ul>
<li>存储Meta表所在的RS节点。存储Hmaster地址。</li>
<li>RS会主动向ZK注册，使得HMaster可以随时感知RS的健康状态。</li>
<li>构建HMaster的HA，避免单点故障。</li>
</ul>
<h3 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h3><p>表相关的工作都会经过Hmaster，包括：</p>
<ul>
<li>Table的管理：包括建表、修改列族配置等。</li>
<li>Region的管理：分配Region到具体的RegionServer、Region的分割和合并、rs挂的时候，Region的迁移等。</li>
</ul>
<h3 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h3><p>主要负责：数据的路由（数据具体写在哪个Region）、数据的读写、数据的持久化–&gt;数据以HFile存储在HDFS上。</p>
<p>HRegionServer是内容比较多的一个部分。包含：</p>
<ul>
<li>HLog（WAL）：预写日志。</li>
<li>BlockCache：读缓存。</li>
<li>HRegion：一个HRegion只属于一张表。<ul>
<li>Store：由MemStore和StoreFile/Hfile组成。</li>
</ul>
</li>
</ul>
<h4 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h4><ul>
<li>一个HRegionServer上，同一时间只能写一份HLog（但可以存多个HLog）（其实就是HDFS上的一个文件）<ul>
<li>HLog存放在HDFS上，并分成两个文件夹存放：WALs 和 oldWALs。</li>
</ul>
</li>
<li>写数据前，总时先写WAL，再写MemStore。</li>
<li>充当灾难恢复的作用。</li>
<li>HLog中的所有日志记录都已经落盘到HFile，则失效，移入oldWALs文件夹中。</li>
<li>满足确认删除条件，则删除oldWALs下的Hlog<ul>
<li>HLog是否还在参与主从复制。</li>
<li>文件存在于oldWALs时间是否已超过10分钟。</li>
</ul>
</li>
</ul>
<h4 id="BlockCache"><a href="#BlockCache" class="headerlink" title="BlockCache"></a>BlockCache</h4><p>Client的读请求，会将从本RegionServer在Hfile上读的数据写入BlockCache。一个Rs只有一个BlockCache。启动时完成初始化。</p>
<p>读请求时，BlockCache已经缓存的数据，就不用再去Hfile找了。很久没用的数据会被新数据替换掉。</p>
<h4 id="HRegion"><a href="#HRegion" class="headerlink" title="HRegion"></a>HRegion</h4><p>Region由许多许多的Store组成，<strong>不同的列簇会拆成不同的Store</strong>。</p>
<p>Store中，又由MemStore和StoreFile/HFile组成。</p>
<ul>
<li>MemStore：写入缓存。写入数据时，先有序写入该缓存，满了后Flush成为一个HFile文件。<ul>
<li>读的时候也会用！</li>
</ul>
</li>
<li>HFile：其实就是对应于HDFS上的文件。<ul>
<li>HFile本身只有一份，不过HDFS自己会弄成3副本保存。</li>
<li>HFile数量增加到阈值时，会触发Compaction合并。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase的RowKey设计</title>
    <url>/2019/09/30/hbase-de-rowkey-she-ji/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于hbase，会将一张Table中的数据，按照定义的RowKey，将数据切分到不同的Region中。一般不同的Region会被Master分配到不同的机器上。<br>所以如果RowKey设计良好，能够让集群<code>负载均衡，提高吞吐量。防止出现热点问题</code>。</p>
<blockquote>
<p>Hbase版本：1.2.0</p>
</blockquote>
<h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><ol>
<li>RowKey<strong>长度不应该超过16字节</strong>，否则对于HFile和MemStore来说会极大的占用存储空间</li>
<li>RowKey设计时需要保证<strong>唯一性</strong>。当两个rowKey经过变换可能生产同一个结果时，这种设计就是有问题的。</li>
<li>Rowkey是按照<a href="https://zh.wikipedia.org/wiki/%E5%AD%97%E5%85%B8%E5%BA%8F">字典序</a><strong>排序</strong>。设计后请确认rowKey的排序能否符合预期。</li>
<li>RowKey设计的最终目的是<strong>避免热点问题</strong>。设计时需要确认自己的方案对于上游来的数据能够做到均匀分布。<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2>整体来看，有两种情况：</li>
<li>第一种，在RowKey前增加一个<strong>完全随机</strong>的随机前缀。这样简单，也会带来一些坏处，但其实也有解决方案。</li>
<li>第二种，我们可以自己设计RowKey。我们可利用数据中的某1个或者某几个字段，<strong>通过一定的处理，组装成我们的RowKey</strong>。这样尽管设计复杂，但能够带来一些好处，但其也会有一些受限情况。</li>
</ol>
<p>实际上，上述两种情况虽有些许不同，不过本质都是需要随机。随机意味着数据打散，意味着查找变困难。但Hbase本身就是一个重写轻读的系统。</p>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><h3 id="加盐前缀"><a href="#加盐前缀" class="headerlink" title="加盐前缀"></a>加盐前缀</h3><p>盐就是<a href="https://zh.wikipedia.org/wiki/%E7%9B%90_(%E5%AF%86%E7%A0%81%E5%AD%A6)">Salt</a>。<br>我们在创建Hbase表示，就能够人工指定上rowKey的分隔符（而不是等某个Region已经很大了，再让系统自动去划分）：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hbase&gt; create <span class="string">&#x27;ns1:t1&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>, SPLITS =&gt; [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>这样就能够创建出5个Region：（都是左闭右开的区间）<br>【null—a】、【a—b】、【b—c】、【c—d】、【d—null】<br>假如说我们rowKey本来都是：001，002，003。这样就都会分到第一个Region。这显然有问题。<br>通过给我们RowKey加上a-d的前缀，比如说：a-001，b-002。就能够均匀分到设计的<strong>4</strong>个Region了，而不是说基准到某一个Region。<br>ps：<br>实际上，我们如果使用Phoenix来管理hbase，这里会更加的方便。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">table</span> (</span><br><span class="line">a_key <span class="type">VARCHAR</span> <span class="keyword">PRIMARY</span> KEY, </span><br><span class="line">a_col <span class="type">VARCHAR</span></span><br><span class="line">) SALT_BUCKETS <span class="operator">=</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure>
<p>不过phoenix不在本文章讨论范围，以后有机会再弄phoenix相关的。更多的请参考官网链接：<a href="http://phoenix.apache.org/salted.html">Salted Tables</a><br>但加盐前缀解决了数据分布的问题，却也有很多缺点：</p>
<h4 id="加盐前缀的缺点："><a href="#加盐前缀的缺点：" class="headerlink" title="加盐前缀的缺点："></a>加盐前缀的缺点：</h4><ol>
<li>第一个分区永远没有数据。因为我们的加盐是从a开始加的。</li>
<li>因为是完全随机，我们查询时并不知道之前加入的随机数是什么。这个问题不仅仅是查询，还会影响到put和delete指令。<h4 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h4>我们可以在Table的基础上，建立二级索引。即通过在外部维护一张小表，将我们查询查用的字段放到这张表中。这个方案可以使用Phoenix实现，也可以使用ES来实现。<br>Phoenix的实现：<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> INDEX my_index <span class="keyword">ON</span> my_table (v1,v2) INCLUDE(v3)</span><br></pre></td></tr></table></figure>
由于Phoenix不在本文章讨论范围，可以参考：<a href="http://phoenix.apache.org/secondary_indexing.html">Secondary Indexing</a><h4 id="关于Phoenix多说一句"><a href="#关于Phoenix多说一句" class="headerlink" title="关于Phoenix多说一句"></a>关于Phoenix多说一句</h4>尽管本文章不会过多的介绍Phoenix，不过使用phoenix管理盐前缀，还有个十分强大的功能。不感兴趣可以直接跳过。<br>前面的加前缀的方法中提到过，我们需要在代码中手工的拼接成新的rowKey。而如果使用phoenix管理，我们可以省略拼接这部工作，phoenix会帮助我们完成：<br>这里我们新建一张表mySalt，并设计成4个分区：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mysalt (a_key <span class="type">VARCHAR</span> <span class="keyword">PRIMARY</span> KEY, a_col <span class="type">VARCHAR</span>) SALT_BUCKETS <span class="operator">=</span> <span class="number">4</span>;</span><br></pre></td></tr></table></figure>
<p>这是upsert前：所有区域都没有数据</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20191110230534325.png" alt="insert前"></p>
<p>我们向其中upsert 5条数据，实际上他们的rowKey是十分相似的。但通过查看web界面，我们可以知道他们被发往了不同的Region。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">upsert <span class="keyword">into</span> mysalt <span class="keyword">values</span> (<span class="string">&#x27;111&#x27;</span>,<span class="string">&#x27;jone&#x27;</span>);</span><br><span class="line">upsert <span class="keyword">into</span> mysalt <span class="keyword">values</span> (<span class="string">&#x27;112&#x27;</span>,<span class="string">&#x27;jone&#x27;</span>);</span><br><span class="line">upsert <span class="keyword">into</span> mysalt <span class="keyword">values</span> (<span class="string">&#x27;113&#x27;</span>,<span class="string">&#x27;jone&#x27;</span>);</span><br><span class="line">upsert <span class="keyword">into</span> mysalt <span class="keyword">values</span> (<span class="string">&#x27;114&#x27;</span>,<span class="string">&#x27;jone&#x27;</span>);</span><br><span class="line">upsert <span class="keyword">into</span> mysalt <span class="keyword">values</span> (<span class="string">&#x27;115&#x27;</span>,<span class="string">&#x27;jone&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20191110230733649.png" alt="insert 后"><br>这里不过多的展开。</p>
<h3 id="加Hash值"><a href="#加Hash值" class="headerlink" title="加Hash值"></a>加Hash值</h3><p>本方案的核心思路是对本来的RowKey去其Hash值，然后构建成新的RowKey。比如：Hash+RowKey。其实Hash我们取前几位就可以了，不然RowKey会太长了。<br>使用本方法时，建表可以使用官方自带的Hash分区规则：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> create <span class="string">&#x27;t1&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>, &#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; <span class="string">&#x27;HexStringSplit&#x27;</span>&#125;</span></span><br></pre></td></tr></table></figure>
<p>我们在代码中处理时，人为的获取rowKey的hash值，再进行拼接。这样带来了几个好处：</p>
<ol>
<li>根据rowKey，我们能够拿到随即后的值，这样我们就能够快速定位到Region了。</li>
<li>通过这种办法，也能够将数据打散，避免热点问题。</li>
</ol>
<h4 id="加hash值的缺点"><a href="#加hash值的缺点" class="headerlink" title="加hash值的缺点"></a>加hash值的缺点</h4><ol>
<li>只有通过rowKey来索引会快些，我们想<strong>通过其他字段</strong>查找时依旧会面临一样的问题。</li>
<li>原本可能在一块的数据会被打散，这一定程度上会影响查询效率。而实际业务中，查询一块的数据是非常常见的需求。</li>
</ol>
<p>实际上，第二点是无法避免的，毕竟本文的核心就是将原本有规律的rowKey打散。在加盐前缀中，解决方案就是二级索引。其实这个思路是通用的。</p>
<blockquote>
<p>注意：生成Hash时，请使用Hbase自己的工具：org.apache.hadoop.hbase.util.Hash</p>
</blockquote>
<h3 id="更加复杂的RowKey设计"><a href="#更加复杂的RowKey设计" class="headerlink" title="更加复杂的RowKey设计"></a>更加复杂的RowKey设计</h3><p>在Hash值那个方案的缺点里，有提过一个点：我们想<strong>通过其他字段</strong>查找时，有没有办法加速呢？<br>答案是有的。但会有诸多的限制。没有rowKey设计是完美的，本节会指导一个设计思路，而非通用的办法。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>其实设计思路和加Hash值差不多，简单来说，我们需要<code>利用已有字段的数据，人工拼接成出一个RowKey</code>。并且保证这个RowKey是唯一的。因为我们的RowKey里包含了这部分数据，所以我们就能够快速的找到Region。<br>当然会有很多限制，我们在末尾讨论。</p>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>假设我们在生产过程中，需要经常进行下列几种查询：越前面越常用。</p>
<ol>
<li>查询某个用户的订单数据。</li>
<li>获得某个用户最近的订单数据。</li>
<li>查询过去一段时间的订单数据。</li>
</ol>
<p>通过分析，我们知道关键是两个字段：用户，时间。<br>所以我们可以考虑利用用户ID与订单时间，来构建RowKey。比如：<code>[Hash_userID]+[UserID]+[TIME]</code><br>这样，当我们需要使用userId进行查询时，就能够提高查询速度了。</p>
<blockquote>
<p>Hash主要是用来打散数据。因为UserID更加重要，所以我们取UserID的hash值作为前缀。</p>
</blockquote>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>我们知道，RowKey在Region里面的存储默认是按照<strong>字典排序</strong>的，对于我们刚刚创建的RowKey，新的数据反倒会在后面。这在查询时会额外浪费一些时间。所以对于第二种场景，将新数据放在老数据之前是有必要的。</p>
<blockquote>
<p>这里虽然说的是新旧数据，但其实是两条数据，不要和Hbase内部的version机制搞混了。</p>
</blockquote>
<p>所以可以进一步对RowKey中的Time进行处理，可以找一个很大的值，对于本例的时间戳来说，我们可以找Long_MaxValue来做减法，或者你找2050年的时间戳来做减法。得到新的：<code>[USERID_HASH]+[USERID]+[xxxxxx-TIME]</code>，这样就达到了逆序的效果。</p>
<blockquote>
<p>注意：运算后的数据位数可能不一样，可能就达不到我们想要的结果，有必要的情况下，不要忘了补位处理。</p>
</blockquote>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><p>对于第三种场景，这种rowKey设计是无能为力的，我们仍然需要遍历所有Region。</p>
<blockquote>
<p>因为Hbase底部的条件查询其实类似于SQL中的Like ‘XXX%’。现阶段我们无法利用中间的数据完成过滤。</p>
</blockquote>
<p>所以需要根据实际业务来设计Rowkey。本节只是讲述一种非常简单的例子。</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>RowKey的设计，还有很多，比如反转RowKey等等。<br>但归根结底，就是两大类：</p>
<ol>
<li>加盐前缀。前缀与我们的数据毫无关系。这种方案最大的缺点就是会影响查询和数据的删除修改。不过我们可以通过建立二级索引来优化这些问题。Phoenix能够帮助我们更加方便的使用加盐前缀。</li>
<li>根据某个字段配合Hash来设计我们的RowKey。这个字段最好选用我们查询经常会用到的字段，这样能够提升大部分业务的查询速度。</li>
</ol>
<p>我们需要结合我们拿到的数据是什么样的，再去进行设计。<br>归根结底，Hbase是一个重写轻读的系统，设计RowKey的本来目的就是让Hbase能够更好的取写数据。通过将数据打散，肯定会影响读场景的效率。<br>但数据存下来终归是要拿来用的，设计RowKey时也要考虑读场景，尽量优化。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop+Yarn的HA架构解析</title>
    <url>/2018/01/12/hadoop-yarn-de-ha-jia-gou-jie-xi/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问。依赖其的组件也无法正确运转。重启NameNode是耗时的。导致其之前只适用于离线处理场景。</p>
<h2 id="HDFS的HA"><a href="#HDFS的HA" class="headerlink" title="HDFS的HA"></a>HDFS的HA</h2><p>hadoop在2.0版本中，解决了HDFS NameNode 和 YARN ResourceManger的单点问题。<br>整个架构可以参考两张图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222852.png" alt="HDFS的HA"></p>
<p>从图中可以看到，整个HA框架可以分为几个部分：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">两台 NameNode ，一主一备。一个皇上，一个皇上他儿子。</span><br><span class="line">备Name会不断从JournalNode中读取editLog，保持元数据信息与主NameNode一致，并实时与DataNode相互通信，使皇上挂了后，儿子能够正常无缝上位。 </span><br><span class="line">值得注意的是，只有主NameNode才能对外提供读写服务。（3.0支持多个standBy NameNodes了）</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>ZKFailoverController(zkfc)：主备切换控制器。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">作为一个独立进程，对 NameNode 的主备切换进行总体控制。</span><br><span class="line">zkfc会不断检测到 NameNode 的健康状况，定期向zk集群发送心跳信息。</span><br><span class="line">自己被zk选举为active的时候，zkfc进程通过RPC协议调用使NN节点的状态变为active，对外提供实时服务</span><br><span class="line">（顺带一提，yarn中，也有类似的zkfc，不过是作为线程存在的。）</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Zookeeper集群：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">为zkfc的选举切换提供支持。 </span><br><span class="line">（条件允许，可将zookepper配置在独立的机器上，防止部署了zk的机器业务过于繁忙，从而导致nameNode无法及时切换。） </span><br><span class="line">（由于nameNode是通过zk投票选举，zk部署太多也会影响选举效率）</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>DataNode 节点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DataNode 会同时向主备 NameNode 发送心跳信息、数据的块信息 。</span><br></pre></td></tr></table></figure></li>
<li><p>共享存储系统：（本文以JournalNodes为例）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">是实现高可用至关重要的一步。</span><br><span class="line">JournalNodes会保存NameNode 在运行过程中所产生的HDFS元数据，主从NameNode 通过JN实现元数据的同步。</span><br><span class="line">具体来说： </span><br><span class="line">	Active的NameNode的命名空间有任何修改时，会告知大部分的大部分的JournalNodes进程并将EditLog提交到JournalNodes中，</span><br><span class="line">	备NameNode会定期从JN中读取修改记录，并在自己这边进行重演，从而保证主从NameNode 的元数据信息相同</span><br><span class="line">	另外，对于HA集群，保证只有一个NameNode是Active也是非常重要的，否则两个NameNode的数据状态就会产生分歧，所以，JournalNodes必须确保同一时刻只有一个NameNode可以向自己写数据。</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>关于JournalNode</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">运行的JournalNode进程非常轻量，可以部署在其他的服务器上。</span><br><span class="line">注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个。</span><br><span class="line">（在active namenode写数据到journalnode上时，必须有半数以上的journalnode写成功的话，才标志写成功。因此需要奇数。）</span><br></pre></td></tr></table></figure>

<p>参考图：</p>
<h3 id="主备切换流程概述"><a href="#主备切换流程概述" class="headerlink" title="主备切换流程概述"></a>主备切换流程概述</h3><p>主备切换由：ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现。</p>
<ol>
<li>zkfc：<br>作为NameNode上的一个独立进程存在。<br>启动时会创建HealthMonitor和ActiveStandbyElector两个组件。并注册回调方法。</li>
<li>HealthMonitor：<br>负责检测NameNode的健康情况。</li>
<li>ActiveStandbyElector：<br>负责完成自动的主备选举。<br>参考图：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222931.png" alt="主备切换流程概述"><h3 id="NameNode的元数据存储概述"><a href="#NameNode的元数据存储概述" class="headerlink" title="NameNode的元数据存储概述"></a>NameNode的元数据存储概述</h3>NameNode 在执行 HDFS 客户端提交写操作（如：创建文件、移动文件等）的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。<br>内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog <strong>仅仅只是在数据恢复的时候起作用</strong>。EditLog 会被切割为很多段，每一段称为一个 Segment。<br>正在写入的EditLog Segment，其文件名形如 edits_inprogress_{start_txid}。<br>写入完成的EditLog Segment，其文件名形如 edits_{start_txid}-{end_txid}。</li>
</ol>
<p>NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，<br>NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222638.png"></p>
<h3 id="实现主备切换的额外内容"><a href="#实现主备切换的额外内容" class="headerlink" title="实现主备切换的额外内容"></a>实现主备切换的额外内容</h3><p>主备NameNode实际是两个不同的机器。所以对外需要提供一个通用的命名空间服务，才能够在主Name故障时，无感知的完成切换。<br>对于用户或者代码来说，不需要关心主备切换的事情。<br>需要说明的是，nameSpace并<strong>不是一个独立进程</strong>。而仅仅知识core-site/hdfs-site之中的一些配置实现的。<br>通过命名空间的配置，通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs://nameNodeService/</span><br></pre></td></tr></table></figure>
<p>就能够直接访问，而不用去管自己访问的具体是那个NameNode。</p>
<h2 id="Yarn的HA"><a href="#Yarn的HA" class="headerlink" title="Yarn的HA"></a>Yarn的HA</h2><p>Yarn整体思路和Hdfs是差不多的，并且有相当部分的代码重用。不过也有一些区别，顺带记录下。<br>总体而言，数据是不能丢的,但是作业是可以挂的,挂了重启即可。因此YARN的架构比较轻量级。<br>本例中基于ZKRMStateStore。</p>
<p>HA整体框架图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223126.png" alt="Yarn的HA"></p>
<p>从图中可以看到，整个HA框架可以分为几个部分：</p>
<ol>
<li>主备ResourceManager（RM）：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">启动时，向zk的目录（如：/hadoop-ha）写入一个lock文件。成功的成为Active节点。失败则是standBy。 </span><br><span class="line">StandBy的会去监控这个lock文件，如果不存在，则会尝试创建，以成为Active节点。</span><br><span class="line">RM会在Container上启动并监控ApplocationMaster（AM）。</span><br></pre></td></tr></table></figure>

<p>关于RM的本身用途（放一起方便回忆）： </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ResourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统。</span><br><span class="line">NodeManager以心跳的方式向ResourceManager汇报资源使用情况（目前主要是CPU和内存的使用情况）。</span><br><span class="line">RM只接受NM的资源回报信息，对于具体的资源处理则交给NM自己处理。</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>NodeManager（NM）：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a）运行在所有节点上。</span><br><span class="line">b）启动Container容器，用于运行task运算。 </span><br><span class="line">c）Yarn的HA中，NM只会与Active的RM进行通信。 </span><br><span class="line">d）监控容器并上报资源：将Container的情况报告给RM，将Task的处理情况汇报给AM。</span><br></pre></td></tr></table></figure></li>
</ol>
<p>关于Container（放一起方便回忆）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Container（容器）是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。 </span><br><span class="line">Container 是 一种资源抽象，它封装了某个节点上的多维度资源。从而限定每个任务使用的资源量。</span><br><span class="line">一个节点会运行多个Container，但一个Container不会跨节点。 </span><br><span class="line">任何一个 job 或 application 必须运行在一个或多个 Container 中。</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>AppLocationMaster（AM）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a）AM运行在Container上，Container运行在NodeManager之上（The ApplicationMaster is started on a container） </span><br><span class="line">b）负责单个Applocation（Job）的task的资源管理和调度。计算Job的资源需求。  </span><br><span class="line">c）与调度器协商，并向RM进行资源申请。 </span><br><span class="line">d）向NM发出Launch Container请求。并接受NM的task处理状态信息。</span><br></pre></td></tr></table></figure></li>
<li><p>RMStateStore（本例中指：ZKRMStateStore）</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">负责数据交换。  </span><br><span class="line">active的RM会向zk的目录（如：/rmstore）写入Job信息。</span><br><span class="line">active的RM挂掉后，另一个standBy成功转换为Active后，会从该目录读取Job信息。重新构建作业的内存信息，随后启动内部服务。</span><br><span class="line">成为Active后，才会开始接受NM的心跳，接受Client的作业请求。</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>ZKFC：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">作为RM中的一个线程存在。 </span><br><span class="line">负责自动故障转移</span><br></pre></td></tr></table></figure>

<h2 id="HDFS与YARN的HA的主要区别"><a href="#HDFS与YARN的HA的主要区别" class="headerlink" title="HDFS与YARN的HA的主要区别"></a>HDFS与YARN的HA的主要区别</h2><table>
<thead>
<tr>
<th>HDFS</th>
<th>YARN</th>
</tr>
</thead>
<tbody><tr>
<td>standBy会不断从JN上读取修改数据，并进行复现。保证切换迅速</td>
<td>在Active节点挂掉后，才会从zk中读取Job信息。恢复不如HDFS那么及时，故更轻量。</td>
</tr>
<tr>
<td>HDFS HA使用JN集群同步数据</td>
<td>使用存储在Zookeeper中的RMstatestore同步数</td>
</tr>
<tr>
<td>ZKFC为进程</td>
<td>ZKFC为线程</td>
</tr>
<tr>
<td>Active和Standby都要接受来自NN的心跳等信息</td>
<td>只有Active才会接受NM的心跳</td>
</tr>
</tbody></table>
<p>参考：</p>
<ol>
<li><a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager High Availability</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop使用LZO压缩</title>
    <url>/2018/05/17/hadoop-shi-yong-lzo-ya-suo/</url>
    <content><![CDATA[<h3 id="为什么要使用LZO压缩。"><a href="#为什么要使用LZO压缩。" class="headerlink" title="为什么要使用LZO压缩。"></a>为什么要使用LZO压缩。</h3><p>Hadoop目前支持的压缩中，用得多的主要有：Snappy，Bzip2，LZO。</p>
<p>其中：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>Bzip2</td>
<td>压缩率高，可切分。</td>
</tr>
<tr>
<td>LZO</td>
<td>压缩率中，速度快，可建索引来完成分片。</td>
</tr>
<tr>
<td>Snappy</td>
<td>压缩率中，速度快。不可切分。</td>
</tr>
</tbody></table>
<h3 id="为啥使用了lzo仍然不能分片"><a href="#为啥使用了lzo仍然不能分片" class="headerlink" title="为啥使用了lzo仍然不能分片"></a>为啥使用了lzo仍然不能分片</h3><p>在hdfs.xml中，有这样的配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.blocksize&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;134217728&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>这个配置设置了块大小为128M，在MapReduce/Spark的过程中，inputformat执行完毕之后，默认就会根据该配置，对文件进行切块(split)，进而根据块的数量来决定map task的数量。</p>
<p>除了textFile之外，压缩格式中的lzo，bzip2也可以进行文件的切块操作。</p>
<p>但是从一般情况，lzo本身是无法进行切块的——如果直接将大于128M的data.lzo文件作为map的输入时，默认blocksize为128M的情况下，number of splits的值仍然为1，即data.lzo仍然被当为一块直接输入map task。</p>
<p>所以为了实现lzo的切块，需要为lzo的压缩文件生成一个索引文件data.lzo.index。</p>
<h3 id="如何生成lzo文件"><a href="#如何生成lzo文件" class="headerlink" title="如何生成lzo文件"></a>如何生成lzo文件</h3><p><code>lzop -v data</code>，就会生成data.lzo文件</p>
<h3 id="给data-lzo配置索引文件"><a href="#给data-lzo配置索引文件" class="headerlink" title="给data.lzo配置索引文件"></a>给data.lzo配置索引文件</h3><p>需要准备hadoop-lzo-0.4.21-SNAPSHOT.jar，如果没有的话就需要编译生成一下。</p>
<p>1.安装编译所需文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install lzo-devel zlib-devel gcc autoconf automake libtool</span><br></pre></td></tr></table></figure>

<p>2.下载，解压</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure>

<p>3.修改pom.xml，将其中的hadoop.current.version改为自己的hadoop版本<br>4.编译</p>
<p>在hadoop-lzo-master/下执行<code>mvn clean package -Dmaven.test.skip=true</code>进行编译，编译好的jar包在hadoop-lzo-master/target/</p>
<p>5.修改hadoop的配置文件：core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6.重启hadoop集群，将data.lzo丢到hdfs里。</p>
<p>7.创建index文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用mapreduce创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/data.lzo</span><br><span class="line"></span><br><span class="line"># 使用本地程序创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /input/data.lzo</span><br></pre></td></tr></table></figure>

<p>8.执行自己的Spark程序的时候，输入路径为/input而非/input/data.lzo，这样就能实现lzo的分片操作了。</p>
<h3 id="什么时候选择LZO？"><a href="#什么时候选择LZO？" class="headerlink" title="什么时候选择LZO？"></a>什么时候选择LZO？</h3><p>这个主要看离线任务的粒度。因为创建索引需要耗时的。数据越大，耗时也越长。</p>
<p>如果是以天为单位，一般是具有创建索引时间，而创建索引后，Spark就能直接并行读取这批数据。而我们不用去关注文件的大小是否合适</p>
<p>如果以小时甚至分钟为单位，一般不具备创建索引的时间，这时候就需要控制单个文件的大小，具体选择需要进行测试选择一个能接受的大小，或者也可以让单个文件&lt; Block（我们测试后认为0.7*Block是一个比较合适的值），这样能提供和切片后一样的性能，但元数据也会有明显的负担。这种粒度选择Snappy与LZO区别并不是很大。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase的读写流程</title>
    <url>/2019/05/18/hbase-de-du-xie-liu-cheng/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>HBase虽然也是分布式架构，但相比于HDFS，读写流程有点特殊，简单来说读写操作不需要走Master。Master只负责涉及表结构的变更。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217203305.png" alt="读写流程"></p>
<h2 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h2><ol>
<li><p>zookeeper：</p>
<p>去ZK拿到/hbase/meta-region-server。（<strong>这张表误删后，重启hbase能够自动生成。</strong>）这里面存储着hbase:meta这张表维护在那台RS节点上。（一个集群中，只有一台RS上会有这张表，这张表也是HDFS上的文件）</p>
<p>注：早期Hbase时-root-这张表。并且会维护多张子表（3层结构）</p>
</li>
<li><p>RegionServerA：</p>
<p>向RS节点获取hbase:meta表的内容，写数据根据RowKey，去meta表找到对应的RS节点和Region。（<strong>hbase:meta表误删后，能够通过命令进行恢复</strong>。）</p>
<p>这部分元数据会缓存到本地，供之后的读写请求使用。</p>
<p>meta表存储了table，region，startKey，EndKey，RS节点的映射关系。ROW的格式：table,[start key],xxxxxx.xxxxxx(Region创建时间.RegionID)</p>
</li>
<li><p>对应的RegionServerB：</p>
<p>将写数据请求发送给对应的RS节点。</p>
<p>1）先写Hlog(WAL)</p>
<p>2）再写对应的Region列族的MemStore。</p>
</li>
<li><p>Flush：</p>
<p>MemStore容量超过阈值，会异步Flush，将内存数据写入文件，为StoreFile（HFile）</p>
</li>
</ol>
<h2 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h2><ol>
<li><p>zookeeper：</p>
<p>拿到/hbase/meta-region-server（与写流程一样，找到hbase:meta在哪）</p>
</li>
<li><p>RegionServerA：</p>
<p>向RS节点获取hbase:meta的内容。根据RowKey，去meta表找到对应的RS节点和Region。</p>
</li>
<li><p>RegionServerB：</p>
<p>将读请求进行封装。与RS建立通信。构建Scanner。查找步骤：</p>
<ol>
<li>先去MemStore找。</li>
<li>再去BlockCache上找。</li>
<li>再去HFile找。</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM的运行时数据区</title>
    <url>/2019/02/02/jvm-de-yun-xing-shi-shu-ju-qu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>JDK8中，JVM的运行时数据区逻辑上由5个部分组成：</p>
<ul>
<li>程序计数器（PC Register）</li>
<li>JVM栈</li>
<li>本地方法栈</li>
<li>堆</li>
<li>方法区</li>
</ul>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218111628.png" alt="逻辑分布"></p>
<hr>
<h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>主要功能：指向当前线程字节码文件执行到哪了。直白来说，就是指向当前线程执行到哪行代码了。</p>
<p>独有or共享：每个线程独有</p>
<p>相关异常：一般无异常</p>
<h3 id="JVM栈"><a href="#JVM栈" class="headerlink" title="JVM栈"></a>JVM栈</h3><p>主要功能：每个方法在执行时会创建一个<strong>栈帧</strong>。方法的调用和完成，相当于栈帧的入栈和出栈。</p>
<p>​    栈帧主要存放：<strong>局部变量表</strong>。操作栈，动态链接、方法出口。</p>
<p>​    局部变量表：</p>
<pre><code>    - 编译期可知的各种**基本数据类型**（原生类型）（boolean、byte、char、short等等）
    - **对象引用**（引用类型）（User A = new User的这个A。）
    - returnaddress类型。
</code></pre>
<p>独有or共享：每个线程独有</p>
<p>相关异常：</p>
<ul>
<li>StackOverFlow（栈溢出）：出现于<strong>单线程阶段</strong>，一般是<strong>死循环、递归</strong>导致的方法过多。也可能是单个栈帧过大导致的。<pre><code>- 解决：检查代码，或者调大栈内存（与多线程需要平衡）
</code></pre>
</li>
<li>OutOfMemoryError（OOM/内存溢出）：出现于<strong>多线程阶段</strong>，栈动态扩展时（新开线程），无法申请到足够的内存。一般可能是单个栈的内存占用过多，<strong>考虑减少堆或栈的内存来换取更多的线程。</strong><pre><code>- 提示：unable to create new native thread
</code></pre>
<ul>
<li>参数：<pre><code>- -Xmx 和 -Xms。控制堆占空间的上下限
- -Xss。**线程栈**的大小。不过过小的栈内存可能导致栈溢出。
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>主要功能：基本与JVM栈一样（独享、异常也一样），不过是服务的方法不一样：为虚拟机使用的Native方法服务。（一般会调用非java代码）</p>
<hr>
<h3 id="堆区"><a href="#堆区" class="headerlink" title="堆区"></a>堆区</h3><p>主要功能：存放对象实例。几乎所有的对象实例都在这里分配内存。是垃圾收集器的主要管理区域。</p>
<p>独有or共享：共享。</p>
<p>相关异常：OutOfMemoryError（OOM/内存溢出）</p>
<pre><code>- 一般是**大量的对象**占用了对空间，且未能及时回收。或者创建的一个对象超过了堆允许的最大值。
- 提示：Java heap space
- 参数：-Xmx 和 -Xms。控制堆占空间的上下限。
- 注：出现这种溢出，首先应该考虑代码问题。在Spark中，如果某个算子将过大对象返回给Driver，就可能遇到这种情况。
</code></pre>
<p>​    </p>
<h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><blockquote>
<p>（1.7以前通过永久代实现，1.8+通过元数据区实现）</p>
</blockquote>
<p>主要功能：编译后的Class文件会加载到方法区。主要存放：类的信息（代码）、运行时常量信息（即运行时常量池）。</p>
<p>独有or共享：共享。</p>
<p>相关异常：OutOfMemoryError（OOM/内存溢出）</p>
<pre><code>- 一般出现于系统不断生成**新的类**，而不回收。（注意是类，而非对象。）
- 提示：MetaSpace（1.8+）
- 参数：-XX:MaxMetaspaceSize（不设置则仅受系统内存限制）
</code></pre>
<h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><blockquote>
<p>（1.8+后，物理上，不同的JVM实现不同，有的放在了堆上）（逻辑上仍然是方法区的一部分）</p>
</blockquote>
<p>本质上属于方法区的一块。（线程共享，异常与方法区处理类似（增大方法区空间）），存放<strong>编译期</strong>生成的各种字面量和符号引用。</p>
<h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>并非运行时数据区的一部分。相当于直接使用系统内存，而非JVM内存。主要是NIO类会直接使用系统内存（一种基于Channel的IO方式）</p>
<p>相关异常：一般是过度的使用堆外内存造成的。</p>
<p>参数：-XX:MaxDirectMemorySize。实际使用时，应该限制堆外内存的使用。如果未设置，该数值=堆的最大值</p>
<hr>
<h3 id="关于方法区"><a href="#关于方法区" class="headerlink" title="关于方法区"></a>关于方法区</h3><p><strong>方法区</strong>在不同的JDK版本中有不同的实现：</p>
<ul>
<li>JDK7以前：永久代，存放在堆上。</li>
<li>JDK8：原空间，存储在堆外内存上。</li>
<li>方法区中的<strong>运行时常量池</strong>物理上的存放位置并没有跟着方法区走，在JDK8中，其存放在Java堆上。</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>运行时数据区主要是<strong>逻辑上的分区</strong>。具体实现上会有一些区别（其实主要体现在方法区上。）</li>
<li><strong>栈管运行，堆管存储</strong>。两者都使用JVM内部虚拟内存。。只有方法区的实现源空间（MetaSpace）会使用JVM外部的物理内存。</li>
<li>一般涉及多线程的情况会导致OOM（如线程共享的区OOM，线程独立的区在多线程情况下回OOM）。单线程一般会栈溢出。PC计数器是唯一没有异常的区域。</li>
</ol>
<p>参考图：</p>
<img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218113235.png" alt="运行时数据区"  />]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM中的垃圾回收</title>
    <url>/2019/02/18/jvm-zhong-de-la-ji-hui-shou/</url>
    <content><![CDATA[<h2 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h2><h3 id="JVM的垃圾回收针对哪些内容"><a href="#JVM的垃圾回收针对哪些内容" class="headerlink" title="JVM的垃圾回收针对哪些内容"></a>JVM的垃圾回收针对哪些内容</h3><p>Java堆、方法栈。</p>
<h3 id="如何定位垃圾"><a href="#如何定位垃圾" class="headerlink" title="如何定位垃圾"></a>如何定位垃圾</h3><ol>
<li><p>引用计数法（已经基本不用）</p>
<p>每个对象都有一个引用计数器。被引用是计数器+1。引用失效时，计数器-1。回收计数器为0的对象。</p>
<p>存在问题：两个对象相互引用时，无法回收。</p>
</li>
<li><p>跟搜索法</p>
<p>以一系列，名为GC Root的对象作为起始点。GCRoot无法到达的对象（无引用关系的对象），即为可被回收的。</p>
<p>GCRoot是一种对象，可能是：</p>
<ul>
<li>JVM栈中的引用的对象。</li>
<li>方法区中静态属性（Static）引用的对象。</li>
<li>等等….</li>
</ul>
</li>
</ol>
<h3 id="方法区回收了什么"><a href="#方法区回收了什么" class="headerlink" title="方法区回收了什么"></a>方法区回收了什么</h3><ol>
<li>废弃的常量。如没有任何String对象引用的String常量。</li>
<li>无用的类：同时满足：<ol>
<li>Java堆中不存在该类的任何实例对象；</li>
<li>加载该类的类加载器（ClassLoader）已经被回收；</li>
<li>该类对应的java.lang.Class对象不在任何地方被引用，且无法在任何地方通过反射访问该类的方法。</li>
</ol>
</li>
</ol>
<h3 id="垃圾清除算法"><a href="#垃圾清除算法" class="headerlink" title="垃圾清除算法"></a>垃圾清除算法</h3><p>主要由三种算法，在垃圾回收中会结合起来使用。</p>
<h4 id="标记清除"><a href="#标记清除" class="headerlink" title="标记清除"></a>标记清除</h4><p>标记出需要回收的对象。统一回收标记了的对象。</p>
<p>缺点：清除后，存活对象所占的内存位置不变，会产生<strong>内存碎片</strong>。new大对象可能会触发full GC</p>
<h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><p>将整个内存空间均分为两块，同时只使用一块。GC时，存活对象复制到另一块区中，清空正在使用块中的所有对象。这样就解决了内存碎片问题，回收只需要移动堆顶指针，效率高。</p>
<p>缺点：只能使用一半的内存空间。</p>
<h4 id="标记整理"><a href="#标记整理" class="headerlink" title="标记整理"></a>标记整理</h4><p>在标记-清除的基础上。将存活对象全部移动到一侧。</p>
<p>缺点：效率低。</p>
<hr>
<h2 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h2><p>垃圾回收器中整体分为老年代和新生代。新生代存放比较新的对象，这部分对象的特点就是经常被回收。老年代的对象一般是很久都没被回收的对象。</p>
<p>默认情况下，内存比率：老年代:新生代=2:1。新生代内存比率：eden:s0:s1 = 8:1:1。</p>
<p>新对象会new在eden区。每次新生代回收后，存活的对象年龄+1，进入s0/s1区。年龄到15后，进入老年代。</p>
<p>一般来说，老年代和新生代会采用不同的垃圾回收器。JDK7以后引入了G1回收，会在最后单独讨论。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218114634.png" alt="垃圾回收对应关系"></p>
<h3 id="新生代的垃圾回收器"><a href="#新生代的垃圾回收器" class="headerlink" title="新生代的垃圾回收器"></a>新生代的垃圾回收器</h3><h4 id="Serial（单线程）"><a href="#Serial（单线程）" class="headerlink" title="Serial（单线程）"></a>Serial（单线程）</h4><p>基于复制算法。GC靠单线程完成工作。触发时，会<strong>stw</strong>，导致其他工作线程暂停。</p>
<p>开启：-XX:UseSerialGC（同时老年代会采用Serial Old（单线程的老年代版本））</p>
<h4 id="ParNew（多线程版Serial）"><a href="#ParNew（多线程版Serial）" class="headerlink" title="ParNew（多线程版Serial）"></a>ParNew（多线程版Serial）</h4><p>基于复制算法。GC靠多线程完成工作。触发时，会<strong>stw</strong>，导致其他工作线程暂停。</p>
<p>开启：-XX:+UseParNewGC（同时老年代会采用Serial Old（注意，单线程））</p>
<p>注：但可以和CMS配合使用（需要通过老年代开启CMS收集器，这边开启不行）</p>
<h4 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h4><p>基于复制算法。可看作ParNew的加强版。<strong>重视吞吐量</strong>的垃圾收集器。吞吐量 = 运行代码时间/ (运行代码时间+GC时间)，适合后台运算而无需太多交互的任务。<strong>是JDK8的默认新生代收集器</strong>。触发时，会“stw”，导致其他工作线程暂停。</p>
<p>开启：+XX:UseParallelGC（老年代会使用Parallel Old（Parallel Scavenge的老年代版本））</p>
<h3 id="老年代的垃圾回收器"><a href="#老年代的垃圾回收器" class="headerlink" title="老年代的垃圾回收器"></a>老年代的垃圾回收器</h3><h4 id="Serial-old"><a href="#Serial-old" class="headerlink" title="Serial old"></a>Serial old</h4><p>使用标记整理算法。单线程的。主要是作为CMS的后备方案使用。</p>
<p>开启参数JDK8已移除。</p>
<h4 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h4><p>基于标记整理算法。其他基本同：Parallel Scavenge收集器</p>
<p>开启：-XX:+UseParallelOldGC（会同时让新生代开启Parallel Scavenge收集器）</p>
<h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h4><p>基于标记清除算法。分为4大阶段：（1,3阶段速度快，会触发stw）</p>
<pre><code>- 初始标记：仅标记GC ROOT能直接关联的对象。速度很快，会触发stw。
- 并发标记：进行GC Root Tracing过程（即跟搜索过程）。速度慢，不会触发stw。
- 重新标记：修改并发标记期间，由于程序运行额外产生的变化。速度快，会触发stw。
- 并发清除：清除工作。速度慢，不会触发stw。
</code></pre>
<p>重视短暂停的垃圾处理器。可以近似看做是并发的垃圾处理器（用户进程和GC进程能够同时运行）</p>
<p>开启：-XX:+UseConcMarkSweepGC（会同时让新生代采用ParNewGC）（同时让老年代采用Serial Old作为备选GC）</p>
<p>缺点：</p>
<ul>
<li>GC时耗费CPU性能。</li>
<li>无法处理浮动垃圾（即GC处理过程中产生的垃圾）</li>
<li>会产生大量空间碎片。可通过参数缓解：<ul>
<li>-XX:Usecmscompactfullcollection：Full GC后会进行碎片整理</li>
<li>-XX:CMSFULLGCSBeforeCompaction：多少次FullGC进行碎片整理。</li>
</ul>
</li>
</ul>
<h3 id="G1垃圾回收器"><a href="#G1垃圾回收器" class="headerlink" title="G1垃圾回收器"></a>G1垃圾回收器</h3><h4 id="区域划分"><a href="#区域划分" class="headerlink" title="区域划分"></a>区域划分</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218120617.png" alt="G1垃圾回收器"></p>
<p>G1将整个内存区域划成等大小的块。一般约为2000个。有些块是年轻代的Eden、survivor，有些则是老年代。（当前块所属的代并不是确定的，可能现在是Eden，明天就是老年代，后天就是survivor）</p>
<p>块之间是不相连的，意味这一个对象不能跨块保存。垃圾回收是按照区域来回收的，即回收区域A的同时，不会回收区域B。（不过在一整个GC中，可能回收多个区域）</p>
<h4 id="回收阶段划分"><a href="#回收阶段划分" class="headerlink" title="回收阶段划分"></a>回收阶段划分</h4><ol>
<li><p>年轻代的回收</p>
<p>所有的eden区满后，触发young GC。young GC会回收eden区，上次存放存活对象的survivor区。young GC后，两个区中存活的对象会被copy到一个新的survivor区，eden区则会被清空。达到年龄阈值的对象则会被送往老年区。<strong>年轻代的回收会stw</strong></p>
</li>
<li><p>并发标记周期</p>
<p>当对象占用空间的总和，达到整个堆的大小的45%时触发该阶段。（45%可调整）</p>
<p>标记周期分为6个步骤：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>重要特点</th>
</tr>
</thead>
<tbody><tr>
<td>初始标记</td>
<td>标记从GC Root<strong>直接可达</strong>的对象。触发这个步骤时，会<strong>伴随一个youngGC</strong>。</td>
<td>需要stw</td>
</tr>
<tr>
<td>根区域扫描</td>
<td>扫描由survivor区域直接可达的老年代区域。并标记这些直接可达的对象。<br />该步骤必须在下一次youngGC前完成（即下一次youngGC会等待该步骤完成再进行。）</td>
<td>不需要stw<br />必须在下一次youngGC前完成</td>
</tr>
<tr>
<td>并发标记</td>
<td>扫描<strong>整个堆中</strong>的存活对象，并标记。<br />该步骤可以被young GC打断。</td>
<td>不需要stw<br />可被youngGC打断</td>
</tr>
<tr>
<td>重新标记</td>
<td>重新标记并发标记期间产生的变化。<br />采用SATB算法（初始快照-G1标记之初为存货对象创建了一个快照。）</td>
<td>需要stw</td>
</tr>
<tr>
<td>独占清理</td>
<td>1）计算每个区域中，存活对象与GC回收对象的比例，并排序，识别出可供混合回收的区域。<br />2）标记需要混合回收的区域。<br />3）更新记忆集(RSet)</td>
<td>需要stw<br />混合回收阶段依靠与这步的标记</td>
</tr>
<tr>
<td>并发清理</td>
<td>识别清理完全空闲的区域。</td>
<td>不需要stw</td>
</tr>
</tbody></table>
<p>并发标记的结果和目的：</p>
<pre><code>- 并发标记的结果时增加了一些标记为G的区域。这些区域的内部垃圾占比高。会在之后的混合回收中清除。
- 这些G区域，被G1记录在Collection Sets（回收集）中。
- 并发标记阶段，G1并不会回收老年代的垃圾，只是为之后的混合回收做好标记。
</code></pre>
</li>
<li><p>混合回收</p>
<p>混合回收会执行多次，每次也会伴随youngGC，也会有OldGC。可以理解为这个阶段，mixedGC（youngGC+oldGC）替代了youngGC。</p>
<p>mixedGC后：eden区必被清空，存活对象会前往survivor（youngGC），标记为G的分区会被选取一些来清空。存活对象会移到其他没被标记为G的老年代分区。</p>
</li>
<li><p>Full GC：</p>
<p>当混合GC时空间不足，或youngGC时survivor区和老年代无法容纳幸存对象时，会触发FullGC。FullGC是单线程的stw阶段</p>
</li>
</ol>
<h4 id="常用参数"><a href="#常用参数" class="headerlink" title="常用参数"></a>常用参数</h4><ul>
<li>-XX:+UseG1GC：开启G1回收器。</li>
<li>-XX:MaxGCPauseMillis：目标最大停顿时间。过小可能增加fullGC的可能性。</li>
<li>-XX:ParallelGCThreads：并行回收时，GC的工作现场数量。</li>
<li>-XX:InitiatingHeapOccupancyPercent：触发并发标记的使用率。默认为45（即堆的使用率达到45%时触发。）</li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Java的ServiceLoader使用</title>
    <url>/2021/03/07/java-de-serviceloader-shi-yong/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s中的POD</title>
    <url>/2021/05/31/k8s-zhong-de-pod/</url>
    <content><![CDATA[<h2 id="何为POD"><a href="#何为POD" class="headerlink" title="何为POD"></a>何为POD</h2><blockquote>
<p>pod是一组并置的容器，是k8s中的基本构建模块。</p>
</blockquote>
<p>一个pod可以包含多个容器。不过一般建议一个pod只有一个容器。原因会在后续详细说明。同时一个pod绝对不会跨越多个工作节点。</p>
<h3 id="了解POD"><a href="#了解POD" class="headerlink" title="了解POD"></a>了解POD</h3><h4 id="多容器好与单容器多进程"><a href="#多容器好与单容器多进程" class="headerlink" title="多容器好与单容器多进程"></a>多容器好与单容器多进程</h4><ul>
<li>当在一个容器中运行多个我们的应用时，获取日志，管理重启都需要通过内部解决。</li>
<li>无法通过k8s直接管理某一个应用的生命周期，同时也会影响缩扩容。</li>
</ul>
<h4 id="同一POD中容器之间的隔离与共享"><a href="#同一POD中容器之间的隔离与共享" class="headerlink" title="同一POD中容器之间的隔离与共享"></a>同一POD中容器之间的隔离与共享</h4><p>POD的目标就是让不同容器中的进程<em>好像</em>运行在单个容器中，却又同时保持着一定的隔离。每个容器组（POD）内的容器能共享一些资源。</p>
<ul>
<li>一个POD内容器共享相同的Linux命名空间。（都在相同的network、IPC和UTS命名空间下运行）<ul>
<li>共享相同的主机名和网络接口。</li>
<li>能够通过IPC进行通信。</li>
<li>容器可以通过localhost与同一pod的其他容器通信。</li>
</ul>
</li>
<li>每个容器的文件系统与其他容器完全隔离。如果要共享需要使用Volume</li>
</ul>
<h3 id="POD间的平坦网络"><a href="#POD间的平坦网络" class="headerlink" title="POD间的平坦网络"></a>POD间的平坦网络</h3><ul>
<li>POD之间没有NAT网关。就算是不同的工作节点也是这样。</li>
<li>每个POD都可以通过其他POD的ip地址来相互访问。</li>
</ul>
<p>POD是<strong>逻辑主机</strong>，其行为与非容器世界中的物理主机或虚拟机非常相似。</p>
<h2 id="合理管理容器"><a href="#合理管理容器" class="headerlink" title="合理管理容器"></a>合理管理容器</h2><blockquote>
<p>我们应该将应用程序组织到多个POD中，每个POD只包含紧密相关的组件或进程。</p>
</blockquote>
<ul>
<li>将多层应用分散到多个POD<ul>
<li>前端和数据库并不需要运行在一个POD中，特别是一个POD只能运行在一台物理工作节点上，如果将前端与数据库分开，能够充分利用资源。</li>
</ul>
</li>
<li>基于缩扩容考虑<ul>
<li>POD是K8s中缩扩容的基本单位。</li>
<li>前后端会有完全不同的扩缩容需求。</li>
</ul>
</li>
</ul>
<h3 id="何时在一个POD中使用多个容器"><a href="#何时在一个POD中使用多个容器" class="headerlink" title="何时在一个POD中使用多个容器"></a>何时在一个POD中使用多个容器</h3><ul>
<li>这些容器必须在一台主机上运行。</li>
<li>缩扩容是否是同步的。</li>
</ul>
<blockquote>
<p>容器不应该包含多个进程，POD也不应该包含多个并不需要运行在同一主机上的容器</p>
<p>即最理想：一个POD=一个容器=一个进程。</p>
</blockquote>
<h2 id="使用yaml创建POD"><a href="#使用yaml创建POD" class="headerlink" title="使用yaml创建POD"></a>使用yaml创建POD</h2><p>POD的定义由下属几个部分组成：</p>
<ul>
<li>kind: Pod</li>
<li><em>metadata</em>: 包含POD的名称、命名空间、标签等元数据信息。 </li>
<li><em>spec</em>: 包括POD内容的实际说明，例如容器、卷和其他信息。</li>
<li><em>status</em>:POD的当前状态，构建时不需要。</li>
</ul>
<h2 id="用标签组织POD"><a href="#用标签组织POD" class="headerlink" title="用标签组织POD"></a>用标签组织POD</h2><p>通过metadata里的labels项，可以在创建pod时指定标签。一个pod可以有多个标签。且key可自由指定。</p>
<p>也可以为标签分类工作节点。</p>
<h3 id="标签的用途"><a href="#标签的用途" class="headerlink" title="标签的用途"></a>标签的用途</h3><ul>
<li>使用标签筛选POD。</li>
<li><strong>使用标签和选择器约束POD调度</strong><ul>
<li>通过给工作节点赋予标签。觉得某个工作节点适合做什么</li>
<li>在创建POD时，在<code>spec.nodeSelector</code>中就可以指定这个pod要创建在哪一类工作节点上。</li>
<li>这样做的好处是让工作节点处理适合的事情，同时避免由于某个节点挂掉导致pod无法更换节点重启。</li>
</ul>
</li>
</ul>
<h2 id="命名空间"><a href="#命名空间" class="headerlink" title="命名空间"></a>命名空间</h2><blockquote>
<p>用于对资源进行分组</p>
</blockquote>
<h3 id="为何需要命名空间"><a href="#为何需要命名空间" class="headerlink" title="为何需要命名空间"></a>为何需要命名空间</h3><blockquote>
<p>类似于数据库中的库概念</p>
</blockquote>
<p>可以通过命名空间，我们能够将不属于一组的资源分到不重叠的组中。这样多用户使用同一个k8s集群时，就能各自管理自己的资源集合，防止无意删除或修改其他用户的资源。</p>
<h3 id="创建和使用命名空间"><a href="#创建和使用命名空间" class="headerlink" title="创建和使用命名空间"></a>创建和使用命名空间</h3><ul>
<li>使用<code>kubectl create namespace</code>命令创建命名空间</li>
<li>使用<code>metadata.namespace: custom-ns</code>或<code>kubectl create xxx -n custom-ns</code>来指定将对象创建在哪个命名空间。</li>
</ul>
<h3 id="命名空间提供的隔离"><a href="#命名空间提供的隔离" class="headerlink" title="命名空间提供的隔离"></a>命名空间提供的隔离</h3><ul>
<li>命名空间之间并不提供对正在运行的对象的任何隔离。</li>
</ul>
]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中awk与sed指令的使用笔记</title>
    <url>/2018/02/17/linux-zhong-awk-yu-sed-zhi-ling-de-shi-yong-bi-ji/</url>
    <content><![CDATA[<p>最近工作要使用到这对命令进行脚本编写，使用感觉特别容易忘，故记录整理下。</p>
<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>简单来说，awk是把文本逐行读入，默认以空格为列分隔符进行切片。对切好对数据再进行处理。<br>本文会再学习实验的同时，简单介绍一些常用的awk命令。</p>
<h3 id="使用方法："><a href="#使用方法：" class="headerlink" title="使用方法："></a>使用方法：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;&#123;pattern + action&#125;&#x27; &#123;filename&#125;</span><br></pre></td></tr></table></figure>
<p>pattern表示awk在数据中查找对内容。<br>action表示找到后执行什么命令。<br>{}用来对一系列指令进行分组。</p>
<h3 id="调用途径："><a href="#调用途径：" class="headerlink" title="调用途径："></a>调用途径：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1)命令行</span></span><br><span class="line">awk [-F field-separator] &#x27;commands&#x27; input-files</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">2)shell脚本</span></span><br><span class="line"><span class="meta">#</span><span class="bash">awk命令解释器作为脚本首行，相当于将<span class="comment">#!/bin/bash =&gt; #!/bin/awk</span></span></span><br></pre></td></tr></table></figure>
<h3 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h3><p>首先，准备一个文本格式对文件（a1）。以空格分开。<br>再准备一个文件b1，以【,】作为分隔符<br>比如说a1对内容：第三行会多处一列。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217214320.png" alt="awk"></p>
<h4 id="1）区分行与列，打印数据"><a href="#1）区分行与列，打印数据" class="headerlink" title="1）区分行与列，打印数据"></a>1）区分行与列，打印数据</h4><p>1）打印a1所有数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;&#123;print&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">输出：</span></span><br><span class="line">first line 3 4</span><br><span class="line">second second 3 4</span><br><span class="line">hadoop 3 4 50</span><br></pre></td></tr></table></figure>
<p>2）打印a1第一列，第二列数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;&#123;print $1,$2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">输出：</span></span><br><span class="line">first line</span><br><span class="line">second second</span><br><span class="line">hadoop 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意,如果不加[,]</span></span><br><span class="line">awk &#x27;&#123;print $1$2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">则输出</span></span><br><span class="line">firstline</span><br><span class="line">secondsecond</span><br><span class="line">hadoop3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意,如果不加[$]</span></span><br><span class="line">awk &#x27;&#123;print $1 2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">则输出</span></span><br><span class="line">first2</span><br><span class="line">second2</span><br><span class="line">hadoop2</span><br></pre></td></tr></table></figure>
<p>3）打印第1列第1+行数据：使用内置的NR变量。（文章结尾会总结下常用的内置变量）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;NR&gt;1&#123;print $1&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">second</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure>
<p>4）如果需要打印第2+列，就不那么友好了，不过也可以使用循环来完成：<br>打印第2+列的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;&#123;for(i=1;i&lt;=2;i++)$i=&quot;&quot;;print&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">  3 4</span><br><span class="line">  3 4</span><br><span class="line">  4 50</span><br></pre></td></tr></table></figure>
<h4 id="2）分隔符-正则查找"><a href="#2）分隔符-正则查找" class="headerlink" title="2）分隔符/正则查找"></a>2）分隔符/正则查找</h4><p>awk默认是识别空格为列分隔符，不过比如csv使用逗号来进行风格的，显然需要指定分隔符。这里会使用：-F fs。-F就是指定输入文件分隔符，fs可以是一个字符串，也可以是一个正则表达式</p>
<p>b1内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1,2,3,4,5,6</span><br><span class="line">a,b,c,d,e,f,g</span><br><span class="line">a,s,d,f,g,h,j,k</span><br><span class="line">a,a,a,a,a,a,a,a</span><br><span class="line">b,b,b,b,b,b,b,b</span><br></pre></td></tr></table></figure>

<p>1）读取以逗号为分隔符的b1文件，打印第2，3行，第1，2列：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk -F &#x27;,&#x27; &#x27;NR&gt;1 &amp;&amp; NR&lt;4&#123;print $1,$2&#125;&#x27; b1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">a b</span><br><span class="line">a s</span><br></pre></td></tr></table></figure>
<p>2）新建c1：文本内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a,1,ok</span><br><span class="line">b,2,no</span><br><span class="line">c,3,ioki</span><br></pre></td></tr></table></figure>
<p>找到ok，并打印第1列：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk -F &#x27;,&#x27; &#x27;/ok/&#123;print $1&#125;&#x27; c1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">a</span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<h4 id="awk常用内置变量"><a href="#awk常用内置变量" class="headerlink" title="awk常用内置变量"></a>awk常用内置变量</h4><table>
<thead>
<tr>
<th align="left">变量</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ARGC</td>
<td align="left">命令行参数个数</td>
</tr>
<tr>
<td align="left">ARGV</td>
<td align="left">命令行参数排列</td>
</tr>
<tr>
<td align="left">ENVIRON</td>
<td align="left">支持队列中系统环境变量的使用</td>
</tr>
<tr>
<td align="left">FILENAME</td>
<td align="left">awk浏览的文件名</td>
</tr>
<tr>
<td align="left">FNR</td>
<td align="left">浏览文件的记录数</td>
</tr>
<tr>
<td align="left">FS</td>
<td align="left">设置输入域分隔符，等价于命令行 -F选项</td>
</tr>
<tr>
<td align="left">NF</td>
<td align="left">浏览记录的域的个数</td>
</tr>
<tr>
<td align="left">NR</td>
<td align="left">已读的记录数</td>
</tr>
<tr>
<td align="left">OFS</td>
<td align="left">输出域分隔符</td>
</tr>
<tr>
<td align="left">ORS</td>
<td align="left">输出记录分隔符</td>
</tr>
<tr>
<td align="left">RS</td>
<td align="left">控制记录分隔符</td>
</tr>
<tr>
<td align="left">1）使用：读取文件a1，统计文件名，行号，列数</td>
<td align="left"></td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;&#123;print &quot;name:&quot;FILENAME&quot;,line:&quot;NR&quot;,cols:&quot;NF&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">name:a1,line:1,cols:4</span><br><span class="line">name:a1,line:2,cols:4</span><br><span class="line">name:a1,line:3,cols:4</span><br></pre></td></tr></table></figure>
<h4 id="结合其他命令行"><a href="#结合其他命令行" class="headerlink" title="结合其他命令行"></a>结合其他命令行</h4><p>awk可以结合其他命令行返回第结果进行处理。通过|风格。awk第用法和上面将第是一样的。<br>1）利用awk统计最近5个登陆的用户的用户名和ip或主机名。（last后面不加指令)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">last | awk &#x27;NR&lt;5&#123;print $1,$3&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br></pre></td></tr></table></figure>

<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed是一种在线编辑器，每次处理一行内容。它将处理的行放入临时缓冲区，然后使用sed命令处理缓冲内容。默认会将处理好的内容发往屏幕，可以通过重定向，将内容输出至文件。<br>sed主要用来自动编辑1～多个文件。<br>特别的，sed是一行一行处理的。</p>
<h3 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sed [nefr] [action] [file]</span><br></pre></td></tr></table></figure>
<p>部分：<br>| nefr | 含义                   |<br>| —- | ———————- |<br>| -f   | 使用file里到sed命令    |<br>| -i   | 直接将结果修改到file里 |</p>
<p>action：[n1[,n2]] func<br>| func | 含义                   |<br>| —- | ———————- |<br>| a    | 新增行（在该行后插入） |<br>| c    | 修改行                 |<br>| d    | 删除                   |<br>| i    | 插入行（在该行前插入） |<br>| s    | 取代（替换）           |</p>
<h4 id="简单使用：替换"><a href="#简单使用：替换" class="headerlink" title="简单使用：替换"></a>简单使用：替换</h4><p>1）将hello,hello,world的第一个hello替换为gwkki</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">语法：<span class="string">&#x27;s/pattern/replacement/flag&#x27;</span></span></span><br><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">gwkki,hello,world</span><br><span class="line"><span class="meta">#</span><span class="bash">特别的，这个替换会作用到每行第一个匹配到的。</span></span><br></pre></td></tr></table></figure>
<p>2）所有hello替换为gwkki</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/g&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">gwkki,gwkki,world</span><br></pre></td></tr></table></figure>
<p>3）替换第二个hello</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/2&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">hello,gwkki,world</span><br></pre></td></tr></table></figure>
<p>说下flag：<br>数字表示替换第几处匹配到的。<br>g表示全部替换<br>p表示匹配的行会被额外打印。<br>w表示保存到指定文件（读取文件可以用 sed -i command来修改并保存）<br>4)修改c1中的xxx为yyy并保存：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sed -i s/xxx/yyy/ a1</span><br></pre></td></tr></table></figure>

<h4 id="操作某些行："><a href="#操作某些行：" class="headerlink" title="操作某些行："></a>操作某些行：</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">操作第二行：</span></span><br><span class="line">sed &#x27;2s/xxx/yyy/&#x27; a1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">操作2-4行：包含2，4</span></span><br><span class="line">sed &#x27;2,4s/xxx/yyy/&#x27; a1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">操作3+行：包含3</span></span><br><span class="line">sed ‘3，$s/xxx/yyy/’ a1</span><br></pre></td></tr></table></figure>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">删除匹配到k到行：</span></span><br><span class="line">sed &#x27;/k/d&#x27; c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">删除第3-5行：</span></span><br><span class="line">sed &#x27;3,5d&#x27; c1</span><br></pre></td></tr></table></figure>
<h4 id="插入新增"><a href="#插入新增" class="headerlink" title="插入新增"></a>插入新增</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">在<span class="string">&quot;line 1&quot;</span>的下一行新增line2</span></span><br><span class="line">echo &quot;line 1&quot; | sed &#x27;a line 2&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">在<span class="string">&quot;line 1&quot;</span>的上一行新增line2</span></span><br><span class="line">echo &quot;line 1&quot; | sed &#x27;i line 2&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="替换特殊字符-如：’"><a href="#替换特殊字符-如：’" class="headerlink" title="替换特殊字符 如：’"></a>替换特殊字符 如：’</h4><p>使用””扩起来即可：“可以用&quot;匹配到</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将<span class="string">&#x27;替换成&#123;&#125;</span></span></span><br><span class="line">sed &quot;s/\&#x27;/&#123;&#125;/&quot; a1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala中的Option、Try、Either类型简介</title>
    <url>/2018/07/01/scala-zhong-de-option-try-either-lei-xing-jian-jie/</url>
    <content><![CDATA[<h2 id="Scala中的异常处理"><a href="#Scala中的异常处理" class="headerlink" title="Scala中的异常处理"></a>Scala中的异常处理</h2><p>Scala提供了许多选择来处理异常的数据。总的来说，三种类型解决三个方面的问题：</p>
<ul>
<li><p>Option：Null或空指针问题。</p>
</li>
<li><p>Try：运行时函数抛出异常的问题。</p>
</li>
<li><p>Either：返回值不确定的问题</p>
<blockquote>
<p>Scala版本：2.11.x</p>
</blockquote>
<h2 id="Option-Some-None："><a href="#Option-Some-None：" class="headerlink" title="Option/Some/None："></a>Option/Some/None：</h2><p>Option类型其实包含有三类：Option、Some、None。其中Some和None都是继承自Option。</p>
</li>
<li><p>Some：返回有效数据。</p>
</li>
<li><p>None：返回无效数据（或空值）。</p>
</li>
</ul>
<h3 id="使用Option"><a href="#使用Option" class="headerlink" title="使用Option"></a>使用Option</h3><p>定义一个拥有Option类型的函数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">validateName</span><span class="params">(name: String)</span>: Option[String] </span>= &#123;</span><br><span class="line">	<span class="keyword">if</span> (name.isEmpty) <span class="function">None</span></span><br><span class="line"><span class="function">	<span class="keyword">else</span> <span class="title">Some</span><span class="params">(name)</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<p>当input为空时，会返回None类型。否则返回Some类型。对于Some，通过get就能够拿到value。<br>Option主要用来处理Null值。通过getOrElse方法可以避免一些异常。</p>
<h2 id="Try-Success-Failure"><a href="#Try-Success-Failure" class="headerlink" title="Try/Success/Failure"></a>Try/Success/Failure</h2><p>在java中，对于可能会出现Exception的情况。我们可以选择try-catch来包裹。在Scala中，Try类型会让代码看上去更加优雅。<br>我们定义一个方法，用Try来对toInt进行包装，避免转换出现异常。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">parseInt</span><span class="params">(value: String)</span>: Try[Int] </span>= Try(value.toInt)</span><br><span class="line"></span><br><span class="line"><span class="comment">//模式匹配</span></span><br><span class="line">val list = List(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;1&quot;</span>,<span class="string">&quot;2&quot;</span>)</span><br><span class="line">    list.map(parseInt).foreach &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">Failure</span><span class="params">(exception)</span> </span>=&gt; println(exception)</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">Success</span><span class="params">(value)</span> </span>=&gt; println(value)</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// isSuccess、isFailure</span></span><br><span class="line">list.map(parseInt).foreach(x=&gt;&#123;</span><br><span class="line">      <span class="keyword">if</span>(x.isSuccess) println(x.get)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>我们可以通过模式匹配来判断Try的结果，并对应处理。也可以通过isSuccess、isFailure来进行判断。<br>使用逻辑上适合try-catch是差不多的。<br>注意：</p>
<blockquote>
<p>1）Try可以调用toOption方法转换成Option类型。isFailure会转换成None。<br>2）recover，recoverWith，transform可以让你优雅地处理Success和Failure的结果。</p>
</blockquote>
<h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p>利用recover，recoverWith方法，能够帮助我们取处理异常，并从故障中恢复。<br>以recover为例，其接收一个偏函数，返回一个新的Try[U]</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def recover[U &gt;: T](f: PartialFunction[Throwable, U]): Try[U]</span><br></pre></td></tr></table></figure>
<p>我们可以将上面的代码改造一下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">list.map(parseInt).foreach(x=&gt;&#123;</span><br><span class="line">      val triedInt = x.recover &#123;<span class="keyword">case</span> e:NumberFormatException=&gt; <span class="number">0</span> &#125;</span><br><span class="line">      println(triedInt)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>这样，我们可以对异常进行匹配，并进行对应的处理，并返回一个新的Try类型</p>
<h2 id="Either-Left-Right"><a href="#Either-Left-Right" class="headerlink" title="Either/Left/Right"></a>Either/Left/Right</h2><p>有时候，我们需要在函数中返回两种不同类型的值，Either就派上了用场。Left和Right代表了两种不同的类型。<br>如果 Either[A, B] 对象包含的是 A 的实例，那它就是 Left 实例，B则是 Right 实例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">validateName</span><span class="params">(name: String)</span>: Either[Int, String] </span>= &#123;</span><br><span class="line"><span class="keyword">if</span> (name.isEmpty) Left(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span> Right(name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后续我们可以通过模式匹配来获取结果。</p>
<h3 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h3><p>注意：Either是无偏向的，我们在使用Try时，能够直接使用map算子，默认情况下map会只对Success进行处理，并忽略Failure。但Either不一样。<br>对于Either对象，我们能够调用left、right方法拿到LeftProjection 或 RightProjection实例。并在之后进行map操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">validateName(&quot;&quot;).left.map(_ + 1).left.get</span><br></pre></td></tr></table></figure>
<p>这样，我们可以将left的数据进行+1处理，right的数据则不进行处理。需要注意，map返回的时一个Either类型，而不是leftProjection。</p>
<blockquote>
<p>例中get的使用其实是有问题的，当left没有数据时，get会报错。如果确实需要get值，建议使用getOrElse。</p>
</blockquote>
<h2 id="如何选择："><a href="#如何选择：" class="headerlink" title="如何选择："></a>如何选择：</h2><ol>
<li>Option[T]：当值不存在或者某些验证可能失败，并且你不关心为什么失败是。</li>
<li>Try[T]：当无法在函数中处理异常时可以选择使用Try。</li>
<li>Either[L,R]：当前两种都无法满足你的需求时。</li>
</ol>
<hr>
<p>参考：<a href="https://xebia.com/blog/try-option-or-either/">Try, Option or Either?</a></p>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Shuffle in Spark</title>
    <url>/2018/06/16/shuffle-in-spark/</url>
    <content><![CDATA[<h1 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h1><p>Shuffle简单来说就是将同一特征的数据经过网络/磁盘IO，分别分发聚集到各自的Executor上，因为这个过程涉及着数据的落地，传输。自然是费性能的。<br>在Spark中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。新版本的spark，已经移除了HashShuffleManager。但仍然需要了解。Shuffle整体可调整的内容有限。<br>虽然很多文章总说要尽量避免shuffle，但shuffle很多时候是不可避免的。目前比较好的解决办法是SMB-Join，但这个是从存储角度出发的解决方案，且有很多的局限性。其实SMB-Join的就是在理解Shuffle上实现的。MapJoin其实本质上也是在理解Shuffle上实现的，所以理解Shuffle很重要。</p>
<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>Shuffle总体可以分为Write和Read两个阶段：</p>
<ol>
<li>Write：即MapTask将数据写出。<br> Write数据会优先写入内存中，达到阈值后再溢写到文件内。</li>
<li>Read：即ReduceTask将数据读入。<br> Reduce会去读MapTask写出的文件，只读自己需要的Key的数据。<br> ReduceTask也会有自己的read buffer缓冲，但理解上没啥难度，通过参数调整大小即可。</li>
</ol>
<blockquote>
<p>本文基于Spark 2.2.x</p>
</blockquote>
<h1 id="HashShuffleManager（2-0后已弃用）"><a href="#HashShuffleManager（2-0后已弃用）" class="headerlink" title="HashShuffleManager（2.0后已弃用）"></a>HashShuffleManager（2.0后已弃用）</h1><p>相比于MapReduce中的shuffle，Spark的第一版本Shuffle是不存在排序过程的，Spark认为减少排序这一过程能够提高性能。但HashShuffle最后之所以被弃用，主要还是在shuffle过程中会产生大量磁盘中间文件，导致大量的磁盘IO操作，影响性能。<br>尽管已经弃用，但理解HashShuffle能够帮助去理解新的shuffleManager。<br>HashShuffle整个生命流程有两个阶段，本文简称未经优化V1，和优化后V2版本。</p>
<ol>
<li>未经优化：</li>
</ol>
<p>Shuffle的MapTask会根据ReduceTask的数量，生成等量的文件，这也是为什么HashShuffle的文件数异常的多。<br>即可能会产生M * R个文件（MapTask数量 * ReduceTask数量）<br>具体逻辑见图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825369515.png" alt="HashShuffle"></p>
<ol start="2">
<li>优化后：</li>
</ol>
<p>针对上一个版本的问题，HashShuffle进行了一层优化：<br>    同一个Core的连续任务会将文件写入到一个FileGroup中：仍然是一个reduceTask对应一个文件，不过在一个Core中连续执行的MapTask会共用这些文件，而非单独生成。<br>即会产生 Core * R 个文件。<br>具体逻辑看图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825492345.png" alt="优化后的HashShuffle"></p>
<h1 id="SortShuffleManager（1-2后默认）"><a href="#SortShuffleManager（1-2后默认）" class="headerlink" title="SortShuffleManager（1.2后默认）"></a>SortShuffleManager（1.2后默认）</h1><p>SortShuffle是参考MapReduce的Shuffle原理设计的，正如名字，会有Sort这一过程。SortShuffle会有3类，不过后两类其实都是在特殊条件下触发的，通过舍弃/修改一些基础步骤，从而得到更好的性能，所以重点还是理解第一类。</p>
<h2 id="普通"><a href="#普通" class="headerlink" title="普通"></a>普通</h2><p>SortShuffle通过index解决了中间文件过多的问题，也正因为这样所以才需要排序。具体实现逻辑如图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825966812.png" alt="SortShuffle"></p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581826998269.png" alt="SortShuffle"></p>
<p>具体步骤：</p>
<ol>
<li>写入内存前：</li>
</ol>
<p>Spark根据不同Shuffle算子，可能会选用不同的数据结构：</p>
<table>
<thead>
<tr>
<th>算子</th>
<th>数据结构</th>
</tr>
</thead>
<tbody><tr>
<td>Reduce等预聚合</td>
<td>Map数据结构，一边通过Map进行聚合，一边写入内存</td>
</tr>
<tr>
<td>Shuffle等普通</td>
<td>Array数据结构，直接写入内存。</td>
</tr>
<tr>
<td>（GroupByKey没有Map预聚合，应该算第二类）</td>
<td></td>
</tr>
</tbody></table>
<ol start="2">
<li>写入磁盘文件前：</li>
</ol>
<p>内存数据达到阈值后，Spark会先进行排序，然后通过BufferedOutputStream，通过缓冲Buffer，分批写入到文件中。<br>即这个阶段会发送多次溢写操作，产生多个临时文件。由于有数据的落地，也伴随着这序列化/反序列化。</p>
<ol start="3">
<li>合并文件：</li>
</ol>
<p>上一步产生的磁盘文件会进行合并。并生成一个索引文件表示key的offset。</p>
<ol start="4">
<li>索引文件：</li>
</ol>
<p>因为所有的Key都放在了一个文件中，所以会为合并的文件生成一个索引文件。<br>标识了下游各个task的数据在文件中的start offset与end offset。（主要包含的就是一个Tuple3（partition， offset, length），其中partition就指定了这个segment数据片段属于哪一个下游的reduceTask，offset和length决定这个segment数据数据内容是哪些。）</p>
<p>最终会产生2 * M个文件。</p>
<h2 id="Bypass"><a href="#Bypass" class="headerlink" title="Bypass"></a>Bypass</h2><p>Bypass会在满足触发条件后自动触发，具有更快的Shuffle速度。</p>
<ol>
<li>shuffle map task数量小于<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值。</li>
<li>非Map端聚合类的shuffle算子（比如join）</li>
</ol>
<blockquote>
<ol>
<li>no map-side combine is specified</li>
<li>the number of partitions is less than or equal to spark.shuffle.sort.bypassMergeThreshold</li>
</ol>
</blockquote>
<p> 其实Bypass机制可以理解为加速非map聚合shuffle算子的shuffle速度，所以我们主要关注的还是<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值。</p>
<p>Bypass的逻辑如图所示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581826653584.png" alt="Bypass"></p>
<p>很明显，相比于普通的：</p>
<ol>
<li>少了一个排序过程。这也是为什么比较快的地方。</li>
</ol>
<blockquote>
<p>Sorting is slower than hashing. It might worth tuning the bypassMergeThreshold parameter for your own cluster to find a sweet spot, but in general for most of the clusters it is even too high with its default</p>
</blockquote>
<ol start="2">
<li>那么Bypass的Index文件是如何产生的呢？其实数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。</li>
</ol>
<h2 id="Unsafe-Shuffle-or-Tungsten-Sort"><a href="#Unsafe-Shuffle-or-Tungsten-Sort" class="headerlink" title="Unsafe Shuffle or Tungsten Sort"></a>Unsafe Shuffle or Tungsten Sort</h2><p>Spark 1.4.0+，Spark启动了TungstenSort，即钨丝计划。具体参见：<a href="https://issues.apache.org/jira/browse/SPARK-7081">SPARK-7081</a>、<a href="https://issues.apache.org/jira/browse/SPARK-7075">SPARK-7075</a>。</p>
<p>Spark 1.6.0+ Tungsten-sort并入Sort Based Shuffle,由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle。</p>
<p>简单来讲，Tungsten Sort带来了如下优化点：</p>
<blockquote>
<ol>
<li>Operate directly on serialized binary data without the need to deserialize it. It uses unsafe (sun.misc.Unsafe) memory copy functions to directly copy the data itself, which works fine for serialized data as in fact it is just a byte array.</li>
<li>Uses special cache-efficient sorter ShuffleExternalSorter that sorts arrays of compressed record pointers and partition ids. By using only 8 bytes of space per record in the sorting array, it works more efficienly with CPU cache.</li>
<li>As the records are not deserialized, spilling of the serialized data is performed directly (no deserialize-compare-serialize-spill logic)</li>
</ol>
</blockquote>
<p>翻译概括以下：</p>
<blockquote>
<ol>
<li>将数据记录用二进制的方式存储，直接在序列化的二进制数据上sort而不是反序列化为java 对象，这样一方面可以减少memory的使用和GC的开销，另一方面避免shuffle过程中频繁的序列化以及反序列化。</li>
<li>在排序过程中，它提供cache-efficient sorter —— <a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java">ShuffleExternalSorter</a>，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能。</li>
</ol>
</blockquote>
<p>开启条件：</p>
<blockquote>
<ol>
<li>The shuffle dependency specifies no aggregation or output ordering.</li>
<li>The shuffle serializer supports relocation of serialized values (this is currently supported<br>by KryoSerializer and Spark SQL’s custom serializers).</li>
<li>The shuffle produces fewer than 16777216 output partitions.</li>
<li>No individual record is larger than 128 MB when serialized.</li>
</ol>
</blockquote>
<ol>
<li>shuffle阶段不能有aggregate操作。</li>
<li>shuffle的produce输出需要少于16777216 分区。</li>
<li>序列化时。单条记录小于128 MB。</li>
<li>能够移动数据的序列化器，如使用Kryo序列化器。</li>
</ol>
<h1 id="三种sortShuffle的判定顺序"><a href="#三种sortShuffle的判定顺序" class="headerlink" title="三种sortShuffle的判定顺序"></a>三种sortShuffle的判定顺序</h1><p>观察源码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">...</span><br><span class="line">      new BypassMergeSortShuffleHandle[K, V](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])</span><br><span class="line">    &#125; else if (SortShuffleManager.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">...</span><br><span class="line">      new SerializedShuffleHandle[K, V](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">...</span><br><span class="line">      new BaseShuffleHandle(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，目前Spark主要的shuffle有三种方式组成，并且判定的<strong>先后顺序</strong>：BypassMergeSortShuffle，SerializedShuffle(Tungsten)，BaseShuffle(SortShuffle)。在使用非map端聚合算子且无需排序时，尽量能够满足触发前两者的条件，这能够带来一些性能提升。</p>
<h1 id="常见参数："><a href="#常见参数：" class="headerlink" title="常见参数："></a>常见参数：</h1><ol>
<li><p>spark.shuffle.file.buffer<br>默认值：32k<br>即MapTask输出内存的大小。增加会占用Execution内存，但能够减少溢写次数。</p>
</li>
<li><p>spark.shuffle.sort.bypassMergeThreshold<br>默认值：200<br>Bypass触发的条件，当使用非map聚合算子，且reduce的分区数小于该值时触发。</p>
</li>
<li><p>spark.reducer.maxSizeInFlight<br>默认值：48m<br>reduce端的缓冲大小，调大该参数，增加会占用Execution内存，但能够减少拉取次数。</p>
</li>
</ol>
<hr>
<p>参考文章：</p>
<ol>
<li><a href="https://0x0fff.com/spark-architecture-shuffle/">Spark Architecture: Shuffle</a></li>
<li><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-7075">SPARK-7075</a></li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-7081">SPARK-7081</a></li>
<li><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala">SortShuffleManager</a></li>
<li><a href="https://www.iteblog.com/archives/1672.html">Spark性能优化：shuffle调优</a></li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkContext执行流程</title>
    <url>/2018/10/01/sparkcontext-zhi-xing-liu-cheng/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>其实这个也算是Spark任务提交的内容之一。Spark任务的关键就是构建DAG图，Action触发然后去解析DAG图，将Stage交给Executor处理，executor处理完成后返回结果。<br>Spark程序都是以SparkContext为入口，触发Action算子后执行具体任务，这边也会顺着这条思路写下去。</p>
<blockquote>
<p>部分源码为了方便阅读会进行精简或调整，与真实源码会略有不同。但整体思路是一样的。<br>Spark版本参照最新的2.3.x<br>涉及模式主要介绍Yarn和Local</p>
</blockquote>
<h1 id="流程图概览"><a href="#流程图概览" class="headerlink" title="流程图概览"></a>流程图概览</h1><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217224954.png" alt="流程图"><br>先看图，整个流程可以分成四个步骤，具体会在源码中有所展现：</p>
<ol>
<li>根据我们的各种算子生成DAG图</li>
<li>解析DAG图，拆分成Stage（Task集合）。将Task集合发往下一步。</li>
<li>Cluster Manager来发起任务，错误或落伍(比如说数据倾斜导致某一任务始终跑不完)，会进行重试。</li>
<li>将Task发往具体的Worker/Container执行。</li>
</ol>
<h2 id="1）获取SparkContext"><a href="#1）获取SparkContext" class="headerlink" title="1）获取SparkContext"></a>1）获取SparkContext</h2><p>在程序里我们会通过<code>new SparkContext(Conf)</code>来得到一个SparkContext实例，具体到SparkContext里，由于代码很多，这边主要的其实就是初始化三个变量：    </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="title">config</span>: <span class="title">SparkConf</span>) <span class="keyword">extends</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">	<span class="comment">// 初始化三个主要变量</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: SchedulerBackend = _</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: TaskScheduler = _</span><br><span class="line">	<span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: DAGScheduler = _</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 之后会对上述三个变量进行初始化：</span></span><br><span class="line">	val (sched, ts) = SparkContext.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">    _schedulerBackend = sched</span><br><span class="line">    _taskScheduler = ts</span><br><span class="line">    _dagScheduler = <span class="keyword">new</span> DAGScheduler(<span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里代码根据具体的master、deployMode来对<code>_schedulerBackend</code>和<code>_taskScheduler</code>进行初始化，同时还会初始化<code>_dagScheduler</code></p>
<h3 id="1-1）其中createTaskScheduler方法"><a href="#1-1）其中createTaskScheduler方法" class="headerlink" title="1.1）其中createTaskScheduler方法"></a>1.1）其中createTaskScheduler方法</h3><p>主要用来初始化<code>_schedulerBackend</code>和<code>_taskScheduler</code>，这里以local为例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">createTaskScheduler</span><span class="params">(..)</span></span>&#123;</span><br><span class="line">	master match &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;local&quot;</span> =&gt;</span><br><span class="line">        val scheduler = <span class="keyword">new</span> TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = <span class="keyword">true</span>)</span><br><span class="line">        val backend = <span class="keyword">new</span> LocalSchedulerBackend(sc.getConf, scheduler, <span class="number">1</span>)</span><br><span class="line">        scheduler.initialize(backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这段代码，我们可以知道，一个是<code>TaskSchedulerImpl</code>的实例，一个是<code>LocalSchedulerBackend</code>的实例。里面主要就是初始化一些参数内容。具体的方法在后续过程会使用到，这里不进行过多赘述。</p>
<h3 id="1-2）其中new-DAGScheduler-this-里"><a href="#1-2）其中new-DAGScheduler-this-里" class="headerlink" title="1.2）其中new DAGScheduler(this)里"></a>1.2）其中new DAGScheduler(this)里</h3><p>初始化时有两个关键点：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">DAGScheduler</span>(...)</span>&#123;</span><br><span class="line">	<span class="keyword">private</span>[spark] val eventProcessLoop = <span class="keyword">new</span> DAGSchedulerEventProcessLoop(<span class="keyword">this</span>)</span><br><span class="line">	taskScheduler.setDAGScheduler(<span class="keyword">this</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这边会生成一个事件队列<code>eventProcessLoop</code>，同时设置我们的<code>taskScheduler</code>。这些内容会在之后使用到。<br>至此，这个阶段SparkContext的主要任务已经基本完成：设置我们上面提到的三个参数。<br>接下来，就要等到Action算子触发实际的作业运行了。</p>
<h2 id="2）Action"><a href="#2）Action" class="headerlink" title="2）Action"></a>2）Action</h2><p>这里以Collect算子为例，里面会调用<code>runJob</code>这个方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="title">T</span>: <span class="title">ClassTag</span>](...)</span>&#123;</span><br><span class="line">	<span class="function">def <span class="title">collect</span><span class="params">()</span>: Array[T] </span>= withScope &#123;</span><br><span class="line">    val results = sc.runJob(<span class="keyword">this</span>, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">    Array.concat(results: _*)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>里面点进去，实际会进行多次跳转，这边把中间过程进行省略，最后我们会来到：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="title">config</span>: <span class="title">SparkConf</span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  def runJob[T, U: ClassTag](...)&#123;</span><br><span class="line">  	val callSite = getCallSite</span><br><span class="line">    val cleanedFunc = clean(func)</span><br><span class="line">  	<span class="comment">// 前面是一些参数初始化</span></span><br><span class="line">  	</span><br><span class="line">  	<span class="comment">// 核心，进入runJob。</span></span><br><span class="line">  	dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  	</span><br><span class="line">  	<span class="comment">// 其他处理</span></span><br><span class="line">    progressBar.foreach(_.finishAll())</span><br><span class="line">    rdd.doCheckpoint()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们这里主要需要关注<code>dagScheduler.runJob</code>方法，跟这个这个方法，我们来到了DAGScheduler：</p>
<h2 id="3）DAGScheduler"><a href="#3）DAGScheduler" class="headerlink" title="3）DAGScheduler"></a>3）DAGScheduler</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def runJob[T, U](...)&#123;</span><br><span class="line">	val start = System.nanoTime</span><br><span class="line">    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)&#123;</span><br><span class="line">		eventProcessLoop.post(JobSubmitted(</span><br><span class="line">			jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">			SerializationUtils.clone(properties)))</span><br><span class="line">		waiter</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我将两段代码进行了合并，runJob会调用<code>submitJob</code>方法，这个方法会返回一个<code>JobWaiter</code>对象，简单来说，就是将我们的Func、Rdd加入之前创建的<code>eventProcessLoop</code>。</p>
<h3 id="3-1）DAGSchedulerEventProcessLoop"><a href="#3-1）DAGSchedulerEventProcessLoop" class="headerlink" title="3.1）DAGSchedulerEventProcessLoop"></a>3.1）DAGSchedulerEventProcessLoop</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span></span>&#123;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	* The main event loop of the DAG scheduler.</span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">	<span class="function">override def <span class="title">onReceive</span><span class="params">(event: DAGSchedulerEvent)</span>: Unit </span>= &#123;..doOnReceive..&#125;</span><br><span class="line">	<span class="function"><span class="keyword">private</span> def <span class="title">doOnReceive</span><span class="params">(event: DAGSchedulerEvent)</span>: Unit </span>= event match &#123;</span><br><span class="line">		<span class="function"><span class="keyword">case</span> <span class="title">JobSubmitted</span><span class="params">(jobId, rdd, func, partitions, callSite, listener, properties)</span> </span>=&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个类我同样进行了简化，实际上就是判断我们提交过来的是什么，然后指向对应的方法。</p>
<blockquote>
<p>这里我们是JobSubmitted。所以指向dagScheduler.handleJobSubmitted</p>
</blockquote>
<h3 id="3-2）handleJobSubmitted"><a href="#3-2）handleJobSubmitted" class="headerlink" title="3.2）handleJobSubmitted"></a>3.2）handleJobSubmitted</h3><p>实际上源码会有很多的try catch，这边进行部分简写方便阅读</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">handleJobSubmitted</span><span class="params">(...)</span></span>&#123;</span><br><span class="line">	<span class="keyword">var</span> finalStage: ResultStage = </span><br><span class="line">		createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">	val job = <span class="keyword">new</span> ActiveJob(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">	finalStage.setActiveJob(job)</span><br><span class="line">	submitStage(finalStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里首先会拿到最后一个Stage，并通过它生成running job。最终我们将finalStage提交。</p>
<blockquote>
<p>A running job in the DAGScheduler. Jobs can be of two types:<br>1）a result job, which computes a ResultStage to execute an action,<br>2）or a map-stage job, which computes the map outputs for a ShuffleMapStage before any downstream stages are submitted.</p>
</blockquote>
<h3 id="3-3）submitStage"><a href="#3-3）submitStage" class="headerlink" title="3.3）submitStage"></a>3.3）submitStage</h3><p>这个部分实际就承担了拆分Stage的职责，其实就是通过递归完成的，为了方便理解就直接搬源码了。大家可以结合提交的作业时打印的log看看。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">submitStage</span><span class="params">(stage: Stage)</span> </span>&#123;</span><br><span class="line">  val jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;submitStage(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      val missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, None)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>方法首先接收到的是finalStage，关键是<code>getMissingParentStages(stage)</code>，方法返回<code>List[Stage]</code>。这里能够得到尚未执行的ParentStage，并按照id排序。<br>所以这边的逻辑实际是：提交finalStage → 尚有父Stage没执行 → 提交父Stage，递归，直到所有父Stage都执行。<br>最终，对于每个Stage，底层都会来到<code>submitMissingTasks(stage, jobId.get)</code></p>
<h3 id="3-4）submitMissingTasks"><a href="#3-4）submitMissingTasks" class="headerlink" title="3.4）submitMissingTasks"></a>3.4）submitMissingTasks</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">submitMissingTasks</span><span class="params">(stage: Stage, jobId: Int)</span> </span>&#123;</span><br><span class="line">	<span class="comment">//..省略一些前置处理..</span></span><br><span class="line">	<span class="comment">//..主要就是将Stage包装成Tasks用来提交</span></span><br><span class="line">	val tasks: Seq[Task[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">		stage match &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: ShuffleMapStage =&gt;</span><br><span class="line">          stage.pendingPartitions.clear()</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            &#123;...&#125;</span><br><span class="line">            <span class="keyword">new</span> ShuffleMapTask(...)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> stage: ResultStage =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            &#123;...&#125;</span><br><span class="line">            <span class="keyword">new</span> ResultTask(...)</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//..提交tasks..//</span></span><br><span class="line">	<span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">		taskScheduler.submitTasks(<span class="keyword">new</span> TaskSet(</span><br><span class="line">        	tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用<code>taskScheduler</code>的<code>submitTasks</code>方法进行task的提交，这段方法的用途注释已经说明，其实目的就是将Stage包装成Task集合。里面有大量的判断这边就忽略了。<br>其实刚刚也提到过，Task在内部被Spark分为中间的：ShuffleMap，以及最后的：Result。这个也对应的不同的Job。<br>代码跟着就来到了TasklScheduler</p>
<h2 id="4）TaskScheduler"><a href="#4）TaskScheduler" class="headerlink" title="4）TaskScheduler"></a>4）TaskScheduler</h2><p>实际上TaskScheduler是一个trait，通过之前的分析(初始化SparkContext)，我们可以知道实际上我们建立的是一个<code>TaskSchedulerImpl</code>对象，实际上Spark本身就实现了这么一个子类。这里直接定位到对应方法即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">TaskSchedulerImpl</span>(...)</span>&#123;</span><br><span class="line">	<span class="function">override def <span class="title">submitTasks</span><span class="params">(taskSet: TaskSet)</span> </span>&#123;</span><br><span class="line">	    val tasks = taskSet.tasks</span><br><span class="line">	    <span class="keyword">this</span>.<span class="keyword">synchronized</span> &#123;</span><br><span class="line">	    	val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">	    	val stage = taskSet.stageId</span><br><span class="line">	    	val stageTaskSets =</span><br><span class="line">        		taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> HashMap[Int, TaskSetManager])</span><br><span class="line">        		</span><br><span class="line">        	<span class="comment">// 建立taskset与manager 的对应关系</span></span><br><span class="line">			stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// TaskSetManager会被放入调度池中。</span></span><br><span class="line">			schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line">	    &#125;</span><br><span class="line">	    <span class="comment">// 为tasks分配资源，调度任务</span></span><br><span class="line">	    backend.reviveOffers()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里会初始化一个Manager对Tasks进行管理，在最开始也说过，对于失败或者落队的任务Manager会进行重试，并与上一级交互执行情况。不过对于本文章讨论的流程来说，这部分实际上最重要的，就是将Task提交到具体的Executor。也就是<code>backend.reviveOffers()</code>。<br>对于不同的任务，SchedulerBackend有两个实现类：</p>
<ul>
<li>CoarseGrainedSchedulerBackend</li>
<li>LocalSchedulerBackend。</li>
</ul>
<p>这边就以CoarseGrainedSchedulerBackend为例进行继续讲解。他们的区别从命名其实就能猜到。</p>
<h3 id="4-1）CoarseGrainedSchedulerBackend"><a href="#4-1）CoarseGrainedSchedulerBackend" class="headerlink" title="4.1）CoarseGrainedSchedulerBackend"></a>4.1）CoarseGrainedSchedulerBackend</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedSchedulerBackend</span>(<span class="title">scheduler</span>: <span class="title">TaskSchedulerImpl</span>, <span class="title">val</span> <span class="title">rpcEnv</span>: <span class="title">RpcEnv</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ExecutorAllocationClient</span> <span class="title">with</span> <span class="title">SchedulerBackend</span> <span class="title">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">	<span class="function">override def <span class="title">reviveOffers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		driverEndpoint.send(ReviveOffers)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// driverEndpoint的初始化，都在CoarseGrainedSchedulerBackend类里。</span></span><br><span class="line">	<span class="function">override def <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    	val properties = <span class="keyword">new</span> ArrayBuffer[(String, String)]</span><br><span class="line">    	driverEndpoint = createDriverEndpointRef(properties)</span><br><span class="line">    &#125;	</span><br><span class="line">	<span class="function"><span class="keyword">protected</span> def <span class="title">createDriverEndpointRef</span><span class="params">(...)</span>: RpcEndpointRef </span>= &#123;</span><br><span class="line">		rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">protected</span> def <span class="title">createDriverEndpoint</span><span class="params">(properties: Seq[(String, String)</span>]): DriverEndpoint </span>= &#123;</span><br><span class="line">    	<span class="keyword">new</span> DriverEndpoint(rpcEnv, properties)</span><br><span class="line">  	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于<code>reviveOffers</code>方法里会使用到<code>driverEndpoint</code>，这边一并将他的初始化源码附上。通过源码我们也能注意到，这里主要是<code>DriverEndpoint</code>完成了实际工作，毕竟，我们要发送ReviveOffers事件，这个事件从哪来的？进入DriverEndPoint一探究竟。</p>
<blockquote>
<p>DriverEndpoint是CoarseGrainedSchedulerBackend的一个内部类。</p>
</blockquote>
<h3 id="4-2）DriverEndpoint"><a href="#4-2）DriverEndpoint" class="headerlink" title="4.2）DriverEndpoint"></a>4.2）DriverEndpoint</h3><p>实际上，driverEndpoint.send(ReviveOffers)会触发DriverEndPoint的makeOffers方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DriverEndpoint</span>(...) <span class="keyword">extends</span> <span class="title">ThreadSafeRpcEndpoint</span> <span class="title">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">	override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">		<span class="keyword">case</span> ReviveOffers =&gt;</span><br><span class="line">        	makeOffers()</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用makeOffers</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> def <span class="title">makeOffers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 主要是找出要在哪些Worker上启动哪些task。</span></span><br><span class="line">		val taskDescs = withLock &#123;</span><br><span class="line">			scheduler.resourceOffers(workOffers)</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (!taskDescs.isEmpty) &#123;</span><br><span class="line">        	launchTasks(taskDescs)</span><br><span class="line">      	&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>resourceOffers返回的是Seq[Seq[TaskDescription]] 类型，主要用途是找出要在哪些Worker上启动哪些task。最后调用launchTasks(taskDescs)。</p>
<h3 id="4-3）launchTasks"><a href="#4-3）launchTasks" class="headerlink" title="4.3）launchTasks"></a>4.3）launchTasks</h3><p>主要完成序列化，发送任务的工作：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">launchTasks</span><span class="params">(tasks: Seq[Seq[TaskDescription]])</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">		<span class="comment">// 序列化Task</span></span><br><span class="line">        val serializedTask = TaskDescription.encode(task)</span><br><span class="line">        <span class="comment">// 省略一些判断</span></span><br><span class="line">        <span class="comment">// 发送序列化后的Task</span></span><br><span class="line">        executorData.executorEndpoint.send(LaunchTask(<span class="keyword">new</span> SerializableBuffer(serializedTask)))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>至此，任务已经发往了Executor</p>
<h2 id="5）Executor"><a href="#5）Executor" class="headerlink" title="5）Executor"></a>5）Executor</h2><p>先找到：CoarseGrainedExecutorBackend.scala</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedExecutorBackend</span>(...)</span></span><br><span class="line"><span class="class">	<span class="keyword">extends</span> <span class="title">ThreadSafeRpcEndpoint</span> <span class="title">with</span> <span class="title">ExecutorBackend</span> <span class="title">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	override def receive: PartialFunction[Any, Unit] = &#123;</span><br><span class="line">		<span class="function"><span class="keyword">case</span> <span class="title">LaunchTask</span><span class="params">(data)</span> </span>=&gt;</span><br><span class="line">			val taskDesc = TaskDescription.decode(data.value)</span><br><span class="line">    	    logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">        	executor.launchTask(<span class="keyword">this</span>, taskDesc)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这部分代码完成：接收，生成Executor，在Executor上launchTask的任务。代码进入到Executor.scala</p>
<h3 id="5-1）Executor-scala"><a href="#5-1）Executor-scala" class="headerlink" title="5.1）Executor.scala"></a>5.1）Executor.scala</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">Executor</span>(...)</span>&#123;</span><br><span class="line">	<span class="keyword">private</span> val runningTasks = <span class="keyword">new</span> ConcurrentHashMap[Long, TaskRunner]</span><br><span class="line">  	<span class="function">def <span class="title">launchTask</span><span class="params">(context: ExecutorBackend, taskDescription: TaskDescription)</span>: Unit </span>= &#123;</span><br><span class="line">    	val tr = <span class="keyword">new</span> TaskRunner(context, taskDescription)</span><br><span class="line">    	runningTasks.put(taskDescription.taskId, tr)</span><br><span class="line">    	threadPool.execute(tr)</span><br><span class="line">  	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里就是将我们的Task新起一个TaskRunner，并加入了线程池，所以具体的处理需要进入TaskRunner里。</p>
<h3 id="5-2）TaskRunner"><a href="#5-2）TaskRunner" class="headerlink" title="5.2）TaskRunner"></a>5.2）TaskRunner</h3><p>这是一个继承了Runnable的线程，所以我们直接找到run方法，其实主要干了三件事：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaskRunner</span>(...) <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">	<span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">	<span class="comment">// 反序列化</span></span><br><span class="line">	    Executor.taskDeserializationProps.set(taskDescription.properties)</span><br><span class="line">        updateDependencies(taskDescription.addedFiles, taskDescription.addedJars)</span><br><span class="line">        task = ser.deserialize[Task[Any]](</span><br><span class="line">          taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 运行我们的任务，得到结果</span></span><br><span class="line">		val value = Utils.tryWithSafeFinally &#123;</span><br><span class="line">			task.run(</span><br><span class="line">        		taskAttemptId = taskId,</span><br><span class="line">        		attemptNumber = taskDescription.attemptNumber,</span><br><span class="line">        		metricsSystem = env.metricsSystem)</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 包装结果，序列化，并发回。</span></span><br><span class="line">	val valueBytes = resultSer.serialize(value)</span><br><span class="line">	val directResult = <span class="keyword">new</span> DirectTaskResult(valueBytes, accumUpdates)</span><br><span class="line">	val serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">	val serializedResult: ByteBuffer = &#123;...serializedDirectResult...&#125;</span><br><span class="line">	execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>基本上到这里，整个执行流程就说完了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>SparkContext<br> 1）根据提交的master类型，deploy类型等，生成TaskScheduler、DAGScheduler、 SchedulerBackend<br> 2）解析程序的业务逻辑：数据源 → trans → action<br> 3）触发Action，就到了DAGScheduler </li>
<li>DAGScheduler<br>1）runJob方法，最终目的就是拆分Stage<br>2）拆分方式是：先提交finalStage，如果尚有父Stage未提交，则会触发拆分。根据shuﬀle来 拆分。<br>3）总的来说，会一直递归，直到最父的Stage提交。最终才会提交finalStage 每次提交Stage，最终就会来到taskScheduler.submitTasks(new TaskSet) </li>
<li>TaskScheduler<br> 1）将接收到的Stage封装成TaskSet。使用submitTasks(taskSet: TaskSet)提交。<br> 2）将每个Task拿出来，通过TaskDescription.encode(task)序列化，用于网络传输。<br> 3）通过executorData.executorEndpoint.send将任务发往Executor </li>
<li>Executor<br> 1）receive方法接收发过来的Task。<br> 2）最终的处理逻辑在TaskRunner里。 <pre><code> 2.1）反序列化。task = ser.deserialize 执行func，
 2.2）返回结果。val value = Utils.tryWithSafeFinally &#123; task.run() &#125;
 2.3）将结果序列化。val serializedResult = serializedDirectResult 将结果发回。execBackend.statusUpdate
</code></pre>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark中的BlockManager</title>
    <url>/2019/11/17/spark-zhong-de-blockmanager/</url>
    <content><![CDATA[<h1 id="一、BlockManager的用途。"><a href="#一、BlockManager的用途。" class="headerlink" title="一、BlockManager的用途。"></a>一、BlockManager的用途。</h1><p>BlockManager会运行在所有节点之上（driver+executor），管理整个Spark运行时的数据读写的。</p>
<blockquote>
<p>Manager running on every node (driver and executors) which provides interfaces for putting and retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap).</p>
</blockquote>
<h1 id="二、BlockManager架构。"><a href="#二、BlockManager架构。" class="headerlink" title="二、BlockManager架构。"></a>二、BlockManager架构。</h1><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223404.png" alt="BlockManager架构"><br>1、每个Blockmanager实例化时都会向BlockManagerMaster进行注册。（实际上是 Executor 中的 BlockManager 注册给 Driver 上的 BlockMangerMasterEndpoiont）<br>2、BlockManagerMaster通过BlockManagerInfo来对Blockmanager进行元数据管理。<br>3、当改变了具体的ExecutorBackend 上的 Block 的信息后，就要向BlockManagerMaster发送消息来更新。<br>4、BlockManager也是典型的主从架构。</p>
<h1 id="三、BlockManager相关源码。"><a href="#三、BlockManager相关源码。" class="headerlink" title="三、BlockManager相关源码。"></a>三、BlockManager相关源码。</h1><blockquote>
<p>基于Spark 2.4.x</p>
</blockquote>
<h2 id="1-SparkEnv："><a href="#1-SparkEnv：" class="headerlink" title="1. SparkEnv："></a>1. SparkEnv：</h2><p>在这里会初始化Blockmanager的实例，但目前还不能用，只是确定初始化时的一些数据。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line">val blockManager = <span class="keyword">new</span> BlockManager(executorId, rpcEnv, blockManagerMaster,</span><br><span class="line">      serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,</span><br><span class="line">      blockTransferService, securityManager, numUsableCores)</span><br></pre></td></tr></table></figure>

<h2 id="2-SparkContext："><a href="#2-SparkContext：" class="headerlink" title="2. SparkContext："></a>2. SparkContext：</h2><p>作为App的入口，初始化SparkContext时会根据AppId初始化Driver的BlockManager：<br>SparkContext.scala：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">_env.blockManager.initialize(_applicationId)</span><br></pre></td></tr></table></figure>
<p>Executor的Blockmanager会在Executor上完成初始化。executor的ID会在sparkEnv初始化时就确定了。<br>Executor.scala：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.blockManager.initialize(conf.getAppId)</span><br></pre></td></tr></table></figure>

<h2 id="3-初始化时，还会一并初始化blockTransferService与shuffleClient。"><a href="#3-初始化时，还会一并初始化blockTransferService与shuffleClient。" class="headerlink" title="3. 初始化时，还会一并初始化blockTransferService与shuffleClient。"></a>3. 初始化时，还会一并初始化blockTransferService与shuffleClient。</h2><pre><code>前者负责本地的块的存取。后者负责读取shuffle的中间文件。
</code></pre>
<blockquote>
<p>BlockTransferService：Initialize the transfer service by giving it the BlockDataManager that can be used to fetch local blocks or put local blocks.<br>ShuffleClient：Provides an interface for reading shuffle files, either from an Executor or external service.</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">initialize</span><span class="params">(appId: String)</span>: Unit </span>= &#123;</span><br><span class="line">    blockTransferService.init(<span class="keyword">this</span>)</span><br><span class="line">    shuffleClient.init(appId)</span><br><span class="line">    ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-向BlockManagerMaster注册："><a href="#4-向BlockManagerMaster注册：" class="headerlink" title="4. 向BlockManagerMaster注册："></a>4. 向BlockManagerMaster注册：</h2><p>其本质是向 Driver 上的 BlockManagerMasterEndpoint 注册。其中slaveEndpoint负责接收Driver 中的 BlockManagerMaster 发过来的指令，如删除RDD。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val id = BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None)</span><br><span class="line"></span><br><span class="line">val idFromMaster = master.registerBlockManager(</span><br><span class="line">      id,</span><br><span class="line">      maxOnHeapMemory,</span><br><span class="line">      maxOffHeapMemory,</span><br><span class="line">      slaveEndpoint)</span><br></pre></td></tr></table></figure>
<p>registerBlockManager方法会向BlockManagerMasterEndpoint 发送注册信息：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val updatedId = driverEndpoint.askSync[BlockManagerId](</span><br><span class="line">      RegisterBlockManager(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint))</span><br></pre></td></tr></table></figure>
<p>消息会在BlockManagerMasterEndpoint 的receiveAndReply方法进行匹配，注册并将ID返回：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">override def <span class="title">receiveAndReply</span><span class="params">(context: RpcCallContext)</span>: PartialFunction[Any, Unit] </span>= &#123;</span><br><span class="line">    <span class="function"><span class="keyword">case</span> <span class="title">RegisterBlockManager</span><span class="params">(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint)</span> </span>=&gt;</span><br><span class="line">      context.reply(register(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint))</span><br></pre></td></tr></table></figure>
<p>最后，BlockManagerInfo会被存放在blockManagerInfo(这是一个Map)中进行管理：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">blockManagerInfo(id) = <span class="keyword">new</span> BlockManagerInfo(</span><br><span class="line">        id, System.currentTimeMillis(), maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint)</span><br></pre></td></tr></table></figure>
<p>每一个 BlockManager 都会对应一个 BlockManagerInfo，最后，BlockManagerMaster 包含了集群中整个 BlockManager 注册的信息。</p>
<h1 id="四、BlockManager的具体操作举例。"><a href="#四、BlockManager的具体操作举例。" class="headerlink" title="四、BlockManager的具体操作举例。"></a>四、BlockManager的具体操作举例。</h1><p>前面说过BlockManager是用来管理数据的，具体我们参考remove这个方法看看逻辑是怎么样的。<br>具体来到RDD的unpersist方法，这里明显会删除缓存的RDD：</p>
<h2 id="1、unpersist算子"><a href="#1、unpersist算子" class="headerlink" title="1、unpersist算子"></a>1、unpersist算子</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">unpersist</span><span class="params">(blocking: Boolean = <span class="keyword">true</span>)</span>: <span class="keyword">this</span>.type </span>= &#123;</span><br><span class="line">  logInfo(<span class="string">&quot;Removing RDD &quot;</span> + id + <span class="string">&quot; from persistence list&quot;</span>)</span><br><span class="line">  sc.unpersistRDD(id, blocking)</span><br><span class="line">  storageLevel = StorageLevel.NONE</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进入 sc.unpersistRDD(id, blocking)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function">def <span class="title">unpersistRDD</span><span class="params">(rddId: Int, blocking: Boolean = <span class="keyword">true</span>)</span> </span>&#123;</span><br><span class="line">    env.blockManager.master.removeRdd(rddId, blocking)</span><br><span class="line">    persistentRdds.remove(rddId)</span><br><span class="line">    listenerBus.post(SparkListenerUnpersistRDD(rddId))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这里就看到调用了env.blockManager.master.removeRdd(rddId, blocking)。</p>
<h2 id="2、blockManagerMaster的removeRdd方法"><a href="#2、blockManagerMaster的removeRdd方法" class="headerlink" title="2、blockManagerMaster的removeRdd方法"></a>2、blockManagerMaster的removeRdd方法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">removeRdd</span><span class="params">(rddId: Int, blocking: Boolean)</span> </span>&#123;</span><br><span class="line">    val future = driverEndpoint.askSync[Future[Seq[Int]]](RemoveRdd(rddId))</span><br><span class="line">    future.failed.foreach(e =&gt;</span><br><span class="line">      logWarning(s<span class="string">&quot;Failed to remove RDD $rddId - $&#123;e.getMessage&#125;&quot;</span>, e)</span><br><span class="line">    )(ThreadUtils.sameThread)</span><br><span class="line">    <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">      timeout.awaitResult(future)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这里会向BlockManagerSlaveEndpoint发送删除RDD的信息并等待回执，BlockManagerSlaveEndpoint的receiveAndReply方法会接收到，并调用对应的方法，让manager去删除RDD：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">override def <span class="title">receiveAndReply</span><span class="params">(context: RpcCallContext)</span>: PartialFunction[Any, Unit] </span>= &#123;</span><br><span class="line">	...</span><br><span class="line">    <span class="function"><span class="keyword">case</span> <span class="title">RemoveRdd</span><span class="params">(rddId)</span> </span>=&gt;</span><br><span class="line">      doAsync[Int](<span class="string">&quot;removing RDD &quot;</span> + rddId, context) &#123;</span><br><span class="line">        blockManager.removeRdd(rddId)</span><br><span class="line">      &#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3、Manager中的removeRdd"><a href="#3、Manager中的removeRdd" class="headerlink" title="3、Manager中的removeRdd"></a>3、Manager中的removeRdd</h2><p>根据RDD的id来拿到需要删除的块的集合，并通过foreach来进行删除操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">removeRdd</span><span class="params">(rddId: Int)</span>: Int </span>= &#123;</span><br><span class="line">	...</span><br><span class="line">    val blocksToRemove = blockInfoManager.entries.flatMap(_._1.asRDDId).filter(_.rddId == rddId)</span><br><span class="line">    blocksToRemove.foreach &#123; blockId =&gt; removeBlock(blockId, tellMaster = <span class="keyword">false</span>) &#125;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>判断是否存在，并找到块完成删除操作：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">removeBlock</span><span class="params">(blockId: BlockId, tellMaster: Boolean = <span class="keyword">true</span>)</span>: Unit </span>= &#123;</span><br><span class="line">	...</span><br><span class="line">  blockInfoManager.lockForWriting(blockId) match &#123;</span><br><span class="line">    <span class="keyword">case</span> None =&gt;</span><br><span class="line">      <span class="comment">// The block has already been removed; do nothing.</span></span><br><span class="line">    <span class="function"><span class="keyword">case</span> <span class="title">Some</span><span class="params">(info)</span> </span>=&gt;</span><br><span class="line">      removeBlockInternal(blockId, tellMaster = tellMaster &amp;&amp; info.tellMaster)</span><br><span class="line">      addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>找到块，并完成删除操作，最后向mater反馈信息。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">removeBlockInternal</span><span class="params">(blockId: BlockId, tellMaster: Boolean)</span>: Unit </span>= &#123;</span><br><span class="line">	...</span><br><span class="line">    val removedFromMemory = memoryStore.remove(blockId)</span><br><span class="line">    val removedFromDisk = diskStore.remove(blockId)</span><br><span class="line">    <span class="keyword">if</span> (!removedFromMemory &amp;&amp; !removedFromDisk) &#123;</span><br><span class="line">      logWarning(s<span class="string">&quot;Block $blockId could not be removed as it was not found on disk or in memory&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    blockInfoManager.removeBlock(blockId)</span><br><span class="line">    <span class="keyword">if</span> (tellMaster) &#123;</span><br><span class="line">      reportBlockStatus(blockId, BlockStatus.empty)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后就来到了具体的删除业务代码了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">removeBlock</span><span class="params">(blockId: BlockId)</span>: Unit </span>= <span class="keyword">synchronized</span> &#123;</span><br><span class="line">	...</span><br><span class="line">          infos.remove(blockId)</span><br><span class="line">          blockInfo.readerCount = <span class="number">0</span></span><br><span class="line">          blockInfo.writerTask = BlockInfo.NO_WRITER</span><br><span class="line">          writeLocksByTask.removeBinding(currentTaskAttemptId, blockId)</span><br><span class="line">    notifyAll()</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark中的窗口函数使用</title>
    <url>/2018/12/31/spark-zhong-de-chuang-kou-han-shu-shi-yong/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>spark自1.4版本后就支持了window function。与sql里的xxx over xxx类似，便于人们进行聚合参数。<br>本文主要是学习时的一些理解，已经遇到的一些注意点。<br>有错误望指正。</p>
<h2 id="创建window"><a href="#创建window" class="headerlink" title="创建window"></a>创建window</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//需要import这个包：</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line"><span class="comment">//按照category分组，按照revenue排序</span></span><br><span class="line">val myWindow = Window.partitionBy($<span class="string">&quot;category&quot;</span>).orderBy($<span class="string">&quot;revenue&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//之后在函数后使用.over即可：</span></span><br><span class="line">sum($<span class="string">&quot;revenue&quot;</span>).over(myWindow)</span><br></pre></td></tr></table></figure>
<p>这里就已经建立了一个简单的window函数了。</p>
<h2 id="支持的函数："><a href="#支持的函数：" class="headerlink" title="支持的函数："></a>支持的函数：</h2><p>SparkSql支持3类窗口函数：ranking functions、analytic functions、aggregate functions<br>前两者只有某些函数能够支持：</p>
<h3 id="1-Ranking-functions：（排名）"><a href="#1-Ranking-functions：（排名）" class="headerlink" title="1.Ranking functions：（排名）"></a>1.Ranking functions：（排名）</h3><table>
<thead>
<tr>
<th>SQL</th>
<th>API</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>rank</td>
<td>rank</td>
<td>排名第几，若值相等，rank相同，下一个值不同的行，rank+n</td>
</tr>
<tr>
<td>dense_rank</td>
<td>denseRank</td>
<td>排名第几，若值相等，rank相同，下一个值不同的行，rank+1</td>
</tr>
<tr>
<td>row_number</td>
<td>rowNumber</td>
<td>当前数据位于第几行，值相同，也会+1</td>
</tr>
<tr>
<td>percent_rank</td>
<td>percentRank</td>
<td>排名百分比，分组内当前行的RANK值-1/分组内总行数-1</td>
</tr>
<tr>
<td>ntile</td>
<td>ntile</td>
<td>用于将分组数据按照顺序切分成n片，返回当前切片值</td>
</tr>
</tbody></table>
<h3 id="2-Analytic-functions（解析）"><a href="#2-Analytic-functions（解析）" class="headerlink" title="2.Analytic functions（解析）"></a>2.Analytic functions（解析）</h3><table>
<thead>
<tr>
<th>SQL</th>
<th>API</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>fiset_value</td>
<td>first</td>
<td>第一条记录的value</td>
</tr>
<tr>
<td>last_value</td>
<td>last</td>
<td>最后记录的value</td>
</tr>
<tr>
<td>cume_dist</td>
<td>cume_dist</td>
<td>小于等于当前值的行数/分组内总行数</td>
</tr>
<tr>
<td>lag</td>
<td>lag</td>
<td>延迟offset位</td>
</tr>
<tr>
<td>lead</td>
<td>lead</td>
<td>领先offset位</td>
</tr>
<tr>
<td>Analytic不能理解的可以结合代码和运行结果看看：</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//window按照category分组，revenue排序</span></span><br><span class="line">readDF</span><br><span class="line">      .withColumn(<span class="string">&quot;first&quot;</span>,first(<span class="string">&quot;revenue&quot;</span>).over(myWindow))</span><br><span class="line">      .withColumn(<span class="string">&quot;last&quot;</span>,last(<span class="string">&quot;revenue&quot;</span>).over(myWindow))</span><br><span class="line">      .withColumn(<span class="string">&quot;cumeDist&quot;</span>,cume_dist().over(myWindow))</span><br><span class="line">      .withColumn(<span class="string">&quot;lag&quot;</span>,lag(<span class="string">&quot;revenue&quot;</span>,<span class="number">1</span>,<span class="number">0</span>).over(myWindow))</span><br><span class="line">      .withColumn(<span class="string">&quot;lead&quot;</span>,lead(<span class="string">&quot;revenue&quot;</span>,<span class="number">1</span>,<span class="number">0</span>).over(myWindow))</span><br><span class="line">      .show()</span><br></pre></td></tr></table></figure>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/1.png" alt="Analytic"></p>
<h3 id="3-Aggregate（聚合）"><a href="#3-Aggregate（聚合）" class="headerlink" title="3. Aggregate（聚合）"></a>3. Aggregate（聚合）</h3><p>所有支持的函数都能够使用窗口函数（sum、avg、max、min），一般会结合row/rangeBetween来一起使用。</p>
<h2 id="window函数"><a href="#window函数" class="headerlink" title="window函数"></a>window函数</h2><p>前两个意义比较简单：</p>
<h3 id="1-partitionBy"><a href="#1-partitionBy" class="headerlink" title="1. partitionBy"></a>1. partitionBy</h3><p>按照哪个值进行分区。其实就是建立一个窗口，范围是分区。</p>
<h3 id="2-orderBy"><a href="#2-orderBy" class="headerlink" title="2. orderBy"></a>2. orderBy</h3><p>按照哪个值在分区内进行排序。</p>
<h3 id="3-rowBetween"><a href="#3-rowBetween" class="headerlink" title="3. rowBetween"></a>3. rowBetween</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val myWindow = Window.partitionBy($<span class="string">&quot;category&quot;</span>).orderBy($<span class="string">&quot;revenue&quot;</span>)</span><br><span class="line">				.rowsBetween(-<span class="number">1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>图引用自：<a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html">Introducing Window Functions in Spark SQL</a><br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231350.png" alt="rowBetween"><br>以当前行为标准，将前1行和后1行的数据组成一个窗口，size=3。<br>这个比较好理解。<br>示例看下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">readDF.withColumn(<span class="string">&quot;sum&quot;</span>,sum(<span class="string">&quot;revenue&quot;</span>).over(myWindow))</span><br></pre></td></tr></table></figure>
<p>结果：sum会计算当前行+前一行+后一行的和。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/2.png" alt="Aggregate"></p>
<h3 id="4-rangeBetween"><a href="#4-rangeBetween" class="headerlink" title="4. rangeBetween"></a>4. rangeBetween</h3><p>根据当前值，来确定窗口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val myWindow = Window.partitionBy($<span class="string">&quot;category&quot;</span>).orderBy($<span class="string">&quot;revenue&quot;</span>)</span><br><span class="line">				.rangeBetween(-<span class="number">2000</span>,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231424.png" alt="rangeBetween">几个注意点：<br>    1. 因为会使用值来加减确定窗口，所以sum(colA).over(window)，这个sum必须是值类型。<br>    2. 如果前方进行你是倒序排序，整个窗口会反过来。即=&gt;(-1000,2000) 应该理解成 (-2000,1000)的窗口。</p>
<p>看下示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">readDF.withColumn(<span class="string">&quot;sum&quot;</span>,sum(<span class="string">&quot;revenue&quot;</span>).over(myWindow)).show()</span><br><span class="line"><span class="comment">//示例的window范围：（-1000，2000）</span></span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/3.png" alt="rangeBetween"></p>
<p>参考：<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html">1.Introducing Window Functions in Spark SQL</a></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark中调用C的动态库</title>
    <url>/2021/07/26/spark-zhong-diao-yong-c-de-dong-tai-ku/</url>
    <content><![CDATA[<p>Spark是基于Scala开发的，通过Java的Native方式，实现对C程序的调用。本文主要记录一些使用中遇到的问题，并不会进行深入探讨。</p>
<span id="more"></span>

<blockquote>
<p>由于项目需要，需要在Spark的Driver与Executor端调用C的动态库(*.so)，不过某些细节网上资料并没有说明清楚，故记录实际实现中遇到的一些问题。</p>
</blockquote>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>可参考<a href="http://icejoywoo.github.io/2018/07/25/spark-jni.html#spark-shell-%E6%B5%8B%E8%AF%95">在 Apache Spark 中使用 JNI 调用 C/C++ 代码</a>。</p>
<p>后续代码也会参考这篇文章。</p>
<p>本文暂时不做具体赘述。参考文章很详细了。仅仅记录一些参考该文章遇到的问题，或是原文没特别提及的内容。</p>
<h1 id="Java类需要注意"><a href="#Java类需要注意" class="headerlink" title="Java类需要注意"></a>Java类需要注意</h1><h2 id="定义Java类时需要注意"><a href="#定义Java类时需要注意" class="headerlink" title="定义Java类时需要注意"></a>定义Java类时需要注意</h2><p>这里定义的Java类包含了调用C时的native方法</p>
<ul>
<li><p>定义的类需要实现 Serializable，否则可能在 Spark 程序中出现序列化错误。</p>
</li>
<li><p>类必须是 public。</p>
</li>
<li><p>建议在类中包含<code>System.loadLibrary(&quot;myCpp&quot;);</code></p>
<ul>
<li>```java<br>static {<pre><code>System.loadLibrary(&quot;myCpp&quot;);
</code></pre>
}<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  - `myCpp`后续会使用到。</span><br><span class="line"></span><br><span class="line">参考实现：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">// filename: Base64.java</span><br><span class="line">package org.example.jni;</span><br><span class="line"></span><br><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">public class Base64 implements Serializable &#123;</span><br><span class="line">    static &#123;</span><br><span class="line">        System.loadLibrary(&quot;base64&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Base64() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    public native static String encode(String data);</span><br><span class="line">    public native static String decode(String data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="编译Java类时需要注意"><a href="#编译Java类时需要注意" class="headerlink" title="编译Java类时需要注意"></a>编译Java类时需要注意</h2><blockquote>
<p>这里的类指包含了调用C时的native方法的类</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 编译，生成 .class 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 任意目录均可</span></span><br><span class="line">javac Base64.java</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成头文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要与org同级目录(ls结果有org)执行，测试下无法在jni目录下执行。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要完整的package名</span></span><br><span class="line">javah org.example.jni.Base64</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成的头文件会在org同级目录</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果有额外的依赖，可以通过 -cp 来指定依赖的 jar 包路径或 class 文件路径。</span></span><br><span class="line">javac -cp $class_path:. Base64.java</span><br><span class="line">javah -cp $class_path:. org.example.jni.Base64</span><br></pre></td></tr></table></figure>



<h1 id="编译动态库需要注意"><a href="#编译动态库需要注意" class="headerlink" title="编译动态库需要注意"></a>编译动态库需要注意</h1><blockquote>
<p>请根据系统来编译C。这部分C的伙伴比较熟悉。故简单写一下</p>
</blockquote>
<ul>
<li><p>不同系统需要使用/生成不一样的动态库</p>
<ul>
<li>Mac系统需要使用*.jnilib</li>
<li>Linux系统一般是*.so</li>
</ul>
</li>
<li><p>makefile里有些内容需要注意</p>
<ul>
<li><p>```makefile<br>CC=g++<br>CFLAGS=-Wall<br>OBJS=Base64.o<br>all: libbase64.so</p>
<p>Base64.o: Base64.cpp</p>
<pre><code>$(CC) $(CFLAGS) -I$&#123;JAVA_HOME&#125;/include \
        -I$&#123;JAVA_HOME&#125;/include/linux \
        -I. \
  -fpic -c $&lt; -o $@
</code></pre>
<p>libbase64.so: $(OBJS)</p>
<pre><code>g++ -shared -o $@ $(OBJS)
rm -f $(OBJS)
</code></pre>
<p>.PHONY : clean<br>clean:</p>
<pre><code>rm -f $(OBJS) libbase64.so
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  - 对于第7行后半部分：`-I$&#123;JAVA_HOME&#125;/include`，实际是需要指向`jni.h`所在目录。</span><br><span class="line"></span><br><span class="line">  - 对于第8行：`-I$&#123;JAVA_HOME&#125;/include/linux`，实际是需要指向`jni_md.h`所在目录。</span><br><span class="line"></span><br><span class="line">    - 对于Mac系统，则一般需要修改为`-I$&#123;JAVA_HOME&#125;/include/darwin`</span><br><span class="line"></span><br><span class="line">- 产生的文件需要参考Java类的定义</span><br><span class="line"></span><br><span class="line">  - ```java</span><br><span class="line">    static &#123;</span><br><span class="line">        System.loadLibrary(&quot;myCpp&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
<li><p>如果这里写的<code>myCpp</code>，则生成的动态库文件必须为：</p>
<ul>
<li>Mac下：<code>libmyCpp.jnilib</code></li>
<li>Linux下：<code>libmyCpp.so</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Spark中使用需要注意"><a href="#Spark中使用需要注意" class="headerlink" title="Spark中使用需要注意"></a>Spark中使用需要注意</h1><ul>
<li>包含Java类的jar包需要作为依赖调用</li>
<li>所有动态库需要作为文件上传。<ul>
<li>可以在submit时，使用<code>--files *.so</code>上传。并同时设置两个参数：<ul>
<li><code>--conf spark.executor.extraJavaOptions=&#39;-Djava.library.path=./&#39;</code></li>
<li><code>--conf spark.driver.extraJavaOptions=&#39;-Djava.library.path=./&#39;</code></li>
<li>其路径需要指向<code>*.so</code>所在的目录。</li>
</ul>
</li>
<li>也可以放在hdfs上，在spark代码里通过<code>spark.sparkContext.addFile(&quot;hdfs:///c-lib/libbase64.so&quot;)</code>加载亦可。<ul>
<li>如果只是在executor中使用，则不需要设置额外参数。</li>
<li>如果通过这种途径，要在Driver中使用，需要一些额外操作<ul>
<li>在submit时，需要<code> --conf spark.driver.extraJavaOptions=&#39;-Djava.library.path=/tmp/spark-test/c-lib/&#39;</code>。并且在代码里，通过<code>SparkFiles.get(&quot;libbase64.so&quot;)</code>拿到实际动态库的位置，移动到设置的<code>/tmp/spark-test/c-lib/</code>下才行。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>注：这部分也仅仅是简单测试使用后得到的结论，由于时间有限并没有针对一些疑问点进行深入测试。后续有新结论会进行补充和纠错。目前结论仅供参考。</p>
</blockquote>
<h1 id="未完整验证的已知问题"><a href="#未完整验证的已知问题" class="headerlink" title="未完整验证的已知问题"></a>未完整验证的已知问题</h1><p>当C中的*.so使用了其他*.so时，会出现无法调用的情况。建议将完整的C功能打包成一个完整的*.so。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务提交流程</title>
    <url>/2018/09/30/spark-ren-wu-ti-jiao-liu-cheng/</url>
    <content><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>通过源码，整理了下Spark-submit任务提交-&gt;执行流程。源码中其实有很多保证可靠性的判断代码，这边会将不那么核心的源码略去，所以贴的代码实际与源码会有些许区别。不过源码中的英文注释会保留重要的。<br>任务执行流程会放在另一篇博客。（马上就国庆节了~，连着写两篇）</p>
<h1 id="Spark-submit提交"><a href="#Spark-submit提交" class="headerlink" title="Spark-submit提交"></a>Spark-submit提交</h1><h2 id="提交入口"><a href="#提交入口" class="headerlink" title="提交入口"></a>提交入口</h2><p>我们通过spark-submit提交我们的任务，所以打开spark-submit这个脚本，跳到最后，可以看到脚本具体调用的类<code>org.apache.spark.deploy.SparkSubmit</code>，<code>“$@”</code>表示所有传入参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> randomized <span class="built_in">hash</span> <span class="keyword">for</span> string <span class="keyword">in</span> Python 3.3+</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br></pre></td></tr></table></figure>
<h2 id="提交作业"><a href="#提交作业" class="headerlink" title="提交作业"></a>提交作业</h2><p>1）前往<code>spark.deploy.SparkSubmit</code>,找到<code>main</code>函数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This entry point is used by the launcher library to start in-process Spark applications.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] object InProcessSparkSubmit &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val submit = <span class="keyword">new</span> SparkSubmit()</span><br><span class="line">    submit.doSubmit(args)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过注释和源码，我们可以看到这是SparkApp的启动入口，我们的参数会传入submit.doSubmit(args)。</p>
<h3 id="2）doSubmit"><a href="#2）doSubmit" class="headerlink" title="2）doSubmit"></a>2）doSubmit</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="function">def <span class="title">doSubmit</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">   <span class="comment">// 初始化日志的一些操作</span></span><br><span class="line">   val uninitLog = initializeLogIfNecessary(<span class="keyword">true</span>, silent = <span class="keyword">true</span>)</span><br><span class="line"><span class="comment">// 解析我们传入的参数。</span></span><br><span class="line">   val appArgs = parseArguments(args)</span><br><span class="line">   <span class="comment">// 判断我们的操作，我们会match上SUBMIT。</span></span><br><span class="line">   appArgs.action match &#123;</span><br><span class="line">     <span class="keyword">case</span> SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog)</span><br><span class="line">     <span class="keyword">case</span> SparkSubmitAction.KILL =&gt; kill(appArgs)</span><br><span class="line">     <span class="keyword">case</span> SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs)</span><br><span class="line">     <span class="keyword">case</span> SparkSubmitAction.PRINT_VERSION =&gt; printVersion()</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">接下来，由于我们是submit操作，理所应到会调用```submit(appArgs, uninitLog)```方法</span><br><span class="line"></span><br><span class="line">### 3）submit(appArgs, uninitLog)</span><br><span class="line">```java</span><br><span class="line">  /**</span><br><span class="line">   * Submit the application using the provided parameters, ensuring to first wrap</span><br><span class="line">   * in a doAs when --proxy-user is specified.</span><br><span class="line">   */</span><br><span class="line">  @tailrec</span><br><span class="line">  private def submit(args: SparkSubmitArguments, uninitLog: Boolean): Unit = &#123;</span><br><span class="line">	</span><br><span class="line">	def doRunMain(): Unit = &#123;...&#125;</span><br><span class="line"></span><br><span class="line">  	// 这边以常用的Yarn/local为例，所以直接进入else。if里的内容暂时省略</span><br><span class="line">    if (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">      	// 其实主要也是 doRunMain() 方法。不过会有一些其他判断。</span><br><span class="line">      &#125;</span><br><span class="line">    // 对于其他Mode，直接doRunMain</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      doRunMain()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>因为这次以local或yarn模式进行演示，这个地方我们直接就来到了 <code>doRunMain</code>方法。</p>
<h3 id="4）doRunMain"><a href="#4）doRunMain" class="headerlink" title="4）doRunMain()"></a>4）doRunMain()</h3><p>方法其实也就在这个方法里面。我们把方法具体展开：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">doRunMain</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">      <span class="keyword">if</span> (args.proxyUser != <span class="keyword">null</span>) &#123;</span><br><span class="line">        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,</span><br><span class="line">          UserGroupInformation.getCurrentUser())</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          proxyUser.doAs(<span class="keyword">new</span> PrivilegedExceptionAction[Unit]() &#123;</span><br><span class="line">            <span class="function">override def <span class="title">run</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">              runMain(args, uninitLog)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125; </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        runMain(args, uninitLog)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>主要就是判断下<code>args.proxyUser</code>，这个主要是模拟提交应用程序的用户，具体用途参考Spark的参数就好了。</p>
<h3 id="5）runMain"><a href="#5）runMain" class="headerlink" title="5）runMain()"></a>5）runMain()</h3><p>源码中会有很多判断和log信息，这边删去方便看到核心</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">runMain</span><span class="params">(args: SparkSubmitArguments, uninitLog: Boolean)</span>: Unit </span>= &#123;</span><br><span class="line">    val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</span><br><span class="line">    </span><br><span class="line">    val loader = Thread.currentThread.getContextClassLoader)</span><br><span class="line">    Thread.currentThread.setContextClassLoader(loader)</span><br><span class="line">    </span><br><span class="line">    addJarToClasspath(jar, loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> mainClass: Class[_] = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      mainClass = Utils.classForName(childMainClass)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;...&#125;</span><br><span class="line"></span><br><span class="line">    val app: SparkApplication = <span class="keyword">if</span> (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">      mainClass.newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> JavaMainApplication(mainClass)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      app.start(childArgs.toArray, sparkConf)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;...&#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这段代码其实将jar包、依赖加入<code>Classpath</code>，生成一个<code>SparkApplication</code>。调用<code>app.start</code>，将参数传入，启动app。由于<code>SparkApplication</code>是一个<code>trait</code>，它的具体实现类有三个：</p>
<ul>
<li>ClientApp </li>
<li>RestSubmissionClientApp </li>
<li>JavaMainApplication</li>
</ul>
<p>那么具体是调用哪个呢？我们先进入<code>prepareSubmitEnvironment(args)</code>，这个方法的内容非常多，我们只找我们需要的，通过观察源码，构建Spark Application的关键参数其实是<code>childMainClass</code>，我们现在找这个就行了，这是一个String类型的变量。</p>
<h3 id="6）prepareSubmitEnvironment"><a href="#6）prepareSubmitEnvironment" class="headerlink" title="6）prepareSubmitEnvironment"></a>6）prepareSubmitEnvironment</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> childMainClass = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (deployMode == CLIENT) &#123;</span><br><span class="line">      childMainClass = args.mainClass</span><br><span class="line">	&#125;</span><br><span class="line"><span class="keyword">if</span> (args.isStandaloneCluster) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args.useRest) &#123;</span><br><span class="line">        childMainClass = REST_CLUSTER_SUBMIT_CLASS</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        childMainClass = STANDALONE_CLUSTER_SUBMIT_CLASS</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (isYarnCluster) &#123;</span><br><span class="line">      childMainClass = YARN_CLUSTER_SUBMIT_CLASS</span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">// 其他模式暂时省略，有兴趣可以翻阅具体源码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上述部分变量的具体值：</span></span><br><span class="line"><span class="keyword">private</span>[deploy] val YARN_CLUSTER_SUBMIT_CLASS =</span><br><span class="line">    <span class="string">&quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;</span></span><br><span class="line"><span class="keyword">private</span>[deploy] val REST_CLUSTER_SUBMIT_CLASS = classOf[RestSubmissionClientApp].getName()</span><br><span class="line"><span class="keyword">private</span>[deploy] val STANDALONE_CLUSTER_SUBMIT_CLASS = classOf[ClientApp].getName()</span><br></pre></td></tr></table></figure>
<p>这下就很清楚了，我们的SparkApplication是在<code>runMain</code>中的这里创建的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val app: SparkApplication = <span class="keyword">if</span> (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">  mainClass.newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">new</span> JavaMainApplication(mainClass)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">而yarn并非SparkApplication的子类，则false进入创建```JavaMainApplication```</span><br><span class="line"></span><br><span class="line">### 7）JavaMainApplication</span><br><span class="line">```java</span><br><span class="line">private[deploy] class JavaMainApplication(klass: Class[_]) extends SparkApplication &#123;</span><br><span class="line"></span><br><span class="line">  override def start(args: Array[String], conf: SparkConf): Unit = &#123;</span><br><span class="line">    val mainMethod = klass.getMethod(&quot;main&quot;, new Array[String](0).getClass)</span><br><span class="line">    val sysProps = conf.getAll.toMap</span><br><span class="line">    sysProps.foreach &#123; case (k, v) =&gt;</span><br><span class="line">      sys.props(k) = v</span><br><span class="line">    &#125;</span><br><span class="line">    mainMethod.invoke(null, args)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终就是调用<code>mainMethod.invoke(null, args)</code><br>后面就是Java部分的内容了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>将主要的方法和参数用幕布整理了下。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217224352.png" alt="Spark任务提交"></p>
<p>其实提交流程简单来说可以是：</p>
<ol>
<li>解析参数。     </li>
<li>将该添加的内容添加到ClassPath。     </li>
<li>判断deploy模式。     </li>
<li>提交任务。</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark内存管理</title>
    <url>/2018/04/15/spark-nei-cun-guan-li/</url>
    <content><![CDATA[<h2 id="Spark的内存管理"><a href="#Spark的内存管理" class="headerlink" title="Spark的内存管理"></a>Spark的内存管理</h2><p>Spark的内存管理一共分为了：UnifiedMemoryManager和StaticMemoryManager。1.6以后Spark默认会使用前者。本文主要讨论在yarn上运行的情况。大体上是一样的。</p>
<blockquote>
<p>Version ：2.2.x</p>
</blockquote>
<h2 id="StaticMemoryManager："><a href="#StaticMemoryManager：" class="headerlink" title="StaticMemoryManager："></a>StaticMemoryManager：</h2><p>由于已经不被推荐使用，也不过多介绍：<br>在静态内存管理里，其实也是分了堆外内存和堆内内存，在Yarn的模式下，布局如下图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581761503848.png" alt="Executor布局"></p>
<ol>
<li><p>堆外内存：</p>
<p> 堆外内存主要由参数<code>spark.yarn.executor.memoryOverhead</code>指定，此部分为用户代码及Spark 不可操作的内存，具体了解可以看下官网介绍：</p>
<blockquote>
<p>The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</p>
</blockquote>
<p> 部分博客把这个参数和统一内存管理里的<code>spark.memory.offheap.size</code>混为一谈，这完全是两个版本里的参数。</p>
</li>
<li><p>堆内内存：<br> 这部分是主要需要关注的内存，静态内存主要分为执行内存和存储内存，默认情况下：<br>  2.1 执行内存 = SystemMemory * execution.memory.Fraction(0.2) * executor.safetyFraction(0.8)<br>  2.2 存储内存 = SystemMemory * storage.memory.Fraction(0.2) * storage.safetyFraction(0.8)<br>  2.3 其中SystemMemory可以近似小于executor-memory，对于不同的JVM会略有区别。</p>
</li>
</ol>
<h2 id="UnifiedMemoryManager："><a href="#UnifiedMemoryManager：" class="headerlink" title="UnifiedMemoryManager："></a>UnifiedMemoryManager：</h2><p>1.6后默认使用UnifiedMemoryManager，简单来说就是执行内存和存储内存能够相互借用。对于统一的内存管理，也分为堆内内存和堆外内存。</p>
<h3 id="堆内内存："><a href="#堆内内存：" class="headerlink" title="堆内内存："></a>堆内内存：</h3><p>堆内内存主要还是分为执行内存和存储内存，另外还有两块空间：UserMemory，Reserved Memory。具体图示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762131167.png" alt="统一内存管理"></p>
<table>
<thead>
<tr>
<th>空间</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>execution</td>
<td>执行内存，计算、shuffle时使用</td>
</tr>
<tr>
<td>storage</td>
<td>存储内存，用来Cache数据</td>
</tr>
<tr>
<td>user memory</td>
<td>存储转换时的依赖信息。</td>
</tr>
<tr>
<td>reserved memory</td>
<td>用来存储Spark内部对象。</td>
</tr>
</tbody></table>
<p>通过观察UnifiedMemoryManager的源码，我们就能够得到这几块内存区域的计算公式：</p>
<table>
<thead>
<tr>
<th>区域</th>
<th>计算来源</th>
</tr>
</thead>
<tbody><tr>
<td>系统内存（SystemMemory）</td>
<td>近似小于executor-memory</td>
</tr>
<tr>
<td>预留内存（reservedMemory）</td>
<td>testing.reservedMemory（默认300M）</td>
</tr>
<tr>
<td>可用最大内存（MaxMemory）</td>
<td>（SystemMemory-reservedMemory）*memory.fraction(默认0.6)</td>
</tr>
<tr>
<td>用户内存</td>
<td>（SystemMemory-reservedMemory）*（1-memory.fraction)</td>
</tr>
<tr>
<td>执行内存</td>
<td>MaxMemory * 0.5</td>
</tr>
<tr>
<td>存储内存</td>
<td>MaxMemory * 0.5</td>
</tr>
</tbody></table>
<h3 id="堆外内存："><a href="#堆外内存：" class="headerlink" title="堆外内存："></a>堆外内存：</h3><p>堆外内存只分：执行内存和存储内存。功能上与堆内内存互补。两者的比例也是通过参数 <code>spark.memory.storageFraction</code> 控制。</p>
<p>相关参数：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>参数</th>
</tr>
</thead>
<tbody><tr>
<td>开启</td>
<td>spark.memory.offHeap.enabled</td>
</tr>
<tr>
<td>总大小</td>
<td>spark.memory.offHeap.size（只由这个参数控制，submit是那个参数只控制堆内内存）</td>
</tr>
<tr>
<td>比例</td>
<td>spark.memory.storageFraction</td>
</tr>
</tbody></table>
<h3 id="借用规则"><a href="#借用规则" class="headerlink" title="借用规则"></a>借用规则</h3><p>统一内存管理的有点就在于能够相互借用，具体规则其实很简单：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762641262.png" alt="借用规则"></p>
<ol>
<li>Executor和Storage都能够相互借用对方的内存，但以executor优先。</li>
<li>Storage需要借用时，只能借用executor释放后的内存。</li>
<li>Execution需要借用时，则可以主动要求storage释放占用的己方内存。</li>
<li>这种借用关系存在于堆外和堆内，但堆外不能借堆内，反之亦然。</li>
</ol>
<h4 id="从Spark的源码看两者的借用逻辑："><a href="#从Spark的源码看两者的借用逻辑：" class="headerlink" title="从Spark的源码看两者的借用逻辑："></a>从Spark的源码看两者的借用逻辑：</h4><ol>
<li>acquireStorageMemory</li>
</ol>
<p>核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">  // There is not enough free memory in the storage pool, so try to borrow free memory from</span><br><span class="line">  // the execution pool.</span><br><span class="line">  val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree,</span><br><span class="line">    numBytes - storagePool.memoryFree)</span><br><span class="line">  executionPool.decrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">  storagePool.incrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里就能够看到，但己方内存不够用时，Storage最终的借用内存。<code>Math.min(executionPool.memoryFree,      numBytes - storagePool.memoryFree)</code>，前者为Execution池中的空闲内存。</p>
<ol start="2">
<li>acquireExecutionMemory</li>
</ol>
<p>核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">  if (extraMemoryNeeded &gt; 0) &#123;</span><br><span class="line">    ...</span><br><span class="line">    if (memoryReclaimableFromStorage &gt; 0) &#123;</span><br><span class="line">      val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">        math.min(extraMemoryNeeded, memoryReclaimableFromStorage))</span><br><span class="line">      storagePool.decrementPoolSize(spaceToReclaim)</span><br><span class="line">      executionPool.incrementPoolSize(spaceToReclaim)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">def computeMaxExecutionPoolSize(): Long = &#123;</span><br><span class="line">  maxMemory - math.min(storagePool.memoryUsed, storageRegionSize)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">while (true) &#123;</span><br><span class="line">      ...</span><br><span class="line">      maybeGrowPool(numBytes - memoryFree)</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      ...</span><br><span class="line">      val toGrant = math.min(maxToGrant, memoryFree)</span><br><span class="line">      if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;</span><br><span class="line">        ...</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        memoryForTask(taskAttemptId) += toGrant</span><br><span class="line">        return toGrant</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这段具体的计算公式我们可以不过多关注，其实官方注释一句话就把这段逻辑解释清楚了，通过：</p>
<blockquote>
<p>The size the execution pool would have after evicting storage memory.</p>
</blockquote>
<p>上述代码在计算好待释放内存后，通过<code>storagePool.freeSpaceToShrinkPool</code> -&gt; <code>memoryStore.evictBlocksToFreeSpace</code>来完成内存释放。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Typesafe的ConfigFactory的简单使用</title>
    <url>/2017/11/26/typesafe-de-configfactory-de-jian-dan-shi-yong/</url>
    <content><![CDATA[<h2 id="ConfigFactory"><a href="#ConfigFactory" class="headerlink" title="ConfigFactory"></a>ConfigFactory</h2><p>ConfigFactory能够帮助我们快速的读取配置文件，帮助我们修改一些配置信息，而不用修改代码。<br>例如我们可以将sql的连接信息放入配置文件中，而不是写死在代码里。<br>类似于Java的properties，不过ConfigFactory支持多种文件格式，包括：</p>
<blockquote>
<p>The convenience method ConfigFactory.load() loads the following (first-listed are higher priority):<br>a) system properties<br>b) system properties application.conf (all resources on classpath with this name)<br>c) application.json (all resources on classpath with this name)<br>d) application.properties (all resources on classpath with this name)<br>e) reference.conf (all resources on classpath with this name)</p>
</blockquote>
<p>其他的优点，用途，用法大家可以参见<a href="https://github.com/lightbend/config">github项目</a>。<br>里面的<code>README.md</code>进行了详细的说明。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="引入依赖："><a href="#引入依赖：" class="headerlink" title="引入依赖："></a>引入依赖：</h3><p>需要注意：1.2.1及以前的版本是基于Java6开发的，而1.3及之后的版本都需要基于1.8。</p>
<blockquote>
<p>Version 1.2.1 and earlier were built for Java 6, while newer versions(1.3.0 and above) will be built for Java 8.</p>
</blockquote>
<p>以maven为例：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>config<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="获取配置文件"><a href="#获取配置文件" class="headerlink" title="获取配置文件"></a>获取配置文件</h3><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217220011.png" alt="获取配置文件"><br>使用<code>load</code>方法可以加载到实现准备好的配置文件，默认加载<code>classpath</code>下的<code>application.conf</code>，<code>application.json</code>和<code>application.properties</code>文件。当然也可以调用<code>ConfigFactory.load(confName)</code>加载指定的配置文件。<br>获取到之后通过getxxx方法，就能够提取里面的值了。本文主要介绍下简单使用，更复杂的使用请参考:<a href="https://github.com/lightbend/config">github项目</a>。</p>
<h3 id="常见配置文件的书写规则："><a href="#常见配置文件的书写规则：" class="headerlink" title="常见配置文件的书写规则："></a>常见配置文件的书写规则：</h3><p>上面提到过，ConfigFactory支持多种配置文件格式，常见的有<code>application.conf</code>，<code>application.json</code>和<code>application.properties</code>文件。</p>
<h4 id="application-conf"><a href="#application-conf" class="headerlink" title="application.conf"></a>application.conf</h4><p>一般使用k=v的形式即可：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.url=<span class="string">&quot;jdbc:mysql://localhost:3306&quot;</span></span><br><span class="line">db.<span class="keyword">default</span>.name=<span class="string">&quot;dbname&quot;</span></span><br><span class="line">db.<span class="keyword">default</span>.passwd=<span class="number">123456</span></span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">conf=ConfigFactory.load(confName)</span><br><span class="line">conf.getString(<span class="string">&quot;db.default.url&quot;</span>))</span><br><span class="line">conf.getInt(<span class="string">&quot;db.default.passwd&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>也可以使用这种配置形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mysql-app &#123;</span><br><span class="line">    url=<span class="string">&quot;jdbc:mysql://localhost:3306&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">conf=ConfigFactory.load(confName)</span><br><span class="line">conf.getString(<span class="string">&quot;mysql-app.url&quot;</span>))</span><br></pre></td></tr></table></figure>

<h4 id="application-json"><a href="#application-json" class="headerlink" title="application.json"></a>application.json</h4><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">&quot;db&quot;</span>:&#123;</span><br><span class="line">	<span class="attr">&quot;url&quot;</span>:<span class="string">&quot;jdbc:mysql://localhost:3306&quot;</span></span><br><span class="line">	<span class="string">&quot;dbname&quot;</span>:<span class="string">&quot;dbname&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">conf.getString(<span class="string">&quot;db.url&quot;</span>))</span><br><span class="line">conf.getString(<span class="string">&quot;db.dbname&quot;</span>))</span><br></pre></td></tr></table></figure>

<h4 id="application-properties"><a href="#application-properties" class="headerlink" title="application.properties"></a>application.properties</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.url=jdbc:mysql:<span class="comment">//localhost:3306</span></span><br><span class="line"><span class="comment">//使用同上。</span></span><br></pre></td></tr></table></figure>

<h3 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object mysqlApp &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder()<span class="comment">//.appName(this.getClass.getSimpleName).master(&quot;local[*]&quot;)</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(args.size != <span class="number">2</span>)&#123;</span><br><span class="line">      println(<span class="string">&quot;args error////inputPath///confPath&quot;</span>)</span><br><span class="line">      System.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">val <span class="title">Array</span><span class="params">(intput,conf)</span> </span>= args</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 文件内容：</span></span><br><span class="line"><span class="comment"> * a,1,A</span></span><br><span class="line"><span class="comment"> * b,1,B</span></span><br><span class="line"><span class="comment"> * c,1,C</span></span><br><span class="line"><span class="comment"> */</span>	</span><br><span class="line">    val readDF = spark.read.format(<span class="string">&quot;text&quot;</span>).load(intput)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df = readDF.rdd.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.getString(<span class="number">0</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      (splits(<span class="number">0</span>), splits(<span class="number">1</span>), splits(<span class="number">2</span>))</span><br><span class="line">    &#125;).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.write.options(getDBConf(conf))</span><br><span class="line">      .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">      .format(<span class="string">&quot;jdbc&quot;</span>).save()</span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">getDBConf</span><span class="params">(confPlace:String=<span class="string">&quot;application.conf&quot;</span>)</span></span>=&#123;</span><br><span class="line">      val config = ConfigFactory.load(confPlace)</span><br><span class="line">      val url = config.getString(<span class="string">&quot;db.default.url&quot;</span>)</span><br><span class="line">      val configMap = HashMap[String, String](</span><br><span class="line">        (<span class="string">&quot;url&quot;</span>, config.getString(<span class="string">&quot;db.default.url&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;db&quot;</span>, config.getString(<span class="string">&quot;db.default.name&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;dbtable&quot;</span>, config.getString(<span class="string">&quot;db.default.table&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;user&quot;</span>, config.getString(<span class="string">&quot;db.default.user&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;password&quot;</span>, config.getString(<span class="string">&quot;db.default.password&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;driver&quot;</span>,config.getString(<span class="string">&quot;db.default.driver&quot;</span>))</span><br><span class="line">      )</span><br><span class="line">      configMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn工作流程(Spark cluster)</title>
    <url>/2017/02/10/yarn-gong-zuo-liu-cheng/</url>
    <content><![CDATA[<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217141526.png" alt="Spark on Yarn-cluster"></p>
<ol>
<li>客户端向yarn提交作业，yarn会启动ApplicationsManager。</li>
<li>RM为作业分配NM，启动ApplicationMaster（AM）</li>
<li>AM向ApplicationManager注册，并向ResourceScheduler申请资源。</li>
<li>ResourceScheduler将打包好的资源信息返回给AM，AM通知对应的NodeManager启动Container。</li>
<li>Container启动成功后，会保持与AM的通信，直到任务完成。</li>
<li>所有任务完成后，AM向ApplicationManager申请注销资源。</li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn资源调优</title>
    <url>/2017/12/01/yarn-zi-yuan-diao-you/</url>
    <content><![CDATA[<h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><p>Yarn的资源设置主要是6个参数，目的其实就是资源最大化。同时限制下面任务如某个Spark任务过度调用资源。</p>
<h2 id="CPU资源调度"><a href="#CPU资源调度" class="headerlink" title="CPU资源调度"></a>CPU资源调度</h2><p>目前的CPU被划分为虚拟CPU（CPU virtual Core）考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，可以通过多配置几个虚拟CPU弥补差异。CPU的相关配置如下：</p>
<ol>
<li><p>yarn.nodemanager.resource.cpu-vcores<br>表示该节点服务器上yarn可以使用的虚拟CPU个数，默认是8。</p>
<p>推荐将值配置与物理核心个数相同。如果CPU的性能好可以调大。</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-vcores<br>单个任务最小可申请的虚拟核心数，默认为1。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-vcores<br>单个任务最大可申请的虚拟核心数，默认为4，如果申请资源时，超过这个配置，会抛出InvalidResourceRequestException</p>
</li>
</ol>
<h2 id="Memory资源调度"><a href="#Memory资源调度" class="headerlink" title="Memory资源调度"></a>Memory资源调度</h2><p>yarn一般允许用户配置每个节点上可用的物理资源，集群上除了跑作业，还要留出资源给Hive、HDFS等其他服务。</p>
<ol>
<li><p>yarn.nodemanager.resource.memory-mb<br>设置该节点上yarn可使用的内存，默认为8G。</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb<br>单个任务最小申请物理内存量，默认1024MB。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-mb<br>单个任务最大申请物理内存量，默认为8291MB</p>
</li>
</ol>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>如果有一个服务器16核，64G内存，我们应该如何配置上面的6个参数呢（一句话：资源最大化利用）</p>
<ol>
<li><p>yarn.nodemanager.resource.cpu-vcores </p>
<p>虚拟CPU核数<br>根据具体的服务器水平去设置。即虚拟CPU的核数。我们生产设置的是32。</p>
</li>
<li><p>yarn.nodemanager.resource.memory-mb </p>
<p>总内存<br>生产上建议预留15%~20%的内存，所以我们设置的50G</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb </p>
<p>单任务最小内存<br>如果设置成2G，就是最多可以跑25个container<br>如果设置成3G，就是最多可以跑16个container</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb </p>
<p>单任务最少虚拟Core<br>如果设置vcore = 1，就是最多可以跑32个container，但如果设置成这个，如果上面分配的是2G内存。最多只能跑25个container，就有点浪费了。<br>如果设置vcore = 2，就是最多可以跑16个container，内存就能用3G，比较均匀化</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-vcores</p>
<p>单任务最多虚拟Core<br>一般就设置成4个，cloudera公司做过性能测试。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-mb </p>
<p>单任务最大内存<br>如果有大任务，需要5-6G内存，那就设置为8G</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>go中的测试与性能调优</title>
    <url>/2021/07/04/go-zhong-de-ce-shi-yu-xing-neng-diao-you/</url>
    <content><![CDATA[<h2 id="表格驱动测试"><a href="#表格驱动测试" class="headerlink" title="表格驱动测试"></a>表格驱动测试</h2><ul>
<li>分离测试数据与测试逻辑</li>
<li>明确出错信息</li>
<li>go语言语法允许更好的使用表格驱动测试。</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 测试类</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestAdd</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">  <span class="comment">// 构建测试逻辑</span></span><br><span class="line">	testDic := []<span class="keyword">struct</span>&#123; a, b, c <span class="keyword">int</span> &#125;&#123;</span><br><span class="line">		&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;,</span><br><span class="line">		&#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">9</span>&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">  <span class="comment">// 通过for构建测试</span></span><br><span class="line">	<span class="keyword">for</span> _, ele := <span class="keyword">range</span> testDic &#123;</span><br><span class="line">		<span class="keyword">if</span> act := ele.a + ele.b; act != ele.c &#123;</span><br><span class="line">      <span class="comment">// 不通过报错会提示，并将测试视为失败，但不会中断测试</span></span><br><span class="line">			t.Errorf(<span class="string">&quot;error, act is %d, expected %d&quot;</span>, act, ele.c)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>方法必须使用<code>TestAbc</code>命名。</p>
</blockquote>
<h2 id="在命令行使用测试"><a href="#在命令行使用测试" class="headerlink" title="在命令行使用测试"></a>在命令行使用测试</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd #测试代码所在目录</span><br><span class="line">go test . # 测试所有需要测试的代码</span><br></pre></td></tr></table></figure>

<blockquote>
<p>效果：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210704140213.png" alt="image-20210704140213338"></p>
</blockquote>
<h3 id="代码覆盖率测试"><a href="#代码覆盖率测试" class="headerlink" title="代码覆盖率测试"></a>代码覆盖率测试</h3><p>可以通过生成一个文件，来观察测试代码覆盖的代码。找到哪些代码没有被测试到.</p>
<blockquote>
<p>也可以直接使用idea提供的方法查看覆盖测试结果</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">go test -coverprofile=test.out</span><br><span class="line">go tool cover -html test.out</span><br></pre></td></tr></table></figure>



<h3 id="代码性能测试"><a href="#代码性能测试" class="headerlink" title="代码性能测试"></a>代码性能测试</h3><p>使用<code>testing.B</code>可以进行性能测试</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkAdd</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">	a, c := <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">	ans := <span class="number">3</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">		<span class="keyword">if</span> act := Add(a, c); act != ans &#123;</span><br><span class="line">			b.Errorf(<span class="string">&quot;error, act is %d, expected %d&quot;</span>, act, ans)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>方法必须使用<code>BenchmarkABC</code>命名</p>
</blockquote>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go中的自带的常见库的使用</title>
    <url>/2021/07/09/go-zhong-de-zi-dai-de-chang-jian-ku-de-shi-yong/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>go的函数式编程</title>
    <url>/2021/06/30/go-de-han-shu-shi-bian-cheng/</url>
    <content><![CDATA[<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>当函数作为返回值时，会讲该函数所携带的状态一起返回。</p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go入门</title>
    <url>/2021/06/20/go-ru-men/</url>
    <content><![CDATA[<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 数据类型在后。与scala类似（不过不需要引号）</span></span><br><span class="line"><span class="comment">// 所有数据会有个初始值。int为0</span></span><br><span class="line"><span class="keyword">var</span> a <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 能够自动进行类型推断。属性可省略</span></span><br><span class="line"><span class="keyword">var</span> a = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以使用 := 定义变量</span></span><br><span class="line">a := <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 包变量（方法外变量，类似于java的全局变量），必须使用var定义，而不能使用 :=</span></span><br><span class="line"><span class="keyword">var</span> aa = <span class="string">&quot;a&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义多个变量时。可以使用 var()</span></span><br><span class="line"><span class="keyword">var</span>(</span><br><span class="line">	a = <span class="number">1</span></span><br><span class="line">  b = <span class="string">&quot;a&quot;</span></span><br><span class="line">  c = <span class="string">&quot;d&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// 也可以使用如下方式同时定义</span></span><br><span class="line"><span class="keyword">var</span> aa,bb,cc = <span class="number">1</span>,<span class="string">&quot;2&quot;</span>,<span class="literal">true</span></span><br><span class="line">aa,bb,cc := <span class="number">1</span>,<span class="string">&quot;2&quot;</span>,<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 变量定义完后类型是确定的。go不会进行隐式数据类型转换</span></span><br><span class="line"><span class="comment">// 如使用math.Sqrt(x)时，x必须传入float</span></span><br><span class="line">a, b := <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">math.Sqrt(<span class="keyword">float64</span>(a*a + b*b))</span><br><span class="line"></span><br><span class="line"><span class="comment">// go中，定义了的变量必须要用到</span></span><br></pre></td></tr></table></figure>

<h3 id="常量定义"><a href="#常量定义" class="headerlink" title="常量定义"></a>常量定义</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用const定义常量，类似于scala中的val，java中的final</span></span><br><span class="line"><span class="comment">// int可以省略</span></span><br><span class="line"><span class="comment">// go中首字母大写有含义，故常量不会全部大写。</span></span><br><span class="line"><span class="keyword">const</span> a <span class="keyword">int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 未指定int时，a能够被识别为数字数据类型</span></span><br><span class="line"><span class="comment">// 不需要像var一样强制转换</span></span><br><span class="line"><span class="keyword">const</span> a, b = <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">math.Sqrt(a*a + b*b)</span><br><span class="line"></span><br><span class="line"><span class="comment">// go中没有枚举类型。可利用const定义枚举</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">enums</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">const</span> (</span><br><span class="line">  	cpp = <span class="number">1</span></span><br><span class="line">    java = <span class="number">2</span></span><br><span class="line">  )</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义有递增关系的枚举。可使用iota</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	a = <span class="literal">iota</span> <span class="comment">// 0</span></span><br><span class="line">  b				 <span class="comment">// 1</span></span><br><span class="line">  c				 <span class="comment">// 2</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// 也可以使用公式</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  aa = <span class="number">2</span> * (<span class="number">1</span> + <span class="literal">iota</span>) <span class="comment">// 2 </span></span><br><span class="line">  bb                  <span class="comment">// 4</span></span><br><span class="line">  cc                  <span class="comment">// 6</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="数据类型-1"><a href="#数据类型-1" class="headerlink" title="数据类型"></a>数据类型</h3><p>go的内建类型与java等有一定区别。</p>
<ul>
<li><code>bool</code>， <code>string</code>， <code>byte</code><ul>
<li>这几类与一般语言类似</li>
</ul>
</li>
<li><code>(u)int</code>，<code>(u)int8/16/32/64</code>，<code>uintptr</code><ul>
<li><code>(u)int</code>为不定长整形。具体范围视设备/运行的操作系统决定。带u的为无符号整型。</li>
<li><code>(u)int8/16/32/64</code>为定长整型。</li>
<li><code>uintptr</code>为指针<ul>
<li><code>var i = *T</code>：变量<code>i</code>是一个指向T类型值的指针。</li>
<li><code>t := &quot;1&quot; ; r := &amp;t</code>：<code>t</code>是一个字符串变量，<code>r</code>表示一个指向变量t的指针</li>
<li><code>l := *r</code>：<code>r</code>若为指针，则变量<code>l</code>表示表示指针指向变量的本身，即<code>t</code></li>
</ul>
</li>
</ul>
</li>
<li><code>float32/64</code>，<code>complex64/128</code><ul>
<li><code>float32/64</code>为浮点数</li>
<li><code>complex64/128</code>为复数。实部虚部均为<code>float32/64</code></li>
</ul>
</li>
<li><code>rune</code><ul>
<li>字符型。类似于java中的<code>char</code>。但有一些区别</li>
</ul>
</li>
</ul>
<h2 id="条件语句"><a href="#条件语句" class="headerlink" title="条件语句"></a>条件语句</h2><p>go中使用<code>if</code>和<code>swith-case</code>作为条件语句</p>
<h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p><code>if</code>的用法和大多数语言相似。有几个区别：</p>
<ul>
<li><p>判断不需要加小括号。</p>
</li>
<li><p>可以在<code>if</code>后直接跟变量定义</p>
</li>
<li><p>```go<br>// 不需要小括号<br>if a == 3 {<br>  // do something1<br>}else if a == 2 {<br>  // do something2<br>}else {<br>  // do something3<br>}</p>
<p>// if后跟定义. a 只能在if的作用域中使用<br>if a := func1(); a == 1 {<br>  fmt.Println(a)<br>}else if a == 2 {<br>  // do something2<br>}else{<br>  // do something3<br>}</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### swith</span><br><span class="line"></span><br><span class="line">`switch`用法与java类似。只是不需要在每句后break</span><br><span class="line"></span><br><span class="line">```go</span><br><span class="line">// 可以switch某一个值</span><br><span class="line">a := 1</span><br><span class="line">switch a &#123;</span><br><span class="line">  case 1:</span><br><span class="line">	  fmt.Println(&quot;A&quot;)</span><br><span class="line">  case 2:</span><br><span class="line">	  fmt.Println(&quot;B&quot;)</span><br><span class="line">  default:</span><br><span class="line">	  fmt.Println(&quot;60&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 可以直接在case后跟条件</span><br><span class="line">a := 1</span><br><span class="line">switch &#123;</span><br><span class="line">case a &lt; 60:</span><br><span class="line">	fmt.Println(&quot;A&quot;)</span><br><span class="line">case a &gt; 60:</span><br><span class="line">	fmt.Println(&quot;B&quot;)</span><br><span class="line">default:</span><br><span class="line">	fmt.Println(&quot;60&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><p>go中只有<code>for</code>一种循环。与java的<code>fori</code>类似，分为三段。但这三段可以任意省略。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">30</span> ; i++ &#123;</span><br><span class="line">  <span class="comment">// do something</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略定义i，则可以使用外部变量</span></span><br><span class="line">i := <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> ; i &lt; <span class="number">30</span> ; i++ &#123;</span><br><span class="line">  <span class="comment">// do something</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再省略递增处，则类似于一般语言的while循环</span></span><br><span class="line"><span class="comment">// 这种情况可以省略;号</span></span><br><span class="line">i := <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i &lt; <span class="number">30</span> &#123;</span><br><span class="line">  <span class="comment">// like while</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略退出条件。则是死循环</span></span><br><span class="line">i := <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> ; ; i++&#123;</span><br><span class="line">  fmt.Println(i)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 可以全省略</span></span><br><span class="line">i := <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">  fmt.<span class="built_in">println</span>(i)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>go的函数有几个特点：</p>
<ul>
<li><code>func</code>关键字</li>
<li>返回类型在最后（类似于scala），但函数除非不需要返回值，否则必须设定返回值（不能类似于变量那样自动推断）</li>
<li>可以返回多个返回值（写法类似于scala的元组，但必须用多个变量去接收）<ul>
<li>返回值也能够进行命名。</li>
</ul>
</li>
<li>不能使用<code>const</code>去接收函数的返回值？似乎是，存疑</li>
<li>参数部分，同类型的参数可以放一起，省略类型命名</li>
<li>对于不想要的返回值，可以使用<code>_</code>来接收</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 参数部分，同类型的参数可以放一起，省略类型命名</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">eval</span><span class="params">(a, b <span class="keyword">int</span>, op <span class="keyword">string</span>)</span> <span class="title">int</span></span> &#123;&#125;</span><br><span class="line"><span class="comment">// 多个返回值</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">div</span><span class="params">(a, b <span class="keyword">int</span>)</span> <span class="params">(<span class="keyword">int</span> <span class="keyword">int</span>)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 返回值命名/合并</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">div</span><span class="params">(a, b <span class="keyword">int</span>)</span> <span class="params">(q, r <span class="keyword">int</span>)</span></span>&#123;&#125;</span><br><span class="line"><span class="comment">// 多返回值一般把第二个用来返回error</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 丢弃不需要的返回值</span></span><br><span class="line">q, _ := div(<span class="number">13</span>, <span class="number">3</span>)</span><br><span class="line">fmt.Println(q)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>函数的参数也可以是一个函数</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">apply</span><span class="params">(op <span class="keyword">func</span>(<span class="keyword">int</span>, <span class="keyword">int</span>)</span> <span class="title">int</span> , <span class="title">a</span> , <span class="title">b</span> <span class="title">int</span>) <span class="title">int</span></span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>go不支持默认参数等。只支持可变参数列表</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sum</span><span class="params">(numbers ...<span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;&#125;</span><br></pre></td></tr></table></figure>



<h2 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h2><blockquote>
<p>go语言的指针不能预算。只能赋值</p>
</blockquote>
<h3 id="参数传递"><a href="#参数传递" class="headerlink" title="参数传递"></a>参数传递</h3><blockquote>
<p>Go语言只有值传递。没有引用传递</p>
</blockquote>
<p>即便是传递指针，也是会将指针拷贝，然后再传入函数。不过内外两个指针会指向同一个内存地址。</p>
<h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><blockquote>
<p>在go语言中一般不直接使用数组，而是使用切片</p>
</blockquote>
<p>定义数组时，长度在前，数据类型在后。</p>
<blockquote>
<p>定义数组时必须指定数据类型。利用函数返回值时例外</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">numbers := [<span class="number">5</span>]<span class="keyword">int</span>&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以不定义初始值, 此时必须制定类型+长度</span></span><br><span class="line"><span class="keyword">var</span> numbers [<span class="number">5</span>]<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以由系统推断长度, 此时必须指定初始元素</span></span><br><span class="line">numbers := [...]<span class="keyword">int</span>&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用[index]来取得索引上的值.go也是从0开始</span></span><br><span class="line">a := numbers[<span class="number">0</span>] <span class="comment">// 1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>[]int</code> 是切片。<code>[5]int</code>才是数组</p>
</blockquote>
<p>可以使用<code>range</code>关键字遍历数组</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> index, value := <span class="keyword">range</span> numbers &#123;</span><br><span class="line">  <span class="comment">// do something</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>数组是值类型</strong></p>
<ul>
<li>定义func时，如果参数时<code>(a [5]int)</code>，则只能接受<code>[5]int</code>，不能接受<code>[3]int</code></li>
<li>func中修改数组内容时，其实是修改的拷贝。</li>
</ul>
<h3 id="切片（slice）"><a href="#切片（slice）" class="headerlink" title="切片（slice）"></a>切片（slice）</h3><blockquote>
<p>和python的切片很像。区间也是左闭右开</p>
<p><strong>slice是array的视图</strong></p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// start , end均可省略。</span></span><br><span class="line"><span class="comment">// slice包含start对应的内容，不包含end对应的内容。</span></span><br><span class="line"><span class="comment">// 若start，end均省略，则为arr数组的完整视图</span></span><br><span class="line">slice := arr[start:end]</span><br></pre></td></tr></table></figure>

<p><strong>slice是array的视图</strong></p>
<ul>
<li><p>修改slice的值，会对array本身产生影响</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">slice := arr[<span class="number">2</span>:]</span><br><span class="line">slice[<span class="number">0</span>] = <span class="number">100</span> <span class="comment">// arr[2] =&gt; 100</span></span><br></pre></td></tr></table></figure></li>
<li><p>当函数参数为slice时，修改slice会影响slice所指向的数组</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">update</span><span class="params">(sl []<span class="keyword">int</span>)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span>[<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">update(arr[<span class="number">2</span>:]) <span class="comment">// arr[2] =&gt; 100</span></span><br></pre></td></tr></table></figure></li>
<li><p>可以在slice的基础上继续slice</p>
<ul>
<li><p>继续slice时，可以扩展</p>
<ul>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210620171841.png" alt="image-20210620171835125"></p>
</blockquote>
</li>
</ul>
</li>
<li><p>s2实际上能拿到<code>arr[6]</code>，不过s1是没有<code>s1[4]</code>的。这个源于slice的实现</p>
<ul>
<li><p>slice主要由三部分组成</p>
<ul>
<li><p>ptr：指向slice开始的指针。</p>
</li>
<li><p>len：slice的长度。如果越界会报错</p>
</li>
<li><p>cap：从slice开始，到arr结尾的所有元素</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210620172136.png" alt="image-20210620172136456"></p>
</blockquote>
</li>
<li><p><code>s[i]</code>不能超越len(s)，向后扩展不可以超过底层数组cap(s)</p>
<ul>
<li>能够通过<code>len(s)</code>，<code>cap(3)</code>获取slice的这两个长度。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>slice可以向后扩展，不能向前扩展</p>
</li>
</ul>
</li>
</ul>
<h4 id="向slice添加元素"><a href="#向slice添加元素" class="headerlink" title="向slice添加元素"></a>向slice添加元素</h4><blockquote>
<p>使用append函数，append会返回一个修改后的切片</p>
</blockquote>
<ul>
<li>首先会在len后，cap处修改元素。逐个向后修改。此时会影响底层的array</li>
<li>如果在新的切片上增加多次，长度超过了cap，则系统会重新分配一个更大的底层数组，此时slice不再指向原本的arr</li>
</ul>
<h4 id="创建slice"><a href="#创建slice" class="headerlink" title="创建slice"></a>创建slice</h4><blockquote>
<p>可以直接创建slice，而不是在数组的基础上。</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> s []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line">s1 := []<span class="keyword">int</span>&#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// (type, len, cap)</span></span><br><span class="line">s2 := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">16</span> ,<span class="number">32</span>)</span><br></pre></td></tr></table></figure>



<h4 id="复制slice"><a href="#复制slice" class="headerlink" title="复制slice"></a>复制slice</h4><blockquote>
<p>使用copy方法复制slice</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">s1 := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;</span><br><span class="line">s2 := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">3</span>, <span class="number">16</span>)</span><br><span class="line"><span class="comment">// copy(dst, src)</span></span><br><span class="line"><span class="built_in">copy</span>(s2, s1)</span><br><span class="line">fmt.Println(s2) <span class="comment">// [1 2 3]</span></span><br><span class="line">s3 := s2[:<span class="number">4</span>]</span><br><span class="line">fmt.Println(s3) <span class="comment">// [1 2 3 0]</span></span><br></pre></td></tr></table></figure>



<h4 id="删除slice的某个元素"><a href="#删除slice的某个元素" class="headerlink" title="删除slice的某个元素"></a>删除slice的某个元素</h4><blockquote>
<p>go不自带该功能，可以使用append方法实现</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">s1 := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除第2个元素2。</span></span><br><span class="line">s1 = <span class="built_in">append</span>(s1[:<span class="number">1</span>], s1[<span class="number">2</span>:]...)</span><br><span class="line"></span><br><span class="line">fmt.Println(s1) <span class="comment">// [1 3 4 5]</span></span><br></pre></td></tr></table></figure>



<h4 id="移除slice首尾元素"><a href="#移除slice首尾元素" class="headerlink" title="移除slice首尾元素"></a>移除slice首尾元素</h4><blockquote>
<p>同样不自带。可以使用切片完成</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">s1 := []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line">front := s1[<span class="number">0</span>]</span><br><span class="line">s1 = s1[<span class="number">1</span>:]</span><br><span class="line">fmt.Printf(<span class="string">&quot;front: %d, s1: %v\n&quot;</span>, front, s1) <span class="comment">// front: 1, s1: [2 3 4 5]</span></span><br><span class="line"></span><br><span class="line">tail := s1[<span class="built_in">len</span>(s1)<span class="number">-1</span>]</span><br><span class="line">s1 = s1[:<span class="built_in">len</span>(s1)<span class="number">-1</span>]</span><br><span class="line">fmt.Printf(<span class="string">&quot;tail: %d, s1: %v\n&quot;</span>, tail, s1) <span class="comment">// tail: 5, s1: [2 3 4]</span></span><br></pre></td></tr></table></figure>



<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><h4 id="Map的定义"><a href="#Map的定义" class="headerlink" title="Map的定义"></a>Map的定义</h4><blockquote>
<p>正常Map：<code>map[K]V</code>,</p>
<p>复合Map： <code>map[K1]map[K2]V</code></p>
<p>map是一个hashMap</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 初始化一个map</span></span><br><span class="line">m1 := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> &#123;<span class="string">&quot;id&quot;</span>:<span class="number">1</span>, <span class="string">&quot;age&quot;</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 空map</span></span><br><span class="line"><span class="keyword">var</span> m2 <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> <span class="comment">// m3 == nil</span></span><br><span class="line"></span><br><span class="line">m3 := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span>) <span class="comment">// m2 == empty map</span></span><br></pre></td></tr></table></figure>



<h4 id="Map遍历"><a href="#Map遍历" class="headerlink" title="Map遍历"></a>Map遍历</h4><blockquote>
<p>同样可使用range来遍历</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> k, v := <span class="keyword">range</span> m &#123;</span><br><span class="line">  <span class="comment">// do something with k, v </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="Map的内容获取"><a href="#Map的内容获取" class="headerlink" title="Map的内容获取"></a>Map的内容获取</h4><blockquote>
<p>value := map[key]</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">m1 := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> &#123;<span class="string">&quot;id&quot;</span>:<span class="number">1</span>, <span class="string">&quot;age&quot;</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ok这个参数可省略，主要用于判断key是否存在</span></span><br><span class="line">age, ok := m1[<span class="string">&quot;age&quot;</span>] <span class="comment">// 2, true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果key不存在</span></span><br><span class="line">other, ok2 := m1[<span class="string">&quot;other&quot;</span>] <span class="comment">// 0 , false</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>当key在map中不存在时，会得到对应数据格式的zero value。</p>
<p>对于int，就是0。对于string，则是空字符串</p>
</blockquote>
<h4 id="Map的数据删除"><a href="#Map的数据删除" class="headerlink" title="Map的数据删除"></a>Map的数据删除</h4><blockquote>
<p>使用delete函数进行删除</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">m1 := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> &#123;<span class="string">&quot;id&quot;</span>:<span class="number">1</span>, <span class="string">&quot;age&quot;</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一次只能删除一个key</span></span><br><span class="line"><span class="built_in">delete</span>(m1, age) <span class="comment">// &#123;&quot;id&quot;: 1&#125;</span></span><br></pre></td></tr></table></figure>



<h4 id="Map数据添加或修改"><a href="#Map数据添加或修改" class="headerlink" title="Map数据添加或修改"></a>Map数据添加或修改</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">m := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span>&#123;<span class="string">&quot;id&quot;</span>: <span class="number">1</span>, <span class="string">&quot;age&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">m[<span class="string">&quot;age2&quot;</span>] = <span class="number">2</span>	</span><br><span class="line">m[<span class="string">&quot;age&quot;</span>] = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">fmt.Println(m) <span class="comment">// map[age:3 age2:2 id:1]</span></span><br></pre></td></tr></table></figure>



<h4 id="Map的使用"><a href="#Map的使用" class="headerlink" title="Map的使用"></a>Map的使用</h4><ul>
<li>map使用hash表，key必须能够比较</li>
<li>除了<code>slice, map, function</code>的其他内建类型都可以作为key</li>
<li>struct类型不包含上述3个类型时，也可以作为key</li>
</ul>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go的依赖管理</title>
    <url>/2021/06/27/go-de-yi-lai-guan-li/</url>
    <content><![CDATA[<h2 id="Go依赖管理的发展"><a href="#Go依赖管理的发展" class="headerlink" title="Go依赖管理的发展"></a>Go依赖管理的发展</h2><h3 id="GOPATH"><a href="#GOPATH" class="headerlink" title="GOPATH"></a>GOPATH</h3><p>编译寻找依赖，会在<code>$GOPATH/src/</code>下去寻找。</p>
<p>存在的问题：</p>
<ul>
<li>由于依赖都是放在linux目录下，所以无法直接进行版本管理。依赖只有一个版本。</li>
</ul>
<h3 id="GOVENDOOR"><a href="#GOVENDOOR" class="headerlink" title="GOVENDOOR"></a>GOVENDOOR</h3><p>为了解决依赖版本的问题，可以：</p>
<ul>
<li>通过在工程内建立<code>vendor</code>文件夹。将依赖放在<code>vendor</code>文件夹下。</li>
</ul>
<h3 id="GOMO"><a href="#GOMO" class="headerlink" title="GOMO"></a>GOMO</h3><p>新的依赖管理工具。会将依赖放在固定目录。</p>
<ul>
<li>go get进行依赖下载。</li>
</ul>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go并发编程入门</title>
    <url>/2021/07/04/go-bing-fa-bian-cheng-ru-men/</url>
    <content><![CDATA[<h2 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h2><p>go中的多线程一般是指协程，其具有几个特点</p>
<ul>
<li>轻量级的“线程”</li>
<li><strong>非抢占式</strong>多任务处理，由协程主动交出控制权。</li>
<li>编译器/虚拟机层面的多任务。</li>
<li>多个协程可能在一个或多个线程上运行。</li>
</ul>
<blockquote>
<p>子程序是协程的一个特例</p>
</blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/%E5%8D%8F%E7%A8%8B.png" alt="image-20210706205133076"></p>
<h2 id="Go语言中的协程"><a href="#Go语言中的协程" class="headerlink" title="Go语言中的协程"></a>Go语言中的协程</h2><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/go%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B.png" alt="image-20210706205305582"></p>
<p>go中的协程具有下述几个特点：</p>
<ul>
<li>任何函数只需要加上go就能送给调度器运行。</li>
<li>不需要在定义方法时区分是否为异步函数。</li>
<li>调度器在合适的点进行切换。</li>
<li>使用<code>-race</code>检测数据访问冲突。</li>
</ul>
<h3 id="goroutine可能的切换点"><a href="#goroutine可能的切换点" class="headerlink" title="goroutine可能的切换点"></a>goroutine可能的切换点</h3><blockquote>
<p>仅供参考。不保证必定切换。不保证其他地方不会切换</p>
</blockquote>
<ul>
<li>I/O，select</li>
<li>channel</li>
<li>等待锁</li>
<li>函数调用（有时）</li>
<li>runtime.Gosched()</li>
</ul>
<h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><blockquote>
<p>是goroutine之间的通道</p>
</blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/channel.png" alt="image-20210706205905450"></p>
<h3 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 构建channel</span></span><br><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"><span class="comment">// 向channel发送数据</span></span><br><span class="line"><span class="comment">// 如果channel没有缓存，则发送的数据必须要有人接收。否则可能会陷入死锁</span></span><br><span class="line">c &lt;- <span class="number">1</span></span><br><span class="line"><span class="comment">// 从channel得到数据</span></span><br><span class="line">n := &lt;- c</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以获取一个只能接收/发送数据的channel</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">maker</span><span class="params">(id <span class="keyword">int</span>)</span> <span class="title">chan</span>&lt;- <span class="title">int</span></span> &#123;</span><br><span class="line">	channel := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	<span class="keyword">return</span> channel</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 带缓存的channel。示例为缓存为3</span></span><br><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 关闭channel</span></span><br><span class="line"><span class="built_in">close</span>(c)</span><br><span class="line"><span class="comment">// 接收方判断channel是否已关闭</span></span><br><span class="line">elem , ok := &lt;- c</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line">	fmt.Println(elem)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">	fmt.Print(<span class="string">&quot;close&quot;</span>)</span><br><span class="line">	<span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用range来接收channel的数据</span></span><br><span class="line"><span class="keyword">for</span> elem := <span class="keyword">range</span> c &#123;</span><br><span class="line">	fmt.Println(elem)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(<span class="string">&quot;close&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="等待任务结束"><a href="#等待任务结束" class="headerlink" title="等待任务结束"></a>等待任务结束</h3><p>使用<code>sync.WaitGroup&#123;&#125;</code>来控制任务的结束。需要在线程中判断是否需要结束，如果结束调用<code>wait.done</code>。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">waitGroupChannel</span><span class="params">()</span></span> &#123;</span><br><span class="line">	waitGroup := sync.WaitGroup&#123;&#125;</span><br><span class="line">	chanGroup := <span class="built_in">make</span>([]<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">	<span class="comment">// create channel</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">20</span>; i++ &#123;</span><br><span class="line">		chanGroup[i] = <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// send data to channel</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">20</span>; i++ &#123;</span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">int</span>, i <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">      <span class="comment">// 需要在一个线程中控制发送与关闭。否则可能出现向已关闭的channel中发送数据的情况。</span></span><br><span class="line">			c &lt;- i</span><br><span class="line">			<span class="built_in">close</span>(c)</span><br><span class="line">		&#125;(chanGroup[i], i)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="number">20</span>; i++ &#123;</span><br><span class="line">		<span class="comment">// 为每个线程开启一个等待</span></span><br><span class="line">		waitGroup.Add(<span class="number">1</span>)</span><br><span class="line">		<span class="keyword">go</span> useWait(&amp;waitGroup, chanGroup[i])</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 等待所有Done</span></span><br><span class="line">	waitGroup.Wait()</span><br><span class="line">	fmt.Println(<span class="string">&quot;done!&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">useWait</span><span class="params">(wg *sync.WaitGroup, c <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		i, ok := &lt;-c</span><br><span class="line">		fmt.Println(<span class="string">&quot;useWait: &quot;</span>, i)</span><br><span class="line">		<span class="keyword">if</span> !ok &#123;</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 与Add相反. 可能导致其中变成-1</span></span><br><span class="line">	wg.Done()</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>几个注意点</p>
<ul>
<li>需要在一个线程中控制发送与关闭。否则可能出现向已关闭的channel中发送数据的情况。</li>
<li>传递waitGroup需要使用指针传递。而channel不用。</li>
<li>add与done注意处理次数，防止done多于add。</li>
</ul>
<h3 id="使用Select处理channel"><a href="#使用Select处理channel" class="headerlink" title="使用Select处理channel"></a>使用Select处理channel</h3><p>可以根据channel发过来的数据。选择收到的的分支进行后续处理</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">UseSelect</span><span class="params">()</span></span> &#123;</span><br><span class="line">	c1 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	c2 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> sendMsg(c1)</span><br><span class="line">	<span class="keyword">go</span> sendMsg(c2)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> &lt;-c1:</span><br><span class="line">		fmt.Println(<span class="string">&quot;c1&quot;</span>)</span><br><span class="line">	<span class="keyword">case</span> &lt;-c2:</span><br><span class="line">		fmt.Println(<span class="string">&quot;c2&quot;</span>)</span><br><span class="line">	<span class="keyword">default</span>:</span><br><span class="line">		fmt.Println(<span class="string">&quot;default&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="传统同步机制"><a href="#传统同步机制" class="headerlink" title="传统同步机制"></a>传统同步机制</h2><p>// TODO</p>
<h2 id="并发编程模式"><a href="#并发编程模式" class="headerlink" title="并发编程模式"></a>并发编程模式</h2><h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MsgGen</span><span class="params">()</span> &lt;-<span class="title">chan</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	chString := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		i := <span class="number">1</span></span><br><span class="line">		<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">			time.Sleep(time.Second)</span><br><span class="line">			i++</span><br><span class="line">			chString &lt;- fmt.Sprintf(<span class="string">&quot;%d&quot;</span>, i)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">return</span> chString</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="合并生成器"><a href="#合并生成器" class="headerlink" title="合并生成器"></a>合并生成器</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MergeChannel</span><span class="params">(c1,c2 <span class="keyword">chan</span> <span class="keyword">string</span>)</span> &lt;-<span class="title">chan</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	total := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">			total&lt;- &lt;-c1</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">			total&lt;- &lt;-c2</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">return</span> total</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在不知道要合并多少个channel时可以选择这个：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MergeChannel</span><span class="params">(chs ...<span class="keyword">chan</span> <span class="keyword">string</span>)</span> &lt;-<span class="title">chan</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	total := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line">	<span class="keyword">for</span> _, ch := <span class="keyword">range</span> chs &#123;</span><br><span class="line">    <span class="comment">// 需要作为参数传入。系统会进行值传递。导致ch被copy了一份。防止for循环的坑</span></span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(chCopy <span class="keyword">chan</span> <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">			<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">				total &lt;- &lt;-chCopy</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;(ch)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> total</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>明确有多少个channel要合并时最好使用select，这样可以控制开启的goroutine为一个：</p>
<h4 id="通过Select合并"><a href="#通过Select合并" class="headerlink" title="通过Select合并"></a>通过Select合并</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MergeBySelect</span><span class="params">(c1, c2 <span class="keyword">chan</span> <span class="keyword">string</span>)</span> &lt;-<span class="title">chan</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	total := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">			<span class="keyword">select</span> &#123;</span><br><span class="line">			<span class="keyword">case</span> n := &lt;-c1:</span><br><span class="line">				total &lt;- n</span><br><span class="line">			<span class="keyword">case</span> n := &lt;-c2:</span><br><span class="line">				total &lt;- n</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	<span class="keyword">return</span> total</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="并发任务的控制"><a href="#并发任务的控制" class="headerlink" title="并发任务的控制"></a>并发任务的控制</h2><h3 id="非阻塞等待"><a href="#非阻塞等待" class="headerlink" title="非阻塞等待"></a>非阻塞等待</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">nonBlockingWait</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">string</span>, <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">  <span class="comment">// 防止一直等待数据到来</span></span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> n := &lt;-c:</span><br><span class="line">		<span class="keyword">return</span> n, <span class="literal">true</span></span><br><span class="line">	<span class="keyword">default</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span>, <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="超时机制"><a href="#超时机制" class="headerlink" title="超时机制"></a>超时机制</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">timeoutWait</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">string</span>, timeout time.Duration)</span> <span class="params">(<span class="keyword">string</span>, <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">select</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> n := &lt;-c:</span><br><span class="line">		<span class="keyword">return</span> n, <span class="literal">true</span></span><br><span class="line">	<span class="keyword">case</span> &lt;-time.After(timeout):</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span>, <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="任务中断-退出"><a href="#任务中断-退出" class="headerlink" title="任务中断/退出"></a>任务中断/退出</h3><p>通过一个额外的channel来控制线程的运行：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">StopTask</span><span class="params">(c &lt;-<span class="keyword">chan</span> <span class="keyword">string</span>, done <span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> <span class="literal">true</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> n := &lt;-c:</span><br><span class="line">			fmt.Println(n)</span><br><span class="line">		<span class="keyword">case</span> &lt;-done:</span><br><span class="line">			fmt.Println(<span class="string">&quot;cleaning&quot;</span>)</span><br><span class="line">			time.Sleep(time.Second * <span class="number">2</span>)</span><br><span class="line">			fmt.Println(<span class="string">&quot;clean&quot;</span>)</span><br><span class="line">			done &lt;- <span class="keyword">struct</span>&#123;&#125;&#123;&#125;</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go的接口</title>
    <url>/2021/06/29/go-de-jie-kou/</url>
    <content><![CDATA[<h2 id="duck-typing"><a href="#duck-typing" class="headerlink" title="duck typing"></a>duck typing</h2><blockquote>
<p>描述事物的外部行为而非内部结构</p>
</blockquote>
<h3 id="Pyhon中的duckTyping"><a href="#Pyhon中的duckTyping" class="headerlink" title="Pyhon中的duckTyping"></a>Pyhon中的duckTyping</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">retriever</span>):</span></span><br><span class="line">  <span class="keyword">return</span> retrierver.get(<span class="string">&quot;ssss&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>运行时才知道传入的retriever是否有get方法。</li>
<li>需要在注释中提醒使用者，传入的retriever需要实现get方法。</li>
</ul>
<h3 id="C-中的duckTyping"><a href="#C-中的duckTyping" class="headerlink" title="C++中的duckTyping"></a>C++中的duckTyping</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">R</span>&gt;</span></span><br><span class="line"><span class="function">string <span class="title">download</span><span class="params">(<span class="keyword">const</span> R&amp; retriever)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> retriever.<span class="built_in">get</span>(<span class="string">&quot;ssss&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编译时才知道传入的retriever有没有get方法。</li>
<li>需要通过注释提醒使用者</li>
</ul>
<h3 id="Java中的duckTyping"><a href="#Java中的duckTyping" class="headerlink" title="Java中的duckTyping"></a>Java中的duckTyping</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;R extends Retriever&gt;</span><br><span class="line"><span class="function">String <span class="title">download</span><span class="params">(R r)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> r.get(<span class="string">&quot;ssss&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>传入的参数必须实现Retriever</li>
</ul>
<h3 id="Go中的duckTyping"><a href="#Go中的duckTyping" class="headerlink" title="Go中的duckTyping"></a>Go中的duckTyping</h3><p>Java的问题：</p>
<ul>
<li>同时需要Readable和Appendable怎么办。（apache polygene）</li>
<li>缺少灵活性</li>
</ul>
<p>Python/C++的问题：</p>
<ul>
<li>不具备类型检查</li>
</ul>
<p>Go的特点：</p>
<ul>
<li>接口由使用者定义</li>
<li>接口的实现是<strong>隐式的</strong>，只需要实现<strong>接口定义的方法</strong>，就表示实现了对应的接口。<ul>
<li>即调用者只关心接口有哪些方法。不关心实现。</li>
<li>实现者只需要实现所有接口方法，就能作为这个接口被使用者调用。</li>
</ul>
</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Http.go</span></span><br><span class="line"><span class="keyword">type</span> Http <span class="keyword">interface</span> &#123;</span><br><span class="line">	Get(url <span class="keyword">string</span>) <span class="keyword">string</span></span><br><span class="line">	Post(url <span class="keyword">string</span>) <span class="keyword">string</span></span><br><span class="line">	Delete(url <span class="keyword">string</span>) <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MyHttp.go</span></span><br><span class="line"><span class="keyword">type</span> MyHttp <span class="keyword">struct</span> &#123;</span><br><span class="line">	version <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要同时实现这三个借口，才能被识别为实现了Http接口</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m MyHttp)</span> <span class="title">Get</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;get!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m MyHttp)</span> <span class="title">Post</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;post!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m MyHttp)</span> <span class="title">Delete</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;delete!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 接口使用</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getHttp</span><span class="params">()</span> <span class="title">inter</span>.<span class="title">Http</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> inter.MyHttp&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ---------</span></span><br><span class="line"><span class="comment">// ---------</span></span><br><span class="line"><span class="comment">// 如果方法实现中接收者为指针类型。则需要当作指针返回</span></span><br><span class="line"><span class="keyword">type</span> MyHttp <span class="keyword">struct</span> &#123;</span><br><span class="line">	version <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *MyHttp)</span> <span class="title">Get</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;get!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *MyHttp)</span> <span class="title">Post</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;post!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *MyHttp)</span> <span class="title">Delete</span><span class="params">(url <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;delete!&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 需要取得地址返回</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getHttp</span><span class="params">()</span> <span class="title">inter</span>.<span class="title">Http</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> &amp;inter.MyHttp&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="接口的值类型"><a href="#接口的值类型" class="headerlink" title="接口的值类型"></a>接口的值类型</h2><p>由于Go都是值传递。所以当讲接口作为一个接口传递时，接口也是一个值。</p>
<p>接口变量里有什么：</p>
<ul>
<li>实现者类型。</li>
<li>实现者的值。（实现者的指针）<ul>
<li>所以几乎不需要使用接口的指针。</li>
<li>指针接收者实现只能以指针方式使用；值接收者则都可。</li>
</ul>
</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// todo</span></span><br></pre></td></tr></table></figure>



<h2 id="查看接口变量"><a href="#查看接口变量" class="headerlink" title="查看接口变量"></a>查看接口变量</h2><ul>
<li><p>表示任何类型：<code>interface&#123;&#125;</code></p>
</li>
<li><p>Type Assertion</p>
<ul>
<li><p>```go</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  - </span><br><span class="line"></span><br><span class="line">- Type Switch</span><br><span class="line"></span><br><span class="line">  - </span><br><span class="line"></span><br><span class="line">    ```go</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>
</li>
</ul>
<h2 id="接口的组合"><a href="#接口的组合" class="headerlink" title="接口的组合"></a>接口的组合</h2><blockquote>
<p>一个接口可以由多个接口组合而成，则表示该接口包含所有接口的特性。</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 已有两个借口</span></span><br><span class="line"><span class="keyword">type</span> Http1 <span class="keyword">interface</span> &#123;</span><br><span class="line">  Get(url <span class="keyword">string</span>) <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> Http2 <span class="keyword">interface</span> &#123;</span><br><span class="line">  Post(url <span class="keyword">string</span>) <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 接口组合, 若实现HTTP接口，则包含了Get/Post/OtherHttpInterfaceFunc三方法</span></span><br><span class="line"><span class="keyword">type</span> Http <span class="keyword">interface</span> &#123;</span><br><span class="line">  Http1</span><br><span class="line">  Http2</span><br><span class="line">  OtherHttpInterfaceFunc()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/07/03/go-de-zi-yuan-he-chu-cuo-guan-li/</url>
    <content><![CDATA[<h2 id="defer"><a href="#defer" class="headerlink" title="defer"></a>defer</h2><ul>
<li><p><code>defer</code>确保其语句在调用结束时发生。</p>
</li>
<li><p>多个<code>defer</code>语句时，遵循先进后出规则（类似于栈）</p>
</li>
<li><p><code>defer</code>的执行世纪早于函数跳出</p>
<ul>
<li>即函数中如果有<code>panic</code>，会先执行<code>defer</code>，再跳出。</li>
</ul>
</li>
<li><p>参数在defer语句时计算</p>
<ul>
<li><p>当<code>defer</code>有参数时，虽然调用<code>defer</code>在退出函数前，但这个参数的值在执行到<code>defer</code>这一行是就决定了。</p>
</li>
<li><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">i := <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">defer</span> fmt.Println(<span class="string">&quot;deferI: &quot;</span>, i)</span><br><span class="line">i++</span><br><span class="line">fmt.Println(<span class="string">&quot;NormalI: &quot;</span>, i)</span><br><span class="line"><span class="comment">// -------</span></span><br><span class="line"><span class="comment">// console</span></span><br><span class="line"><span class="comment">// NormalI:  2</span></span><br><span class="line"><span class="comment">// deferI:  1</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h2><ul>
<li>go中error一般指一个error接口，实现了Error函数。</li>
<li>可以将返回的error转换成具体的某一个error，即可以拿到更加详细的信息</li>
</ul>
<h2 id="panic和recover"><a href="#panic和recover" class="headerlink" title="panic和recover"></a>panic和recover</h2><ul>
<li><p>分别类似于<code>exception</code>和<code>try-catch</code></p>
</li>
<li><p><code>panic</code>会立即停止当前函数执行，并向上返回。执行每一层的<code>defer</code></p>
</li>
<li><p><code>recover</code>只能在<code>defer</code>中被调用。可以获取<code>panic</code>的内容。可以重新<code>panic</code>。</p>
<ul>
<li><pre class=" language-go"><code class="language-go">def <span class="token keyword">func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token builtin">error</span> <span class="token operator">:=</span> <span class="token function">recover</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true">// do with error</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
</li>
<li></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>go的面向对象</title>
    <url>/2021/06/26/go-de-mian-xiang-dui-xiang/</url>
    <content><![CDATA[<ul>
<li>Go语言仅支持封装，不支持继承和多态</li>
</ul>
<h2 id="初始化结构体"><a href="#初始化结构体" class="headerlink" title="初始化结构体"></a>初始化结构体</h2><p>通过<code>type &lt;name&gt; struct &#123;code&#125;</code>的形式定义结构体</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> MyStruct <span class="keyword">struct</span>&#123;</span><br><span class="line">  value <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// new struct</span></span><br><span class="line">myStruct := MyStruct&#123;value: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> myStruct MyStrcut</span><br><span class="line"></span><br><span class="line">node3 := <span class="built_in">new</span>(MyStruct)</span><br></pre></td></tr></table></figure>



<h2 id="结构体的方法"><a href="#结构体的方法" class="headerlink" title="结构体的方法"></a>结构体的方法</h2><p>要为结构体添加函数。需要将结构体作为接收参数（接收者）写在方法前面。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(myStruct MyStruct)</span> <span class="title">getValue</span><span class="params">()</span> <span class="title">int</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> myStruct.value</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(myStruct MyStruct)</span> <span class="title">print</span><span class="params">()</span></span> &#123;</span><br><span class="line">  fmt.Prtinln(myStruct)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这样getValue和print就作为了MyStruct的函数。可以直接调用</span></span><br><span class="line">myStruct := MyStruct&#123;value: <span class="number">3</span>&#125;</span><br><span class="line">myStruct.<span class="built_in">print</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果这样写</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">print</span><span class="params">(myStruct MyStruct)</span></span>&#123;</span><br><span class="line">  fmt.Prtinln(myStruct)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 则需要当作一个普通函数调用</span></span><br><span class="line"><span class="built_in">print</span>(myStruct)</span><br></pre></td></tr></table></figure>



<h3 id="关于值传递"><a href="#关于值传递" class="headerlink" title="关于值传递"></a>关于值传递</h3><p>在结构体方法中，<strong>接收者也是值传递</strong>。意味着<code>myStruct.getValue()</code>调用的其实是<code>myStruct</code>的复制的<code>value</code>。这种时候需要将接收者作为指针，传递地址。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(myStruct *MyStruct)</span> <span class="title">setValue</span><span class="params">(newValue: <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">  myStruct.value = newValue</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// usage</span></span><br><span class="line"></span><br><span class="line">myStruct.setValue(<span class="number">100</span>) <span class="comment">// value = 100</span></span><br></pre></td></tr></table></figure>

<p>这么做，调用<code>setValue</code>方法时，实际上调用的是<code>myStruct</code>对应地址上的方法。不过在使用没有任何区别，都是用<code>.func</code>调用即可。</p>
<p>同样的，当我们将<code>myStruct</code>的指针付给一个新变量时，调用这类办法，也会将<code>myStruct</code>的<code>value</code>值进行修改。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">myStruct2 := &amp;myStruct <span class="comment">// value = 3</span></span><br><span class="line">myStruct2.setValue(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">fmt.Println(myStruct2) <span class="comment">// value = 100</span></span><br><span class="line">fmt.Println(myStruct)  <span class="comment">// value = 100</span></span><br></pre></td></tr></table></figure>

<p>同时，nil也可以调用指针的方法。并且方法调用本身不会报错，只当进行具体操作时，比如过nil并没有value这个属性，所以nil.value时会报错。</p>
<h3 id="指针接收者还是值接收者"><a href="#指针接收者还是值接收者" class="headerlink" title="指针接收者还是值接收者"></a>指针接收者还是值接收者</h3><ul>
<li>要改变内容必须使用指针接收者。</li>
<li>结构过大也考虑使用指针接收者。</li>
<li>一致性：如果有指针接收者，最好其他方法也修改为指针接收者。</li>
</ul>
<h2 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h2><blockquote>
<p>Go没有构造方法一说。不过可以通过方法的形式，创建构造方法</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createMyStruct</span><span class="params">(newValue <span class="keyword">int</span>)</span> *<span class="title">MyStruct</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> &amp;MyStruct&#123;value: newValue&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有几个点需要注意：</p>
<ul>
<li>这里返回的是一个局部变量。在方法里创造的<code>MyStruct</code></li>
<li>Go的编译器会在编译时判断这个变量建立在堆上还是栈上。<ul>
<li>如果返回值是一个指针。Go会将其创建在堆上。供外部使用，同时参与到后续的垃圾回收。</li>
<li>如果返回值是一个值。Go会将其创建的栈上。返回一个其的复制。并在结束时销毁。</li>
</ul>
</li>
</ul>
<h2 id="结构体的可见域与使用"><a href="#结构体的可见域与使用" class="headerlink" title="结构体的可见域与使用"></a>结构体的可见域与使用</h2><h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3><ul>
<li>每个目录只允许有一个包</li>
<li>main包包含可执行入口。<ul>
<li>如果一个目录下有一个main函数，那么这个目录下只能有一个main包(即<code>package main</code>)。</li>
</ul>
</li>
<li>为结构定义的方法必须放在同一个包内。<ul>
<li>但可以在同一个包下的不同文件。</li>
</ul>
</li>
</ul>
<h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><blockquote>
<p>在Go中，首字母大写有含义</p>
</blockquote>
<ul>
<li>首字母大写。表示public</li>
<li>首字母小写。表示private</li>
<li>结构的元素，方法，结构体本身都遵循上述可见规则。</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>使用<code>包.类</code>进行调用。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 假设Node在tree包下,下述代码在use包下的hello.go</span></span><br><span class="line"><span class="comment">// -root</span></span><br><span class="line"><span class="comment">// |-use</span></span><br><span class="line"><span class="comment">// |--hello.go</span></span><br><span class="line"><span class="comment">// |-tree</span></span><br><span class="line"><span class="comment">// |--node.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 一个目录下只允许有一个package main</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要根据目录进行import</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;awesomeProject/tree&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="comment">// 包名tree不能省略</span></span><br><span class="line">	node := tree.Node&#123;Value: <span class="number">2</span>&#125;</span><br><span class="line">	node.Print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="扩展已有类型"><a href="#扩展已有类型" class="headerlink" title="扩展已有类型"></a>扩展已有类型</h2><blockquote>
<p>Go中，没有继承，需要通过其他方法扩充系统类型或者别人的类型。</p>
</blockquote>
<h3 id="定义别名（自定义类型）"><a href="#定义别名（自定义类型）" class="headerlink" title="定义别名（自定义类型）"></a>定义别名（自定义类型）</h3><blockquote>
<p>示例使用queue去扩展[]int</p>
</blockquote>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Queue []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *Queue)</span> <span class="title">push</span><span class="params">(i <span class="keyword">int</span>)</span></span>&#123;</span><br><span class="line">  <span class="comment">// 由于我们需要修改q本身的内容，所以需要使用*q来接收。</span></span><br><span class="line">  <span class="comment">// q本身是Queue的指针。指向Queue的地址。</span></span><br><span class="line">  <span class="comment">// 通过*q，就把Queue本身拿到。然后将其赋予一个新的内容。</span></span><br><span class="line">	*q = <span class="built_in">append</span>(*q, i)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(q *Queue)</span> <span class="title">pop</span><span class="params">()</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	elem := (*q)[<span class="number">0</span>]</span><br><span class="line">	*q = (*q)[<span class="number">1</span>:]</span><br><span class="line">	<span class="keyword">return</span> elem</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>几个注意点</p>
<ul>
<li>使用<code>type 别名 希望扩展的类型</code>进行定义。</li>
<li>自定义类型不能与原始类型进行计算。</li>
<li>可以把type写在main里。</li>
<li>自定义type具有隐藏原type的效果</li>
</ul>
<h3 id="使用组合"><a href="#使用组合" class="headerlink" title="使用组合"></a>使用组合</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	node := tree.Node&#123;Value: <span class="number">3</span>&#125;</span><br><span class="line">	newStruct := MyNewStruct&#123;newValue: &amp;node&#125;</span><br><span class="line">	newStruct.post()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> MyNewStruct <span class="keyword">struct</span> &#123;</span><br><span class="line">	newValue *tree.Node</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(myNewStruct *MyNewStruct)</span> <span class="title">post</span><span class="params">()</span></span> &#123;</span><br><span class="line">	newValue := MyNewStruct&#123;newValue: myNewStruct.newValue&#125;</span><br><span class="line">	fmt.Println(newValue.newValue)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="使用内嵌-Embedding-扩展已有类型"><a href="#使用内嵌-Embedding-扩展已有类型" class="headerlink" title="使用内嵌(Embedding)扩展已有类型"></a>使用内嵌(Embedding)扩展已有类型</h3><blockquote>
<p>相当于组合基础上的语法糖。本质上是一种语法糖</p>
</blockquote>
<ul>
<li>使用上类似于Java的继承</li>
<li>出现同名方法时，会使用新的类型的方法。</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> MyNewStruct <span class="keyword">struct</span> &#123;</span><br><span class="line">	*tree.Node <span class="comment">// embedding, 省略了变量名</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(receiver *MyNewStruct)</span> <span class="title">name</span><span class="params">()</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="comment">// 可以直接使用tree.Node的内容</span></span><br><span class="line">	<span class="keyword">return</span> receiver.Value</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(receiver *MyNewStruct)</span> <span class="title">Print</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">// 隐藏掉tree.Node的Print方法。</span></span><br><span class="line">	fmt.Println(<span class="string">&quot;MyNewStruct&quot;</span>)</span><br><span class="line">	fmt.Println(receiver.Value)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(receiver *MyNewStruct)</span> <span class="title">changeValue</span><span class="params">(newValue <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	receiver.Value = newValue</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	my := MyNewStruct&#123;&amp;tree.Node&#123;Value: <span class="number">3</span>&#125;&#125;</span><br><span class="line">	my.Print()</span><br><span class="line">	my.changeValue(<span class="number">4</span>)</span><br><span class="line">	my.Print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="使用选择"><a href="#使用选择" class="headerlink" title="使用选择"></a>使用选择</h3><p>组合使用比较常见。因为别名和组合之间没办法很好的转换。</p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop fs -ls 相关使用问题的记录</title>
    <url>/2017/02/14/hadoop-fs-ls-xiang-guan-shi-yong-wen-ti-de-ji-lu/</url>
    <content><![CDATA[<h1 id="hadoop-fs-ls-相关使用问题记录"><a href="#hadoop-fs-ls-相关使用问题记录" class="headerlink" title="hadoop fs -ls 相关使用问题记录"></a>hadoop fs -ls 相关使用问题记录</h1><p>新人新学，如有错误，烦请提出指正。</p>
<h2 id="1）直接使用：hadoop-fs-ls"><a href="#1）直接使用：hadoop-fs-ls" class="headerlink" title="1）直接使用：hadoop fs -ls"></a>1）直接使用：hadoop fs -ls</h2><p>正确的使用方式是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br></pre></td></tr></table></figure>
<p>如果将path省去，系统会将：/user/hostname/ 添加。<br>所以实际这条指令等价于：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls /user/hostname/</span><br></pre></td></tr></table></figure>
<p>实验截图参考：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog1.png"></p>
<h2 id="2）使用：hadoop-fs-ls"><a href="#2）使用：hadoop-fs-ls" class="headerlink" title="2）使用：hadoop fs -ls /"></a>2）使用：hadoop fs -ls /</h2><p>实际会默认访问hdfs路径。即实际上，这条指令等价于：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls hdfs:///</span><br></pre></td></tr></table></figure>

<h2 id="3）使用：hadoop-fs-ls-hdfs-hostname-port"><a href="#3）使用：hadoop-fs-ls-hdfs-hostname-port" class="headerlink" title="3）使用：hadoop fs -ls hdfs://hostname:port/"></a>3）使用：hadoop fs -ls hdfs://hostname:port/</h2><p>访问hdfs的完整路径。即：访问hdfs文件系统上的hostname(ip亦可):port/<br>实际上，可以使用这条指令访问本地的文件系统：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfs -ls file:///home/hadoop/</span><br></pre></td></tr></table></figure>

<p>参考：<br>[1] <a href="https://stackoverflow.com/questions/28241251/hadoop-fs-ls-results-in-no-such-file-or-directory">hadoop fs -ls results in “no such file or directory”
</a></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s的副本机制与控制器托管</title>
    <url>/2021/06/05/k8s-de-fu-ben-ji-zhi-yu-kong-zhi-qi-tuo-guan/</url>
    <content><![CDATA[<h1 id="实用存活探针管理容器"><a href="#实用存活探针管理容器" class="headerlink" title="实用存活探针管理容器"></a>实用存活探针管理容器</h1><blockquote>
<p>当容器崩溃时，POD能够自动完成容器的重启，但有时候进程没崩溃只是由于某些原因无法正常工作，需要通过存活探针来检测容器进程是否正常运行。并在异常时重启它。</p>
</blockquote>
<p>Kubernetes提供三种探测容器的机制。</p>
<ul>
<li><em>HTTP GET探针</em>：对容器的IP地址的端口和路径执行HTTP GET请求。若探测器收到相应，且响应状态码不代表错误，则认为探测成功。</li>
<li><em>TCP 套接字探针</em>：尝试与容器指定的端口建立TCP连接，如果连接成功建立，则探测成功。</li>
<li><em>Exec探针</em>：在容器内执行任意命令，并检查命令的推出状态码。如果状态码是0，则探测成功。</li>
</ul>
<blockquote>
<p>在pod的spec.containers.livenessProbe中建立存活探针。</p>
</blockquote>
<h2 id="存活探针的问题"><a href="#存活探针的问题" class="headerlink" title="存活探针的问题"></a>存活探针的问题</h2><p>Kubernetes会在你容器崩溃或其存活探针失败时，通过重启容器来保持运行，但这项任务由<strong>承载Pod的节点上的Kubelet执行</strong></p>
<blockquote>
<p>主节点的Kubernetes Control Plane组件不会参与该过程</p>
</blockquote>
<p>这就意味着，如果节点本身崩溃，存活探针或崩溃重启机制将无法执行任何操作，所以我们应该使用ReplicationController或类似机制管理Pod。</p>
<h1 id="使用ReplicationController创建并管理容器"><a href="#使用ReplicationController创建并管理容器" class="headerlink" title="使用ReplicationController创建并管理容器"></a>使用ReplicationController创建并管理容器</h1><blockquote>
<p>ReplicationController是一种Kubernetes资源。用于确保它的POD始终保持运行状态。如果POD因为任何原因消失，则能够创建替代。</p>
</blockquote>
<blockquote>
<p>ReplicationController的管理对象是Pod，而非之前存活探针/自动重启的容器</p>
</blockquote>
<p>ReplicationController会始终保持响应类型的Pod数量与期望相符，即删除多的，创建缺少的。</p>
<h2 id="ReplicationController主要的三部分"><a href="#ReplicationController主要的三部分" class="headerlink" title="ReplicationController主要的三部分"></a>ReplicationController主要的三部分</h2><ul>
<li><em>label selector</em>：标签选择器。<ul>
<li>确定ReplicationController的作用域。即哪些pod归它管。</li>
</ul>
</li>
<li><em>replica count</em>：副本个数。<ul>
<li>期望运行的Pod数量</li>
</ul>
</li>
<li><em>pod temlate</em>：pod模版。<ul>
<li>创建新的Pod使用的模版。</li>
</ul>
</li>
</ul>
<h3 id="修改标签选择器和模版的影响"><a href="#修改标签选择器和模版的影响" class="headerlink" title="修改标签选择器和模版的影响"></a>修改标签选择器和模版的影响</h3><p><strong>更改标签选择器和Pod模版对现有的Pod没有影响</strong>。</p>
<ul>
<li>更改标签选择器会使现有的pod脱离ReplicationController的范围。</li>
<li>ReplicationController也不关心其Pod的实际“内容”。所以修改模版仅仅影响由此ReplicationController创建的新Pod。</li>
</ul>
<h2 id="RC的优势"><a href="#RC的优势" class="headerlink" title="RC的优势"></a>RC的优势</h2><ul>
<li><p>确保一个Pod（或多个Pod副本）持续运行。</p>
</li>
<li><p>集群节点发生故障时，它将为故障节点上运行的受rc管理的所有pod创建替代副本。</p>
<ul>
<li>rc会创建全新的pod实例，而不是将pod实例重新安置到其他节点。</li>
</ul>
</li>
<li><p>轻松实现pod的水平伸缩。</p>
</li>
</ul>
<h2 id="将pod移入-移出ReplicationController的作用域"><a href="#将pod移入-移出ReplicationController的作用域" class="headerlink" title="将pod移入/移出ReplicationController的作用域"></a>将pod移入/移出ReplicationController的作用域</h2><p>通过更改Pod的标签，能够将pod移入/移出ReplicationController的作用域。</p>
<blockquote>
<p>可以通过pod的metadata.ownerReferences字段，找到pod属于哪个ReplicationController。</p>
</blockquote>
<p>当有pod移出/移入时，</p>
]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka-server-stop显示No kafka server to stop的解决办法</title>
    <url>/2017/07/07/kafka-server-stop-xian-shi-no-kafka-server-to-stop-de-jie-jue-ban-fa/</url>
    <content><![CDATA[<p>我们想通过<code>kafka-server-stop.sh</code>友好的关闭kafka时，一般会出现：<code>No kafka server to stop</code>。<br>如图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231613.png" alt="示例"></p>
<p>其实原因很简单，脚本没有找到kafka的PID。<br>解决方案：<br>修改<code>kafka-server-stop.sh</code><br>原来sh脚本应该是这样：（注意第二行 <code>&#39;kafka\.Kafka&#39;</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SIGNAL=$&#123;SIGNAL:-TERM&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意这行！</span></span><br><span class="line">PIDS=$(ps ax | grep -i &#x27;kafka\.Kafka&#x27; | grep java | grep -v grep | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$PIDS&quot; ]; then</span><br><span class="line">  echo &quot;No kafka server to stop&quot;</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  kill -s $SIGNAL $PIDS</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p>第二行改为：（根据自己kafka版本修改）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SIGNAL=$&#123;SIGNAL:-TERM&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">修改这行</span></span><br><span class="line">PIDS=$(ps ax | grep -i &#x27;kafka_2.11-2.2.1&#x27; | grep java | grep -v grep | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$PIDS&quot; ]; then</span><br><span class="line">  echo &quot;No kafka server to stop&quot;</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  kill -s $SIGNAL $PIDS</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p>不确定可以把这段命令拷贝到控制台试一下，它会返回kafka进程的pid号。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231628.png" alt="示例2"><br>简单解释下：<br>2.11是编译kafka的scala版本。<br>2.2.1是kafka版本（是apache的版本，如果是CDK，请查看它底层用的apache的什么版本。）</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka部分topic无法通过无认证端口查询的问题处理</title>
    <url>/2020/05/20/kafka-bu-fen-topic-wu-fa-tong-guo-wu-ren-zheng-duan-kou-cha-xun-de-wen-ti-chu-li/</url>
    <content><![CDATA[<h1 id="kafka部分topic无法通过无认证端口查询的问题处理"><a href="#kafka部分topic无法通过无认证端口查询的问题处理" class="headerlink" title="kafka部分topic无法通过无认证端口查询的问题处理"></a>kafka部分topic无法通过无认证端口查询的问题处理</h1><h2 id="环境概述"><a href="#环境概述" class="headerlink" title="环境概述"></a>环境概述</h2><p>公司一套XX系统，采用无认证的方式访问XX的kafka集群。另外现场环境开启了kerberos认证。</p>
<h2 id="情况概述"><a href="#情况概述" class="headerlink" title="情况概述"></a>情况概述</h2><p>现场由于还是部署测试环境。忽然有一天部署小哥给我说之前好好的topic，现在在页面上访问不了（页面走的无认证端口去访问的kafka），而新建的topic又能够访问。并且这些所有topic都能够通过kinit认证后的命令行访问。并且log里没有相关的报错信息。</p>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>本身问题很简单，不过由于我是和现场小哥远程交流，现场，那个小哥也是新来的，我也刚刚接手这个项目，现场还是没用过的hw环境，走了不少弯路。</p>
<blockquote>
<p>先上结论，hw的1.1的kafka再删除topic后，acl的信息会保留。导致新建同名topic时，acl认证信息会继承。暂不清楚开源及后续版本有没有类似问题。</p>
</blockquote>
<p>心路历程：</p>
<p>最开始翻代码来定位问题，代码里走的都是adminclient的api，也确实走的kafka的无认证端口。</p>
<p>1、担心认证问题，由于我们的topic是走无认证页面建立的，随尝试kdestory后，通过producer测试下topic的权限问题。发现确实之前的topic带着认证。不过现场小哥一直说都是通过页面建立的topic。也让我很疑惑。</p>
<p>2、当时怀疑会不会是代码哪块把认证加进去了，因为刚刚接手还不太熟悉，所以写了个简单的java代码，让前方测试，证实代码本身没问题。</p>
<p>3、不过原因发现了是认证问题，且代码本身没问题，并且由于是部署环境，为了不耽误进度，表象表示今天new的topic又能正常使用，所以我让现场小哥把之前的topic删掉，重新new topic继续部署测试。</p>
<p>4、结果小哥反馈给我说，同一时间new的topic，有的正常，有的依旧异常。</p>
<p>5、最后灵光一闪，找小哥测试核对了下，确实是：依旧异常的topic与之前建立的第一次遇到问题的topic都是重名，正常的topic之前都没见过。当时以为是不是kafka没删干净。（1.1版本），随取zk和log.dir下核实。发现确实删干净了。</p>
<p>6、最后没包多大希望，让小哥看了下kafka-acl。最终定位问题，将kafka-acl里的topic remove掉就ok了</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kerberos on CDH</title>
    <url>/2020/04/27/kerberos-on-cdh/</url>
    <content><![CDATA[<h2 id="为CDH提供kerberos认证："><a href="#为CDH提供kerberos认证：" class="headerlink" title="为CDH提供kerberos认证："></a>为CDH提供kerberos认证：</h2><h3 id="0-准备工作："><a href="#0-准备工作：" class="headerlink" title="0.准备工作："></a>0.准备工作：</h3><ol>
<li>部署JDK、CDH。网上文章很多，暂略。</li>
<li>安装NTP保证时间同步。网上文章很多，暂略。</li>
<li>修改hosts，指定域名。.localdomain 就是域名，之后会使用到。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.xx.xx hadoop       hadoop.localdomain</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-kerberos安装与配置："><a href="#1-kerberos安装与配置：" class="headerlink" title="1.kerberos安装与配置："></a>1.kerberos安装与配置：</h3><h4 id="1-1-kdc-server-安装"><a href="#1-1-kdc-server-安装" class="headerlink" title="1.1 kdc server 安装"></a>1.1 kdc server 安装</h4><p>选择一台机器作为kdc server。其他机器就是client。<br>这里用的是MIT，这是官网：<a href="https://web.mit.edu/kerberos/">Kerberos: The Network Authentication Protocol
</a><br>centos貌似会自带，没有带用yum安装一下就好</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install krb5-server krb5-libs__</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tips：生产上一般建议单独提供一台安装kdc server的机器。这边是测试所以会把cm和kdc server装一台机器上</p>
</blockquote>
<h4 id="1-2-kdc-server配置"><a href="#1-2-kdc-server配置" class="headerlink" title="1.2 kdc server配置"></a>1.2 kdc server配置</h4><h5 id="1-2-1-配置-etc-krb5-conf，下面是一个示例："><a href="#1-2-1-配置-etc-krb5-conf，下面是一个示例：" class="headerlink" title="1.2.1 配置/etc/krb5.conf，下面是一个示例："></a>1.2.1 配置/etc/krb5.conf，下面是一个示例：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">    default_realm = LOCALDOMAIN</span><br><span class="line">    dns_lookup_realm = false</span><br><span class="line">    dns_lookup_kdc = false</span><br><span class="line">    ticket_lifetime = 24h</span><br><span class="line">    forwardable = true</span><br><span class="line">    udp_preference_limit = 1000000</span><br><span class="line">[realms]</span><br><span class="line">    LOCALDOMAIN = &#123;</span><br><span class="line">        kdc = hadoop</span><br><span class="line">        admin_server = hadoop</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">    .localdomain = LOCALDOMAIN</span><br><span class="line">    localdomain = LOCALDOMAIN</span><br><span class="line"></span><br><span class="line">[logging]</span><br><span class="line">    kdc = FILE:/var/log/krb5kdc.log</span><br><span class="line">    admin_server = FILE:/var/log/kadmin.log</span><br><span class="line">    default = FILE:/var/log/krb5lib.log</span><br></pre></td></tr></table></figure>
<p>几个注意点：</p>
<blockquote>
<p>[realms]里的hadoop ，配置成自己的hostname。后面可以跟域名也可以不跟。<br>注意域名LOCALDOMAIN要大写（domain_realm左边要小写）。最好和准备里hosts里的一样。不一样没测试过。</p>
</blockquote>
<h5 id="1-2-2-配置-var-kerberos-krb5kdc-kdc-conf，下面是一个示例："><a href="#1-2-2-配置-var-kerberos-krb5kdc-kdc-conf，下面是一个示例：" class="headerlink" title="1.2.2 配置/var/kerberos/krb5kdc/kdc.conf，下面是一个示例："></a>1.2.2 配置/var/kerberos/krb5kdc/kdc.conf，下面是一个示例：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">default_realm = LOCALDOMAIN</span><br><span class="line"></span><br><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> LOCALDOMAIN = &#123;</span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>注意点：</p>
<blockquote>
<p>supported_enctypes这一项，默认包含aes256-cts。这个需要在server和client上额外安装。本次示例没使用，直接删去该项。如果需要，可参考：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo wget http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip</span><br><span class="line">sudo unzip jce_policy-8.zip</span><br><span class="line">sudo mv UnlimitedJCEPolicyJDK8/*.jar $JAVA_HOME/jre/lib/security </span><br><span class="line">pscp -h list_krb_clients $JAVA_HOME/jre/lib/security/US_export_policy.jar /tmp </span><br><span class="line">pscp -h list_krb_clients $JAVA_HOME/jre/lib/security/local_policy.jar /tmp </span><br><span class="line">pssh -h list -l admin -A &quot;sudo cp /tmp/US_export_policy.jar $JAVA_HOME/jre/lib/security/&quot; </span><br><span class="line">pssh -h list -l admin -A &quot;sudo cp /tmp/local_policy.jar $JAVA_HOME/jre/lib/security/&quot;</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="1-2-3-配置-var-kerberos-krb5kdc-kadm5-acl"><a href="#1-2-3-配置-var-kerberos-krb5kdc-kadm5-acl" class="headerlink" title="1.2.3 配置/var/kerberos/krb5kdc/kadm5.acl"></a>1.2.3 配置/var/kerberos/krb5kdc/kadm5.acl</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*/admin@LOCALDOMAIN	    *</span><br></pre></td></tr></table></figure>
<h5 id="1-2-4-为kerberos创建数据库"><a href="#1-2-4-为kerberos创建数据库" class="headerlink" title="1.2.4 为kerberos创建数据库"></a>1.2.4 为kerberos创建数据库</h5><ol>
<li>创建数据库： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kdb5_util create -r CW.COM -s</span><br></pre></td></tr></table></figure>
 这段代码含义是：-r 指定realm，简单来说就是本例的LOCALDOMAIN。-s表示会给master key在stash file中创建一份copy。<br> 这一步会要求为这个数据库设置密码。</li>
<li>创建用户： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@kdc ~]# kadmin.local</span><br><span class="line">kadmin.local:  addprinc root/admin  &lt;= 创建一个root/admin用户。接着会让你设置密码</span><br><span class="line">kadmin.local:  ktadd -k /var/kerberos/krb5kdc/root.keytab root/admin  &lt;= 将该用户写入keytab，可选。</span><br><span class="line">kadmin.local:  exit</span><br></pre></td></tr></table></figure>
</li>
<li>启动服务，并设置自启： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start krb5kdc.service</span><br><span class="line">systemctl start kadmin.service</span><br><span class="line">systemctl enable krb5kdc.service</span><br><span class="line">systemctl enable kadmin.service</span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="1-2-5-测试："><a href="#1-2-5-测试：" class="headerlink" title="1.2.5 测试："></a>1.2.5 测试：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kinit root/admin  &lt;= 刚刚创建的，回车输入密码。</span><br><span class="line">kinit -kt /var/kerberos/krb5kdc/root.keytab root/admin@LOCALDOMAIN &lt;= 使用keytab登陆。密码已存在keytab中。</span><br><span class="line">klist  &lt;= 查看当前认证的用户</span><br></pre></td></tr></table></figure>
<h4 id="1-3-client安装："><a href="#1-3-client安装：" class="headerlink" title="1.3 client安装："></a>1.3 client安装：</h4><h5 id="1-3-1-所有client安装："><a href="#1-3-1-所有client安装：" class="headerlink" title="1.3.1 所有client安装："></a>1.3.1 所有client安装：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install krb5-workstation krb5-libs</span><br></pre></td></tr></table></figure>
<p>然后将kdc server的/etc/krb5.conf 分发到所有集群上。替换他们的/etc/krb5.conf。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/krb5.conf hadoop002:/etc/</span><br></pre></td></tr></table></figure>
<h5 id="1-3-2-cm所在节点额外安装："><a href="#1-3-2-cm所在节点额外安装：" class="headerlink" title="1.3.2 cm所在节点额外安装："></a>1.3.2 cm所在节点额外安装：</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install openldap-clients</span><br></pre></td></tr></table></figure>

<h2 id="2-kerberos-on-cdh"><a href="#2-kerberos-on-cdh" class="headerlink" title="2.kerberos on cdh"></a>2.kerberos on cdh</h2><h3 id="2-1-为cm创建账号："><a href="#2-1-为cm创建账号：" class="headerlink" title="2.1 为cm创建账号："></a>2.1 为cm创建账号：</h3><p>和1.2.4中一样，记住这步设置的密码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@kdc ~]# kadmin.local</span><br><span class="line">Authenticating as principal root/admin@LOCALDOMAIN with password.    </span><br><span class="line">kadmin.local:  addprinc cloudera-scm/admin@LOCALDOMAIN</span><br><span class="line">kadmin.local:  exit</span><br></pre></td></tr></table></figure>

<h3 id="2-2-cm启用kerberos："><a href="#2-2-cm启用kerberos：" class="headerlink" title="2.2 cm启用kerberos："></a>2.2 cm启用kerberos：</h3><blockquote>
<p>主要是界面操作。这一步的坑主要是第四步的加密算法。</p>
</blockquote>
<ol>
<li>点击Administration -&gt; security -&gt; kerberos -&gt; enable</li>
<li>阅读一些提示信息，没问题就下一步。(基本就是让你确认kerberos是否装好了)<ul>
<li>KDC已经安装好并且正在运行</li>
<li>将KDC配置为允许renewable tickets with non-zerolifetime，我们在之前修改kdc.conf文件的时候已经添加了max_life和max_renewable_life这个2个属性，前者表示服务端允许的Service ticket最大生命周期，后者表示服务端允许的Service ticket更新周期。这2个属性必须分别大于等于客户端对应的配置ticket_lifetime和renew_lifetime。我们假设，不这样进行配置：ticket_lifetime = 8d, max_life = 7d, renew_lifetime = 25h, max_renew_life = 24h，那么可能造成的结果就是当service持有的票据超过24小时没有去更新，在第24.5小时的时候去进行更新，请求会遭到拒绝，报错：Ticket expired while renewing credentials，永远无法进行正常更新。对于Cloudera来说，因为更新机制被透明(Cloudera有renew进程会去定期更新)，即使我们手动使用<code>modprinc -maxrenewlife 1week krbtgt/DOMAIN.COM@DOMAIN.COM</code> 进行更新，也无济于事。</li>
<li>在Cloudera Manager Server上安装openldap-clients；</li>
<li>为Cloudera Manager创建了超级管理员principal，使其能够有权限在KDC中创建其他的principals；</li>
</ul>
</li>
<li>配置kdc信息：<ul>
<li>注意填写的内容与krb5.conf里一一对应。</li>
<li>这里的 Kerberos Encryption Types 一定要选择之前krb5.conf或/var/kerberos/krb5kdc/kdc.conf里配置有的。不然可能会报错：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KDC has no support for encryption type while getting initial credentials</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>尽量不让cdh管理krb5.conf：</li>
<li>后面一路continue即可。</li>
</ol>
<p>这一步cdh实际会进行如下操作：</p>
<pre><code>1. 集群中有多少个节点，每个账户就会生成对应个数的 principal ;
2. 为每个对应的 principal 创建 keytab；
3. 部署 keytab 文件到指定的节点中；
4. 在每个服务的配置文件中加入有关 Kerberos 的配置；
</code></pre>
<p>到这里，其实集群就已经完成kerberos认证了。不过真正要使用，还需要些其他操作。</p>
<h3 id="2-3-创建我们使用的认证账户"><a href="#2-3-创建我们使用的认证账户" class="headerlink" title="2.3 创建我们使用的认证账户"></a>2.3 创建我们使用的认证账户</h3><p>之前，我们使用hdfs，都会使用cdh提供的hdfs账户。不过完成kerberos认证后，hdfs账户不能直接使用了，而cdh创建的我们又不知道密码。所以我们可以为其在kerberos生成一个key：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@kdc ~]# kadmin.local</span><br><span class="line">Authenticating as principal root/admin@LOCALDOMAIN with password.    </span><br><span class="line">kadmin.local:  addprinc hdfs/test@LOCALDOMAIN  &lt;= 保证/前是hdfs即可</span><br><span class="line">kadmin.local:  exit</span><br></pre></td></tr></table></figure>
<p>之后通过kinit登陆该认证，就可以以hdfs用户的身份操作cdh了。</p>
<h2 id="3-测试及常见问题："><a href="#3-测试及常见问题：" class="headerlink" title="3.测试及常见问题："></a>3.测试及常见问题：</h2><h3 id="3-1-基本测试："><a href="#3-1-基本测试：" class="headerlink" title="3.1 基本测试："></a>3.1 基本测试：</h3><ol>
<li><p>登陆：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kadmin.local &lt;= kdc可以直接使用kadmin.local登陆。client需要先kinit认证后，使用kadmin登陆。</span><br><span class="line">klist &lt;= 登陆后使用klist列出当前的认证账户。</span><br></pre></td></tr></table></figure></li>
<li><p>hdfs使用：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kinit后可以直接使用hadoop fs -ls / 查看hdfs的情况。</span><br><span class="line">kdestory后将无法使用hadoop fs -ls / 查看hdfs的情况。</span><br></pre></td></tr></table></figure></li>
<li><p>运行mr/spark任务：</p>
<blockquote>
<p>spark需要额外设置才能使用,点一点界面就能完成很简单,参考：<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/sg_spark_auth.html">Spark Authentication</a></p>
</blockquote>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-examples.jar pi 10 10000</span><br><span class="line"></span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster $SPARK_HOME/lib/spark-examples.jar 10</span><br></pre></td></tr></table></figure></li>
<li><p>使用yarn查看任务log：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId &lt;ID&gt;</span><br></pre></td></tr></table></figure>
<p> 注意查看log的认证账户要与提交的用户一致，否则看不到log</p>
<h3 id="3-2-常见问题："><a href="#3-2-常见问题：" class="headerlink" title="3.2 常见问题："></a>3.2 常见问题：</h3></li>
<li><p>运行mr/spark任务时提示没有用户：<br> 问题：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	Application application_1527494654301_0004 failed 2 times due to AM Container for appattempt_1527494654301_0004_000002 exited with exitCode: -1000</span><br><span class="line">For more detailed output, check application tracking page:http://fetch-master:8088/proxy/application_1527494654301_0004/Then, click on links to logs of each attempt.</span><br><span class="line">Diagnostics: Application application_1527494654301_0004 initialization failed (exitCode=255) with output: main : command provided 0</span><br><span class="line">main : run as user is admin</span><br><span class="line">main : requested hdfs user is admin</span><br><span class="line">User admin not found</span><br><span class="line">Failing this attempt. Failing the application.</span><br></pre></td></tr></table></figure>
<p> 解决：<br> 参考2.3，创建需要的用户认证。这里就是创建一个admin@xxx</p>
</li>
<li><p>运行mr/spark任务时提示id异常：<br> 问题：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Requested user hdfs is not whitelisted and has id 488,which is below the minimum allowed 1000</span><br></pre></td></tr></table></figure>
<p> 解决：</p>
<ol>
<li>修改用户id。</li>
<li>或修改Clouder关于这个该项的设置 ：ARN -&gt; NodeManager -&gt; Security -&gt; min.user.id改为0</li>
</ol>
</li>
<li><p>运行mr/spark任务时提示用户被禁用：<br> 问题：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Requested user hdfs is banned</span><br></pre></td></tr></table></figure>
<p> 解决：<br> 在CM yarn-&gt; Configuration页面，配置禁止的系统用户banned.users列表。</p>
</li>
<li><p>找不到yarn log<br> 问题：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Can not find the logs for the application with the appOwner: hdfs</span><br></pre></td></tr></table></figure>
<p> 解决：<br> 用运行这个任务的认证查看yarn log</p>
</li>
<li><p>没有账号</p>
<p>问题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Client &#x27;USERNAME-REDACTED&#x27; not found in Kerberos database while getting initial credentials</span><br></pre></td></tr></table></figure>

<p>解决：</p>
<p>创建账号，也可能是账号填错了</p>
</li>
</ol>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title>manage sparkTask in code</title>
    <url>/2020/05/14/manage-sparktask-in-code/</url>
    <content><![CDATA[<h1 id="Manage-SparkTask-in-code"><a href="#Manage-SparkTask-in-code" class="headerlink" title="Manage SparkTask in code"></a>Manage SparkTask in code</h1><h2 id="submit-Spark"><a href="#submit-Spark" class="headerlink" title="submit Spark"></a>submit Spark</h2><ol>
<li>Get SparkLauncher:</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// init SparkLauncher</span></span><br><span class="line">SparkLauncher sparkLauncher = <span class="keyword">new</span> SparkLauncher();</span><br><span class="line"><span class="comment">// set SparkLauncher</span></span><br><span class="line">sparkLauncher.setConf().setXXX()...;</span><br><span class="line"><span class="keyword">return</span> sparkLauncher;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>Start SparkTask, get SparkAppHandle:</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SparkAppHandle handle = sparkLauncher.startApplication();</span><br></pre></td></tr></table></figure>
<ol>
<li>(optional) 如果使用yarn的cluster模式，在Saprk任务running后，需要切断当前机器与AM的联系，这样可以释放一些资源。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">String appIdOnYarn = handle.getAppId();</span><br><span class="line">	if (appIdOnYarn != null) &#123;</span><br><span class="line">	handle.disconnect();</span><br><span class="line">	handle.kill();</span><br><span class="line">&#125;</span><br><span class="line">	// 切段后，需要在我们本地管理的中将handle职位null，这有助于之后判断如何去管理这个spark任务。</span><br></pre></td></tr></table></figure>

<h2 id="stop-Spark"><a href="#stop-Spark" class="headerlink" title="stop Spark"></a>stop Spark</h2><h3 id="Task-have-not-submit-success"><a href="#Task-have-not-submit-success" class="headerlink" title="Task have not submit success."></a>Task have not submit success.</h3><p>use SparkHandle to Stop:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// get SparkHandle , eg. manage in a map: runningApplications.</span><br><span class="line">runningApplications.get(appId).getSparkAppHandle().kill();</span><br></pre></td></tr></table></figure>
<h3 id="Task-on-Yarn"><a href="#Task-on-Yarn" class="headerlink" title="Task on Yarn"></a>Task on Yarn</h3><p>use YarnClient to Stop:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 0. 之前操作有拿到appIdOnYarn</span><br><span class="line">String appIdOnYarn = handle.getAppId();</span><br><span class="line">// 1. 拼接。为上面拿到的appIdOnYarn增加前缀，并生成ApplicationId</span><br><span class="line">int pos1 = &quot;application_&quot;.length() - 1;</span><br><span class="line">int pos2 = appIdStr.indexOf(95, pos1 + 1);</span><br><span class="line">if (pos2 &lt; 0) &#123;</span><br><span class="line">	throw new IllegalArgumentException(&quot;Invalid ApplicationId: &quot; + appIdStr);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	long rmId = Long.parseLong(appIdStr.substring(pos1 + 1, pos2));</span><br><span class="line">	int appId = Integer.parseInt(appIdStr.substring(pos2 + 1));</span><br><span class="line">	return ApplicationId.newInstance(rmId, appId); =&gt; ApplicationIdInstance</span><br><span class="line">&#125;</span><br><span class="line">// 3. Stop:</span><br><span class="line">yarnClient.killApplication(ApplicationIdInstance);</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>spark-3.0 Hadoop2.6-CDH编译的问题</title>
    <url>/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/</url>
    <content><![CDATA[<p>使用cdh5版本编译Spark3时，会出现如下问题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; [ERROR] [Error]</span><br><span class="line">&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:298:</span><br><span class="line">&gt; value setRolledLogsIncludePattern is not a member of</span><br><span class="line">&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext</span><br><span class="line">&gt; [ERROR] [Error]</span><br><span class="line">&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:300:</span><br><span class="line">&gt; value setRolledLogsExcludePattern is not a member of</span><br><span class="line">&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext</span><br></pre></td></tr></table></figure>

<p>这个问题起始官方曾经修复过，不过后来可能改过这部分代码，导致编译报错：</p>
<p><a href="https://github.com/apache/spark/pull/16884">Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p>
<p>这是实际解决的连接：</p>
<p><a href="https://github.com/apache/spark/pull/16884/commits/d7c7e81bafa229bb6083ed5b29789b3f9cb78bf7">[SPARK-19545][YARN]Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p>
<p>我们仔细对比下Spark3中的代码，这段代码亦和修复前的代码一样（只是spark3中多了try-catch）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> logAggregationContext = <span class="type">Records</span>.newRecord(classOf[<span class="type">LogAggregationContext</span>])</span><br><span class="line">logAggregationContext.setRolledLogsIncludePattern(includePattern)</span><br><span class="line">sparkConf.get(<span class="type">ROLLED_LOG_EXCLUDE_PATTERN</span>).foreach &#123; excludePattern =&gt;</span><br><span class="line">  logAggregationContext.setRolledLogsExcludePattern(excludePattern)</span><br><span class="line">&#125;</span><br><span class="line">appContext.setLogAggregationContext(logAggregationContext)</span><br></pre></td></tr></table></figure>

<p>然后是当初修复后的代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> logAggregationContext = <span class="type">Records</span>.newRecord(classOf[<span class="type">LogAggregationContext</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// These two methods were added in Hadoop 2.6.4, so we still need to use reflection to</span></span><br><span class="line"><span class="comment">// avoid compile error when building against Hadoop 2.6.0 ~ 2.6.3.</span></span><br><span class="line"><span class="keyword">val</span> setRolledLogsIncludePatternMethod =</span><br><span class="line">  logAggregationContext.getClass.getMethod(<span class="string">&quot;setRolledLogsIncludePattern&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">setRolledLogsIncludePatternMethod.invoke(logAggregationContext, includePattern)</span><br><span class="line"></span><br><span class="line">sparkConf.get(<span class="type">ROLLED_LOG_EXCLUDE_PATTERN</span>).foreach &#123; excludePattern =&gt;</span><br><span class="line">  <span class="keyword">val</span> setRolledLogsExcludePatternMethod =</span><br><span class="line">    logAggregationContext.getClass.getMethod(<span class="string">&quot;setRolledLogsExcludePattern&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">  setRolledLogsExcludePatternMethod.invoke(logAggregationContext, excludePattern)</span><br><span class="line">&#125;</span><br><span class="line">appContext.setLogAggregationContext(logAggregationContext)</span><br></pre></td></tr></table></figure>

<p>所以只要把源代码改动下就ok了。</p>
<p>Spark在<a href="https://issues.apache.org/jira/browse/SPARK-25016">SPARK-25016</a>中移除了对Hadoop2.6的支持，代码改动也是原来那个时候。</p>
<p>可能新的代码在hadoop2.7中更加高效？如果仍然需要使用hadoop2.6的集群。就只能自己修改源码然后编译了。</p>
]]></content>
      <categories>
        <category>Spark3</category>
      </categories>
      <tags>
        <tag>Spark3</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-3.0 动态分区裁剪</title>
    <url>/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/</url>
    <content><![CDATA[<h2 id="Spark中的静态分区裁剪"><a href="#Spark中的静态分区裁剪" class="headerlink" title="Spark中的静态分区裁剪"></a>Spark中的静态分区裁剪</h2><p>用过Spark的都知道，其实简单来说就是谓词下推。在Spark执行下述查询时，能够尽可能将谓词下推至扫描文件的阶段。从而减少读取的数据量，实现处理速度的提升：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Sales <span class="keyword">WHERE</span> day_of_week <span class="operator">=</span> ‘Mon’</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150635.png" alt="分区裁剪"></p>
<h2 id="Spark3-0中的动态分区裁剪"><a href="#Spark3-0中的动态分区裁剪" class="headerlink" title="Spark3.0中的动态分区裁剪"></a>Spark3.0中的动态分区裁剪</h2><p>所谓的<strong>动态分区裁剪</strong>就是基于运行时（run time）推断出来的信息来进一步进行分区裁剪。</p>
<p>在Join时，如果我们只需要一部分DIM Table中的数据，静态分区裁剪能够将这部分裁剪下推下去。</p>
<img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150941.png" style="zoom:50%;" />

<p>动态分区裁剪则更进一步，会将这部分裁剪作用到FACT Table前，然后再进行Join。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622151112.png" alt="动态分区裁剪"></p>
<h2 id="开启动态分区裁剪"><a href="#开启动态分区裁剪" class="headerlink" title="开启动态分区裁剪"></a>开启动态分区裁剪</h2><p>开启动态分区裁剪：</p>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.enabled</code>设置为<code>true</code>(默认)</p>
<p>其他参数：</p>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.useStats</code>，<code>true</code>(默认)：</p>
<ul>
<li><p>在动态分区修剪后，将使用DISTINCT COUNT统计信息计算分区表的数据大小，以便在广播重用不适用的情况下，评估是否值得添加额外的子查询作为裁剪筛选器。</p>
</li>
<li><blockquote>
<p>When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p>
</blockquote>
</li>
</ul>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio</code>，<code>0.5</code>(默认)：</p>
<ul>
<li><p>当统计数据不可用或配置没有使用时，此配置将用作计算动态分区修剪后分区表数据大小的后备筛选器比率，以便在广播重用不适用的情况下评估是否值得添加额外的子查询作为裁剪筛选器。</p>
</li>
<li><blockquote>
<p>When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p>
</blockquote>
</li>
</ul>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcast</code>，true(默认)：</p>
<ul>
<li><p>动态分区裁剪将寻求重用来自broadcast hash join操作的广播结果。</p>
</li>
<li><blockquote>
<p>When true, dynamic partition pruning will seek to reuse the broadcast results from a broadcast hash join operation.</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark3</category>
      </categories>
      <tags>
        <tag>Spark3</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-3 DataSourceV2-01</title>
    <url>/2020/06/24/spark-3-datasourcev2-01/</url>
    <content><![CDATA[<h2 id="Java-Interfaces"><a href="#Java-Interfaces" class="headerlink" title="Java Interfaces"></a>Java Interfaces</h2><p>Spark3中，DataSourceV2采用Java进行编写，而非Scala。这样做的主要目的是为了更好的和Java交互。</p>
<p>大部分接口可以在下面的包中找到：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.spark.sql.connector.catalog</span><br></pre></td></tr></table></figure>

<p>后文是DataSrouceV2的关键接口。</p>
<h2 id="TableProvider"><a href="#TableProvider" class="headerlink" title="TableProvider"></a>TableProvider</h2><p>这是一个提供读写的数据源，他提供结构化数据。需要实现如下方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">StructType <span class="title">inferSchema</span><span class="params">(CaseInsensitiveStringMap options)</span></span>;</span><br><span class="line"><span class="function">Table <span class="title">getTable</span><span class="params">(StructType schema, Transform[] partitioning, Map&lt;String, String&gt; properties)</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>inferSchema：</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Infer the schema of the table identified by the given options.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> options an immutable case-insensitive string-to-string map that can identify a table,</span></span><br><span class="line"><span class="comment"> *                e.g. file path, Kafka topic name, etc.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过给定的options，推断表结构。从而实现结构化数据。</p>
<ol start="2">
<li>getTable</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a &#123;<span class="doctag">@link</span> Table&#125; instance with the specified table schema, partitioning and properties</span></span><br><span class="line"><span class="comment"> * to do read/write. The returned table should report the same schema and partitioning with the</span></span><br><span class="line"><span class="comment"> * specified ones, or Spark may fail the operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> schema The specified table schema.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partitioning The specified table partitioning.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> properties The specified table properties. It&#x27;s case preserving (contains exactly what</span></span><br><span class="line"><span class="comment"> *                   users specified) and implementations are free to use it case sensitively or</span></span><br><span class="line"><span class="comment"> *                   insensitively. It should be able to identify a table, e.g. file path, Kafka</span></span><br><span class="line"><span class="comment"> *                   topic name, etc.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过用户给定的信息去load数据。</p>
<h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><p>Table是逻辑结构化数据集的接口，需要实现下述3种方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">String <span class="title">name</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">StructType <span class="title">schema</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Set&lt;TableCapability&gt; <span class="title">capabilities</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>name</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A name to identify this table. Implementations should provide a meaningful name, like the</span></span><br><span class="line"><span class="comment"> * database and table name from catalog, or the location of files for this table.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>用于标示表的名称。</p>
<ol start="2">
<li>schema</li>
</ol>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the schema of this table. If the table is not readable and doesn&#x27;t have a schema, an</span><br><span class="line"> * empty schema can be returned here.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回表的schema，如果表不可读，或者没有schema时，将返回空的schema</p>
<ol start="3">
<li>capabilities</li>
</ol>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the set of capabilities for this table.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回表的性能。这是Spark3新的功能，允许指定支持哪种类型的操作表。如BATCH_READ、BATCH_WRITE。这有助于Spark在尝试运行操作之前验证这些信息。</p>
<h2 id="SupportsRead"><a href="#SupportsRead" class="headerlink" title="SupportsRead"></a>SupportsRead</h2><p>该接口表示数据源是支持读的，需要实现下述方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">ScanBuilder <span class="title">newScanBuilder</span><span class="params">(CaseInsensitiveStringMap options)</span></span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a &#123;<span class="doctag">@link</span> ScanBuilder&#125; which can be used to build a &#123;<span class="doctag">@link</span> Scan&#125;. Spark will call this</span></span><br><span class="line"><span class="comment"> * method to configure each data source scan.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> options The options for reading, which is an immutable case-insensitive</span></span><br><span class="line"><span class="comment"> *                string-to-string map.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回一个ScanBuilder用来创建一个Scan（Scan会用来读取表，后文会介绍）。Spark会在配置Souce Scan时调用这个方法。</p>
<h2 id="ScanBuilder"><a href="#ScanBuilder" class="headerlink" title="ScanBuilder"></a>ScanBuilder</h2><p>该接口会用来创建Scan，这个接口可以配合谓词下推（SupportsPushDownXYZ），帮助Scan读取所需数据。需要实现下述方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Scan build();</span><br></pre></td></tr></table></figure>

<h2 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h2><p>数据源扫描的逻辑表现形式。此接口用于提供逻辑信息，如实际的读取模式。这是所有不同扫描模式的通用接口（批、微批等）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">StructType <span class="title">readSchema</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">default</span> Batch <span class="title">toBatch</span><span class="params">()</span></span>&#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>readSchema</p>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the actual schema of this data source scan, which may be different from the physical</span></span><br><span class="line"><span class="comment"> * schema of the underlying storage, as column pruning or other optimizations may happen.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>数据源的实际结构，这个看起来似乎和Table接口有重复。这是因为，在发生列裁剪或其他优化后，结构可能发生改变。这个方法返回的是最初的schema。</p>
<ol start="2">
<li>toBatch</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the physical representation of this scan for batch query. By default this method throws</span></span><br><span class="line"><span class="comment"> * exception, data sources must overwrite this method to provide an implementation, if the</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> Table&#125; that creates this scan returns &#123;<span class="doctag">@link</span> TableCapability#BATCH_READ&#125; support in its</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> Table#capabilities()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> UnsupportedOperationException</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>在批处理时，这个方法需要被override。</p>
<h2 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h2><p>进行批处理时的数据源的物理表现，这个接口用于提供一些物理信息，如：扫描的数据有多少分区，如何去读取分区数据。</p>
<p>需要实现如下方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">InputPartition[] planInputPartitions();</span><br><span class="line"><span class="function">PartitionReaderFactory <span class="title">createReaderFactory</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>planInputPartitions</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a list of &#123;<span class="doctag">@link</span> InputPartition input partitions&#125;. Each &#123;<span class="doctag">@link</span> InputPartition&#125;</span></span><br><span class="line"><span class="comment"> * represents a data split that can be processed by one Spark task. The number of input</span></span><br><span class="line"><span class="comment"> * partitions returned here is the same as the number of RDD partitions this scan outputs.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * If the &#123;<span class="doctag">@link</span> Scan&#125; supports filter pushdown, this Batch is likely configured with a filter</span></span><br><span class="line"><span class="comment"> * and is responsible for creating splits for that filter, which is not a full scan.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * This method will be called only once during a data source scan, to launch one Spark job.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回输入分区列表，这个方法决定了数据的分区数。</p>
<ol start="2">
<li>createReaderFactory</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a factory to create a &#123;<span class="doctag">@link</span> PartitionReader&#125; for each &#123;<span class="doctag">@link</span> InputPartition&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回一个用于创建PartitionReader的工厂，PartitionReader和InputPartition是一一对应的。</p>
<h2 id="PartitionReaderFactory"><a href="#PartitionReaderFactory" class="headerlink" title="PartitionReaderFactory"></a>PartitionReaderFactory</h2>]]></content>
      <categories>
        <category>Spark3</category>
      </categories>
      <tags>
        <tag>Spark3</tag>
      </tags>
  </entry>
  <entry>
    <title>structured里使用state计算</title>
    <url>/2020/05/15/structured-li-shi-yong-state-ji-suan/</url>
    <content><![CDATA[<h1 id="structured里使用state计算"><a href="#structured里使用state计算" class="headerlink" title="structured里使用state计算"></a>structured里使用state计算</h1><p>structured主要提供了mapGroupsWithState和flatMapGroupsWithState来进行有状态计算，以mapGroupsWithState为例，其主要接受：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapGroupsWithState</span></span>[<span class="type">S</span>: <span class="type">Encoder</span>, <span class="type">U</span>: <span class="type">Encoder</span>](</span><br><span class="line">      timeoutConf: <span class="type">GroupStateTimeout</span>)(</span><br><span class="line">      func: (<span class="type">K</span>, <span class="type">Iterator</span>[<span class="type">V</span>], <span class="type">GroupState</span>[<span class="type">S</span>]) =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>] = &#123;...&#125;</span><br></pre></td></tr></table></figure>

<p>对于func来说：</p>
<ul>
<li>K：Key</li>
<li>Iterator[V]：Value的迭代器</li>
<li>GroupState[S]：现在的State</li>
<li>U：输出的类型。</li>
</ul>
<h2 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h2><p>在使用[map|flatMap]GroupsWithState是，可以通过GroupStateTimeout完成对与超时数据的设置。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">.mapGroupsWithState(<span class="type">GroupStateTimeout</span>.<span class="type">EventTimeTimeout</span>())(updateAcrossEvents)</span><br></pre></td></tr></table></figure>

<p>几个注意点：</p>
<ol>
<li><p>GroupStateTimeout设置的参数类型对整个GroupState都有效。但具体的值设置是当group使用setTimeout方法时确定的。</p>
</li>
<li><p>对于ProcessingTimeTimeout，可以通过<strong>GroupState.setTimeoutDuration</strong>设置超时值。如果超时时间设置为Dms：</p>
<ul>
<li>时钟时间增加Dms前，超时不会发生。</li>
<li>超时时间没有严格的上限限制，<strong>即如果数据流中没有任何数据，就没有出发超时的机会，知道有数据时才会触发超时处理。</strong></li>
</ul>
</li>
<li><p>对于EventTimeTimeout，需要设置Dataset.withWatermark()，这样，超时的事件时间会被直接过滤。超时值可以通过**GroupState.setTimeoutTimestamp()**来设置。setTimeoutTimestamp有两类版本，一类是设定一个固定时间戳，另一类是在一个指定的时间戳上再指定一个duration：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setTimeoutTimestamp</span></span>(timestampMs: <span class="type">Long</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setTimeoutTimestamp</span></span>(timestampMs: <span class="type">Long</span>, additionalDuration: <span class="type">String</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>



<p>对于事件时间，是否超时将有两个因素控制：水位线和timeout设置：</p>
<ul>
<li>当水印时间大于timeout设置时，timeout永远不会发生。</li>
<li>与ProcessingTimeout一样，<strong>超时时间没有严格的上限限制</strong>。</li>
</ul>
</li>
<li><p>在func被调用时，timeout会被重置。即当有新数据到达，或发生超时时。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>structured里恢复应用程序</title>
    <url>/2020/05/21/structured-li-hui-fu-ying-yong-cheng-xu/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>updateStateByKey vs mapWithState</title>
    <url>/2019/07/26/updatestatebykey-vs-mapwithstate/</url>
    <content><![CDATA[<h2 id="有状态转换"><a href="#有状态转换" class="headerlink" title="有状态转换"></a>有状态转换</h2><p>在SparkStreaming中，有时候我们需要进行聚合操作，且依赖于以前的数据，就需要使用到有状态算子。<br>不过由于SparkStreaming中，有状态算子依赖Checkpoint，而Checkpoint有个很大的弊端就是修改代码后失效，所以实际使用时需要用到他们的初始化值。但目前应该先理解有状态转换算子。</p>
<h2 id="updateWithStateByKey"><a href="#updateWithStateByKey" class="headerlink" title="updateWithStateByKey"></a>updateWithStateByKey</h2><p>简单来说，这个算子的功能是按照Key，将上一批次得到的State与当前批次的Value进行func操作。</p>
<h3 id="使用层面："><a href="#使用层面：" class="headerlink" title="使用层面："></a>使用层面：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def updateStateByKey[S: ClassTag](</span><br><span class="line">    updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],</span><br><span class="line">    partitioner: Partitioner,</span><br><span class="line">    rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope &#123;</span><br><span class="line">  val cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">  val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; &#123;</span><br><span class="line">    cleanedFunc(it)</span><br><span class="line">  &#125;</span><br><span class="line">  new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用时主要需要传入一个函数func：</p>
<ol>
<li>func传入两个参数：更新值（Seq[V]），累计值（Option[U]）。</li>
<li>将更新值与累计值进行func操作，返回累计值Option[U]。</li>
<li>这个返回值会作为下一阶段的累计值使用，如此循环。</li>
</ol>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val stateDstream = inputKeyDStream</span><br><span class="line">    .updateStateByKey((newValues: Seq[Int], oldValue: Option[Int]) =&gt; &#123;</span><br><span class="line">		// 更新状态</span><br><span class="line">        Some(newValues.foldLeft(oldValue.getOrElse(0))(_ + _))</span><br><span class="line">    &#125;)</span><br><span class="line">    .foreachRDD(rdd =&gt; &#123;</span><br><span class="line">        rdd.foreach(pair =&gt; println(s&quot;k=$&#123;pair._1&#125; v=$&#123;pair._2&#125;&quot;))</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<h3 id="实现原理："><a href="#实现原理：" class="headerlink" title="实现原理："></a>实现原理：</h3><ol>
<li>updateStateByKey方法内部最后会new一个StateDStream。</li>
<li>StateDStream的compute方法会先获取上一个batch计算出的RDD（历史数据）。然后获取本次batch中StateDStream的父类计算出的RDD（本次数据）。然后调用computeUsingPreviousRDD方法。</li>
<li>computeUsingPreviousRDD方法会对两个RDD进行cogroup操作（性能低），并应用func。</li>
</ol>
<p>逻辑图如下：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581835901663.png" alt="updateStateByKey方法内部最后会new"></p>
<h2 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState"></a>mapWithState</h2><p>updateWithStateByKey每次更新状态时，都需要对状态数据进行全量的聚合操作，这一步是相当耗费性能的。从Spark-1.6开始，Spark-Streaming引入一种新的状态管理机制mapWithState。<br>不过目前这个算子仍然被标记为：@Experimental。使用时需要注意。</p>
<h3 id="使用层面"><a href="#使用层面" class="headerlink" title="使用层面"></a>使用层面</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def mapWithState[StateType: ClassTag, MappedType: ClassTag](</span><br><span class="line">    spec: StateSpec[K, V, StateType, MappedType]</span><br><span class="line">  ): MapWithStateDStream[K, V, StateType, MappedType] = &#123;</span><br><span class="line">  new MapWithStateDStreamImpl[K, V, StateType, MappedType](</span><br><span class="line">    self,</span><br><span class="line">    spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>mapWithState接受参数：StateSpec[KeyType,valueType,StateType,MappedType]，其实就是写StateSpec.function</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>KeyType</td>
<td>Key的类型。</td>
</tr>
<tr>
<td>valueType</td>
<td>value。可以理解为新数据的类型。</td>
</tr>
<tr>
<td>stateType</td>
<td>state。可以理解为历史累计数据的类型。</td>
</tr>
<tr>
<td>mappedType</td>
<td>映射数据。可以理解为Dstream/RDD中的数据类型。只有这个数据可以被其他算子使用到。</td>
</tr>
</tbody></table>
</li>
<li><p>StateSpec：</p>
<table>
<thead>
<tr>
<th>算子</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>function(func)</td>
<td>具体如何更新状态。func接受参数：[KeyType , Option[valueType] , State[StateType]] 经过转换返回 [MappedType]。需要在func中通过State.get，State.update等方法来维护State。</td>
</tr>
<tr>
<td>initialState(RDD)</td>
<td>（可选）初始的RDD[(KeyType,StateType)]</td>
</tr>
<tr>
<td>timeout(Duration)</td>
<td>（可选）key的过期时间。过期后会将key删除（因为state一旦调用remove，将不能update）</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
<li><p>返回：MapWithStateDStream[KeyType,valueType,StateType,MappedType]</p>
</li>
<li><p>维护State：<br>mapWithState的历史信息是放在StateRDD中维护的，准确说是MapWithStateRDDRecord。<br>内部会将KV保存成对应的StateMap中</p>
</li>
</ol>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stream</span><br><span class="line">   .map(message =&gt; (message._2, 1))</span><br><span class="line">   .mapWithState(StateSpec.function((key: String, value: Option[Int], state: State[Long]) =&gt; &#123;</span><br><span class="line">       val sum = value.getOrElse(0).toLong + state.getOption.getOrElse(0L)</span><br><span class="line">       val output = (key, sum)</span><br><span class="line">	// 更新状态</span><br><span class="line">       state.update(sum)</span><br><span class="line">       println(s&quot;MapWithState: key=$key value=$value state=$state&quot;)</span><br><span class="line">       output</span><br><span class="line">   &#125;))</span><br><span class="line">    .foreachRDD(rdd =&gt; &#123;</span><br><span class="line">       rdd.foreach(pair =&gt; println(s&quot;MapWithState: key=$&#123;pair._1&#125; value=$&#123;pair._2&#125;&quot;))</span><br><span class="line">   &#125;)</span><br></pre></td></tr></table></figure>

<h3 id="实现逻辑："><a href="#实现逻辑：" class="headerlink" title="实现逻辑："></a>实现逻辑：</h3><ol>
<li><p>mapWithState接受更新函数mappingFunc，该函数会更新指定用户的状态，同时会返回更新后的状态。</p>
<p> Spark-Streaming通过根据我们定义的更新函数，在每个计算时间间隔内更新内部维护的状态，同时返回经过mappingFunc后的结果数据流。<br> <img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581838901682.png" alt="mapWithState"><br> <font color=blue>蓝色箭头</font>为实时过来的数据流<font color=blue>liveDStream</font>，通过liveDStream.mapWithState的调用，会得到一个<font color=green>MapWithStateDStream</font>（mapWithState算子返回值），为方框中上面<font color=green>浅绿色的箭头</font>。</p>
</li>
</ol>
<ol start="2">
<li><p>计算过程中，Spark-Streaming会遍历当前时间间隔内的数据rdd-x，在上一个时间间隔的状态state-(x-1)中查找指定的记录，并更新状态，更新操作就是我们前面定义的mappingFunc函数。</p>
<p> 这里的状态更新不再需要全量扫描状态数据了，状态数据是存在hashmap中，可以根据过来的数据快速定位到。<br> <img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581839214555.png" alt="mapWithState"><br> 首先通过partitionBy将新来的数据分区到对应的状态分区上，每个状态分区中的仅有一条记录，类型为MapWithStateRDDRecord，它打包了两份数据：stateMap保存当前分区内所有的状态、mappedData保存经过mappingFunc处理后的结果。</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">case class MapWithStateRDDRecord[K, S, E](var stateMap: StateMap[K, S], var mappedData: Seq[E])</span><br></pre></td></tr></table></figure>
<p> 默认输出的是mappedData这份（如果一个key在一个batch中存在多次，会一次输出多分mappedData）<br>如果需要输出全量状态，则可以在mapWithState后调用snapshot函数获取。</p>
</li>
</ol>
<h2 id="updateWithStateByKey-vs-mapWithState"><a href="#updateWithStateByKey-vs-mapWithState" class="headerlink" title="updateWithStateByKey vs mapWithState"></a>updateWithStateByKey vs mapWithState</h2><p>相同点：</p>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>都是pairDStream的算子。</td>
</tr>
<tr>
<td>都需要checkPoints</td>
</tr>
</tbody></table>
<p>不同点：最主要的差异就是这两个算子作用的key不一样，update需要作用于所有key，mapWith只需要作用于更新的Key</p>
<table>
<thead>
<tr>
<th></th>
<th>updateWithStateByKey</th>
<th>mapWithState</th>
</tr>
</thead>
<tbody><tr>
<td>保存State</td>
<td>靠内部专门的MapWithStateRDDRecord来维护</td>
<td>依靠本身的RDD来维护。</td>
</tr>
<tr>
<td>性能</td>
<td>只更新需要更新的key，官方称相较于updateStateByKey会有10倍提升</td>
<td>内部逻辑主要是先对两个Rdd进行cogroup，可能会涉及shuffle。</td>
</tr>
<tr>
<td>返回数据</td>
<td>key的state是独立保存的，每次调用函数只会作用于当前key，所以只会返回更新的数据部分。</td>
<td>返回全量数据。</td>
</tr>
<tr>
<td>操作数据</td>
<td>mapWithState是对单独的一条数据进行操作。当前值是一个Option</td>
<td>会对一个批次内的K进行操作，当前值是一个Seq。</td>
</tr>
</tbody></table>
<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>实际使用时，我们发现mapWithState与updateWithState存在两个问题：</p>
<ol>
<li>依赖于Checkpoint，而Checkpoint目前在修改代码后会失效。</li>
<li>也是由于状态维护在内存中，太耗内存了。<br>这也可能是mapWithState直到现在依旧是@Experimental的原因吧。</li>
</ol>
<h3 id="解决思路："><a href="#解决思路：" class="headerlink" title="解决思路："></a>解决思路：</h3><p>其实状态缓存，无非是要有一个介质去存状态，自然眼睛就来到了Redis。实际使用时，为了减少访问Redis的次数，最好在访问前对Redis进行聚合操作，并使用pipeline的方式批量访问。</p>
<p>实现参考：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val updateFunc = (</span><br><span class="line">      records: Seq[(Long, Set(Int))],</span><br><span class="line">      states: Seq[Response[String]],</span><br><span class="line">      pipeline: Pipeline) =&gt; &#123;</span><br><span class="line">  pipeline.sync()</span><br><span class="line">  var i = 0</span><br><span class="line">  while (i &lt; records.size) &#123;</span><br><span class="line">    val (userId, values) = records(i)</span><br><span class="line">    val oldValues: Set[Int] = parseFrom(states(i).get())</span><br><span class="line">    val newValues = values ++ oldValues</span><br><span class="line">    pipeline.setex(userId.toString, 3600, toString(newValues))</span><br><span class="line">    i += 1</span><br><span class="line">  &#125;</span><br><span class="line">  pipeline.sync() </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val Func = (iter: Iterator[(Long, Iterable[Int])]) =&gt; &#123;</span><br><span class="line">  val jedis = ConnectionPool.getConnection()</span><br><span class="line">  val pipeline = jedis.pipelined()</span><br><span class="line">  val records = ArrayBuffer.empty[(Long, Set(Int))]</span><br><span class="line">  val states = ArrayBuffer.empty[Response[String]]</span><br><span class="line">  while (iter.hasNext) &#123;</span><br><span class="line">    val (userId, values) = iter.next()</span><br><span class="line">    records += (userId, values.toSet)</span><br><span class="line">    states += pipeline.get(userId.toString)</span><br><span class="line">    if (records.size == batchSize) &#123;</span><br><span class="line">      updateFunc(records, states, pipeline)</span><br><span class="line">      records.clear()</span><br><span class="line">      states.clear()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  updateFunc(records, states, pipeline)</span><br><span class="line">  Iterator[Int]()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">inputDStream.groupByKey()</span><br><span class="line">  .mapPartitions(Func)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>参考文章：</p>
<ol>
<li><a href="http://sharkdtu.com/posts/spark-streaming-state.html">DStream updateStateByKey vs mapWithState</a></li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Scala反射机制动态生成UDF</title>
    <url>/2021/03/08/shi-yong-scala-fan-she-ji-zhi-dong-tai-sheng-cheng-udf/</url>
    <content><![CDATA[<blockquote>
<p>由于项目需要，需要通过String类型的代码，动态生成UDF函数。</p>
<p>由于需要同时适配Java代码和Scala代码目前考虑到两种实现：</p>
<p>Scala代码通过Scala的反射机制动态编译代码并作为UDF注册到Spark。</p>
<p>Java代码使用Spark的Expression机制。</p>
</blockquote>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>使用隐式转换为Spark-RDD添加简单的算子</title>
    <url>/2017/06/14/shi-yong-yin-shi-zhuan-huan-wei-spark-rdd-tian-jia-jian-dan-de-suan-zi/</url>
    <content><![CDATA[<h2 id="干啥用的"><a href="#干啥用的" class="headerlink" title="干啥用的"></a>干啥用的</h2><p>RDD已经提供了非常多的算子供使用，不过人有时候就是想偷懒，或者有些新想法。<br>比如说我想快速的输出rdd的前x个元素（就像DataFrame中的show一样）<br>利用scala的隐式转换，我们可以对RDD进行一些简单的扩展，将一些常用的操作进行封装，或者增加一些新的算子。<br>初学者文章，有错误望批评指正</p>
<h2 id="说下隐式转换"><a href="#说下隐式转换" class="headerlink" title="说下隐式转换"></a>说下隐式转换</h2><p>实现的基础是利用Scala的隐式转换。如果不了解可以访问官网简单了解下：<a href="https://docs.scala-lang.org/zh-cn/tour/implicit-conversions.html">隐式转换Doc</a>，网上也有大量的文章帮助了解。<br>本次主要使用隐式类，隐式类的一些注意事项（限制）在此一起记录下：</p>
<ol>
<li>构造参数有且只能有一个（参数实际上就是你要把 什么类 转换成该（隐式）类）</li>
<li>不能是顶层类（必须被定义在类，伴生对象和包对象里）</li>
<li>不能是case class（case class在定义会自动生成伴生对象与2矛盾）</li>
<li>作用域内不能有与之相同名称的标示符（就是不能同名）</li>
</ol>
<h2 id="实践才是检验真理"><a href="#实践才是检验真理" class="headerlink" title="实践才是检验真理"></a>实践才是检验真理</h2><p>需求：需求很简单，我想打印一个RDD的元素，并且可以自定义展示元素的个数。不用考虑太多其他的优化问题。<br>分析：利用隐式转换，我们需要将：Rdd[Int]转换成自定义的：SimpleRdd[Int]。使Rdd[Int]能够调用SimpleRdd的方法。<br>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">object RddImplicitAspect &#123;</span><br><span class="line">  implicit class SimpleRdd[T](rdd:RDD[T])&#123;</span><br><span class="line">    def printTopN(printLine:Int = 20) = &#123;</span><br><span class="line">      rdd.take(printLine).foreach(println)</span><br><span class="line">      println(s&quot;$&#123;rdd.getClass.getSimpleName&#125; Output Over&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>首先建立一个工具类，取名<code>RddImplicitAspect</code>。</li>
<li>弄一个<code>implicit class</code>，类名取为<code>SimpleRdd</code>（<strong>不要重名</strong>），接受的对象是**RDD[T]**。这样T类型的Rdd都能够使用这个方法吧。</li>
<li>实际上我们应该限制Rdd的类型，比如说<code>[T&lt;:AnyVal]</code>把作用范围缩小。</li>
<li>之后我们就可以对rdd进行操作了。</li>
</ol>
<p>使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">object TestApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd= sc.parallelize(1 to 30)</span><br><span class="line">    import com.test.spark.utils.RddImplicitAspect._</span><br><span class="line">    rdd.printTopN()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>不要忘记导入隐式转换。</strong></li>
<li>直接调用即可。</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>初试ClickHouse</title>
    <url>/2021/07/30/chu-shi-clickhouse/</url>
    <content><![CDATA[<p>ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统。能够拥有极高的写入吞吐量和查询效率，代价则是牺牲了一定的update及事务特性。ClickHouse适合当前的流式场景。</p>
<p>本文主要介绍了ClickHouse的数据类型，库表引擎等最基础的特性。</p>
<span id="more"></span>

<h1 id="ClickHouse中的数据类型"><a href="#ClickHouse中的数据类型" class="headerlink" title="ClickHouse中的数据类型"></a>ClickHouse中的数据类型</h1><blockquote>
<p>不同版本的ClickHouse的数据类型可能会有些许差别，可通过</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> system.data_type_families;</span><br></pre></td></tr></table></figure>

<p>查看。</p>
</blockquote>
<ul>
<li>ClickHouse支持处理除数为0的情况<ul>
<li><code>inf</code>正无穷，用于<code>1/0</code></li>
<li><code>-inf</code>负无穷，用于<code>-1/0</code></li>
<li><code>nan</code>非数字，用于<code>0/0</code></li>
</ul>
</li>
</ul>
<h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><h3 id="浮点"><a href="#浮点" class="headerlink" title="浮点"></a>浮点</h3><ul>
<li><p>Float的精度问题</p>
<ul>
<li><p>ClickHouse会对浮点数进行计算可能引起四舍五入的误差。</p>
<ul>
<li>```sql<br>select toFloat32(‘0.123456789’) as f32, toFloat64(‘0.123456789012345678’) as f64;– result<br>– f32,f64<br>– 0.12345679, 0.12345678901234568<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    - 可见float32只保留了8位小数。float64保留了17位小数，并都对最后一位进行了四舍五入。</span><br><span class="line"></span><br><span class="line">- Decimal</span><br><span class="line">  - 对于精度有要求的场景可使用Decimal(P,S)。</span><br><span class="line">    - P - 精度。有效范围：[1:38]，决定可以有多少个十进制数字（包括分数）。</span><br><span class="line">    - S - 规模。有效范围：[0：P]，决定数字的小数部分中包含的小数位数。</span><br><span class="line">  - 精度变化规则：</span><br><span class="line">    - 加减法：`S=max(S1,S2)`</span><br><span class="line">    - 乘法：`S=S1+S2`</span><br><span class="line">    - 除法：`S=S1(被除数)`。除数的S不能大于被除数的S。</span><br><span class="line"></span><br><span class="line">### Bool</span><br><span class="line"></span><br><span class="line">ClickHouse没有专门的Bool类型，可使用UInt8 类型，取值限制为 0 或 1。</span><br><span class="line">&gt; 其实用int替换bool也是其他数据常见的调优点。</span><br><span class="line"></span><br><span class="line">###字符串</span><br><span class="line"></span><br><span class="line">ClickHouse提供了2种字符串类型。`String`和`FixedString`。`UUID`也算一种字符串。</span><br><span class="line"></span><br><span class="line">- String</span><br><span class="line"></span><br><span class="line">  - 变长字符串。</span><br><span class="line">  - 它可以包含任意的字节集，包含空字节。</span><br><span class="line">  - ClickHouse 没有编码的概念。字符串可以是任意的字节集，按它们原本的方式进行存储和输出。</span><br><span class="line"></span><br><span class="line">- FixedString</span><br><span class="line"></span><br><span class="line">  - 定长字符串。</span><br><span class="line"></span><br><span class="line">  - 当数据的长度恰好为N个字节时，`FixedString`类型是高效的。 在其他情况下，这可能会降低效率。</span><br><span class="line"></span><br><span class="line">  - 可以有效存储在`FixedString`类型的列中的值的示例：</span><br><span class="line"></span><br><span class="line">    - 二进制表示的IP地址（IPv6使用`FixedString(16)`）</span><br><span class="line">    - 语言代码（ru_RU, en_US … ）</span><br><span class="line">    - 货币代码（USD, RUB … ）</span><br><span class="line">    - 二进制表示的哈希值（MD5使用`FixedString(16)`，SHA256使用`FixedString(32)`）</span><br><span class="line"></span><br><span class="line">  - 当向ClickHouse中插入数据时,</span><br><span class="line"></span><br><span class="line">    - 如果字符串包含的字节数*大于*`N`，将抛出`Too large value for FixedString(N)`异常。</span><br><span class="line"></span><br><span class="line">    - 如果字符串包含的字节数*少于*`N’，**将对字符串末尾进行空字节填充**。</span><br><span class="line"></span><br><span class="line">    - &gt; 当做数据查询时，ClickHouse不会删除字符串末尾的空字节。 如果使用`WHERE`子句，则须要手动添加空字节以匹配`FixedString`的值。</span><br><span class="line"></span><br><span class="line">- UUID</span><br><span class="line"></span><br><span class="line">  - 通用唯一标识符(UUID)是一个16字节的数字，用于标识记录。</span><br><span class="line">  - UUID数据类型只支持 [字符串](https://clickhouse.tech/docs/zh/sql-reference/data-types/string/) 数据类型也支持的函数(比如, [min](https://clickhouse.tech/docs/zh/sql-reference/aggregate-functions/reference/min/#agg_function-min), [max](https://clickhouse.tech/docs/zh/sql-reference/aggregate-functions/reference/max/#agg_function-max), 和 [count](https://clickhouse.tech/docs/zh/sql-reference/aggregate-functions/reference/count/#agg_function-count))。</span><br><span class="line">  - 插入数据时，ClickHouse提供了 [generateuidv4](https://clickhouse.tech/docs/zh/sql-reference/functions/uuid-functions/) 函数。否则UUID值将用零填充:`00000000-0000-0000-0000-000000000000`。</span><br><span class="line"></span><br><span class="line">###时间</span><br><span class="line"></span><br><span class="line">ClickHouse提供了`Date`、`DateTime`、`DateTime64`三类相关时间日期的格式</span><br><span class="line"></span><br><span class="line">- Date：只保留日期。与一般数据库类似</span><br><span class="line"></span><br><span class="line">  - 如果插入时包含时间部分，能够成功插入，不过会只保留日期部分。</span><br><span class="line"></span><br><span class="line">- DateTime：官方解释是精确到秒时间戳类型</span><br><span class="line"></span><br><span class="line">  - 插入时可以使用时间日期格式，如`yyyy-MM-dd HH:mm:ss`。也可以使用时间戳格式:</span><br><span class="line"></span><br><span class="line">    - ```sql</span><br><span class="line">      insert into dt_test values (&#x27;2021-01-02 00:00:00&#x27;,1627625952,&#x27;2021/01/01 00:00:00&#x27;);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>DateTime64：精确到亚秒级的时间类型</p>
<ul>
<li>精确到$10^{-precision} seconds$​</li>
<li>精度表现出来就是<code>HH:mm:ss.0000...</code></li>
</ul>
</li>
</ul>
<p>##复杂数据类型</p>
<p>###Array</p>
<ul>
<li>创建表时，为<code>Array(T)</code>类型。<ul>
<li>如果允许有null元素，则为<code>Array(Nullable(T))</code></li>
</ul>
</li>
<li>可以通过array函数<code>array(T)</code>或<code>[T]</code>的方式指定数组类型。</li>
<li>Array如果有nullable属性，则允许null元素。</li>
<li>Array中元素数据类型必须相同。</li>
<li>ClickHouse不建议多层Array。</li>
</ul>
<p>###Tuple</p>
<p>与Scala中的Tuple类似。</p>
<ul>
<li>创建表时，为<code>Tuple(T1,T2..)</code>。<ul>
<li>Tuple可以存放不同的数据类型，如上述的T1，T2。</li>
<li>如果允许有null元素，则为<code>Nullable(T1)</code>。即：<code>Tuple(Nullable(T1),T2..)</code>表示第一个为T1类型的元素允许为null。</li>
<li>上述T可以是Array等其他复杂数据类型</li>
</ul>
</li>
</ul>
<h3 id="Nested"><a href="#Nested" class="headerlink" title="Nested"></a>Nested</h3><p>结构类似于嵌套表。</p>
<ul>
<li><p>创建表时，为</p>
<ul>
<li><p>```sql<br>create table net_test(</p>
<pre><code>id UUID,
nt Nested
(
    nt_id UUID,
    user_name String,
    user_age Int8
)
</code></pre>
<p>)engine = Memory;</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 由于是嵌套表，则其实对于每个`Nested`的子字段，都是数组组成的。</span><br><span class="line"></span><br><span class="line">  - 插入数据：</span><br><span class="line"></span><br><span class="line">    - ```sql</span><br><span class="line">      insert into net_test (`nt.user_name`, nt.user_age) values ([&#x27;ss&#x27;,&#x27;ss1&#x27;],[12,13]);</span><br></pre></td></tr></table></figure></li>
<li><p>查询数据：</p>
<ul>
<li>```sql<br>SELECT <code>nt.user_name</code> FROM tutorial.net_test WHERE has(nt.user_name, ‘ss’);<br>– result<br>|————–|<br>| nt.user_name |<table>
<thead>
<tr>
<th>[‘ss’,’ss1’]</th>
</tr>
</thead>
</table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">- 所以可以简单地把嵌套数据结构当做是所有列都是相同长度的多列数组。</span><br><span class="line"></span><br><span class="line"># 建库建表语法及引擎</span><br><span class="line"></span><br><span class="line">## 库引擎</span><br><span class="line"></span><br><span class="line">### Lazy引擎</span><br><span class="line"></span><br><span class="line">用于存放访问间隔很长的表。仅适用于 *Log引擎表。</span><br><span class="line"></span><br><span class="line">在距最近一次访问间隔`expiration_time_in_seconds`时间段内，会将表保存在内存中。</span><br><span class="line"></span><br><span class="line">### Atomic引擎</span><br><span class="line"></span><br><span class="line">默认的库引擎。</span><br><span class="line"></span><br><span class="line">支持：</span><br><span class="line"></span><br><span class="line">- 非阻塞 DROP </span><br><span class="line"></span><br><span class="line">- RENAME TABLE 查询</span><br><span class="line">- 原子 EXCHANGE TABLES t1 AND t2 查询。</span><br><span class="line"></span><br><span class="line">###Mysql引擎</span><br><span class="line"></span><br><span class="line">MySQL引擎用于将远程的MySQL服务器中的表映射到ClickHouse中，并允许您对表进行`INSERT`和`SELECT`查询</span><br><span class="line"></span><br><span class="line">## 表引擎</span><br><span class="line"></span><br><span class="line">表引擎（即表的类型）决定了：</span><br><span class="line"></span><br><span class="line">- 数据的存储方式和位置，写到哪里以及从哪里读取数据</span><br><span class="line">- 支持哪些查询以及如何支持。</span><br><span class="line">- 并发数据访问。</span><br><span class="line">- 索引的使用（如果存在）。</span><br><span class="line">- 是否可以执行多线程请求。</span><br><span class="line">- 数据复制参数。</span><br><span class="line"></span><br><span class="line">### 日志系列</span><br><span class="line"></span><br><span class="line">用于需要写入许多小数据量，当您需要快速写入许多小表（最多约100万行）并在以后整体读取它们时，该类型的引擎是最有效的。</span><br><span class="line"></span><br><span class="line">日志系列具体的多种实现，不过有下述共同特性：</span><br><span class="line"></span><br><span class="line">- 数据存储在磁盘上。</span><br><span class="line"></span><br><span class="line">- 写入时将数据追加在文件末尾。</span><br><span class="line"></span><br><span class="line">- 不支持[突变](https://clickhouse.tech/docs/en/sql-reference/statements/alter/#alter-mutations)操作。</span><br><span class="line"></span><br><span class="line">  - 即update、delete、drop等**alter**操作</span><br><span class="line"></span><br><span class="line">  - &gt; ClickHouse把update、delete操作叫做mutation(突变)</span><br><span class="line"></span><br><span class="line">- 不支持索引。</span><br><span class="line"></span><br><span class="line">  - 这意味着 `SELECT` 在范围查询时效率不高。</span><br><span class="line"></span><br><span class="line">- 非原子地写入数据。</span><br><span class="line"></span><br><span class="line">  - 这意味着如果某些事情破坏了写操作，例如服务器的异常关闭，你将会得到一张包含了损坏数据的表。</span><br><span class="line"></span><br><span class="line">#### TinyLog</span><br><span class="line"></span><br><span class="line">最简单的引擎并且提供了最少的功能和最低的性能。具有以下特性：</span><br><span class="line"></span><br><span class="line">- 不支持并行读取和并发数据访问，</span><br><span class="line">- 每一列存储在不同的文件中。写入时，数据将附加到文件末尾。</span><br><span class="line">- 这种表引擎的典型用法是 write-once：首先只写入一次数据，然后根据需要多次读取。</span><br><span class="line">  - *这句话的意思并不是说只能写入一次*</span><br><span class="line">- 此引擎适用于相对较小的表（建议最多1,000,000行）</span><br><span class="line">- **不支持索引**。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### StripeLog</span><br><span class="line"></span><br><span class="line">##### 对于写数据</span><br><span class="line"></span><br><span class="line">`StripeLog` 引擎将所有列存储在一个文件中。对每一次 `Insert` 请求，ClickHouse 将数据块追加在表文件的末尾，逐列写入。</span><br><span class="line"></span><br><span class="line">ClickHouse 为每张表写入以下文件：</span><br><span class="line"></span><br><span class="line">- `data.bin` — 数据文件。</span><br><span class="line">- `index.mrk` — 带标记的文件。标记包含了已插入的每个数据块中每列的偏移量。</span><br><span class="line"></span><br><span class="line">`StripeLog` 引擎不支持 `ALTER UPDATE` 和 `ALTER DELETE` 操作。</span><br><span class="line"></span><br><span class="line">#####对于读数据</span><br><span class="line"></span><br><span class="line">带标记的文件使得 ClickHouse 可以并行的读取数据。这意味着 `SELECT` 请求返回行的顺序是不可预测的。使用 `ORDER BY` 子句对行进行排序。</span><br><span class="line"></span><br><span class="line">&gt; 我们向一张stripeLog里插入点数据，分两次进行数据读取：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 第一次：</span><br><span class="line">&gt;</span><br><span class="line">&gt; ![image-20210731120347358](https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210731120347.png)</span><br><span class="line">&gt;</span><br><span class="line">&gt; 第N次：</span><br><span class="line">&gt;</span><br><span class="line">&gt; ![image-20210731120618853](https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210731120618.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### Log</span><br><span class="line"></span><br><span class="line">可以看作是TinyLog和StripeLog的结合体：</span><br><span class="line"></span><br><span class="line">- 每列文件单独存放。</span><br><span class="line">- 包含偏移量</span><br><span class="line"></span><br><span class="line">这是的读取操作能够**并行读取**。另外Log还有以下特点：</span><br><span class="line"></span><br><span class="line">- 写入操作会阻塞读取和其他写入。</span><br><span class="line">- 不支持索引。</span><br><span class="line">- **如果写入表失败，该表将被破坏，读取数据也会返回错误**。</span><br><span class="line">  - 适用于临时数据、write-once、测试等目的的表。</span><br><span class="line"></span><br><span class="line">#### 总结</span><br><span class="line"></span><br><span class="line">- TinyLog：数据不分块，只能单线程读取。</span><br><span class="line">- StripeLog：数据分块，但所有数据存放在一起。</span><br><span class="line">- Log：数据分块，且按照列存放于不同文件。</span><br><span class="line"></span><br><span class="line">### 集成</span><br><span class="line"></span><br><span class="line">集成表引擎允许ClickHouse直接访问其他数据库/文件系统的数据，或从其他数据库直接拉取数据。以Mysql为例</span><br><span class="line"></span><br><span class="line">#### Mysql</span><br><span class="line"></span><br><span class="line">调用格式：</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">engine = MySQL(&#x27;host:port&#x27;, &#x27;database&#x27;, &#x27;table&#x27;, &#x27;user&#x27;, &#x27;password&#x27;[, replace_query, &#x27;on_duplicate_clause&#x27;]);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>其他参数详询官网：<a href="https://clickhouse.tech/docs/zh/engines/table-engines/integrations/mysql/">ClickHouse-Table-Engine-Mysql</a></p>
<blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_t(</span><br><span class="line">    id UInt8,</span><br><span class="line">    name String</span><br><span class="line">) engine <span class="operator">=</span> MySQL(<span class="string">&#x27;host:port&#x27;</span>, <span class="string">&#x27;database&#x27;</span>, <span class="string">&#x27;table&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;password&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>就能将一张表连接到Mysql去，即建立一张外表。这种情况下，数据仍然存放在Mysql中。</p>
<p>可以通过如下等方式将数据导入ClickHouse</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_copy engine <span class="operator">=</span> Log <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> click_t ;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_copy2 <span class="keyword">as</span> click_t engine <span class="operator">=</span> Log ;</span><br></pre></td></tr></table></figure>

<p>也可以直接在建表后通过insert，而不是建立外表。其实是使用<code>mysql</code>函数去连接Mysql表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> click_copy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mysql(<span class="string">&#x27;host:port&#x27;</span>, <span class="string">&#x27;database&#x27;</span>, <span class="string">&#x27;table&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;password&#x27;</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_copy2 <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mysql(<span class="string">&#x27;host:port&#x27;</span>, <span class="string">&#x27;database&#x27;</span>, <span class="string">&#x27;table&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;password&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>需要注意的是，通过直接通过<code>select from mysql(...)</code>函数创建的表，所有列都会加上可为空的描述符，所以如果要控制这些东西，需要现在ClickHouse中建好表再进行insert。</p>
</blockquote>
]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title>初识MaxWell</title>
    <url>/2019/06/20/chu-shi-maxwell/</url>
    <content><![CDATA[<h2 id="Maxwell"><a href="#Maxwell" class="headerlink" title="Maxwell"></a>Maxwell</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近需要实时采集MySQL的数据到HBase，找了下目前主要有MaxWell和Canal。介于Maxwell部署简单，且支持断点还原，最后选择了MaxWell。</p>
<h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>MaxWell通过采集MySQL的binlog日志，将详细格式化成Json发往Kafka。SparkStreaming只需要对Json数据进行解析，就能够拼接SQL语句，然后同步到HBase。</p>
<h3 id="初次使用"><a href="#初次使用" class="headerlink" title="初次使用"></a>初次使用</h3><p>参考：<a href="http://maxwells-daemon.io/quickstart/">Maxwell’s Daemon</a></p>
<h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>下载、解压MaxWell</p>
<ol>
<li><p>配置MySQL：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">server_id=1</span><br><span class="line">log-bin=master</span><br><span class="line">binlog_format=row</span><br></pre></td></tr></table></figure></li>
<li><p>创建Maxwell相关表、用户：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE USER &#x27;maxwell&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;XXXXXX&#x27;;</span><br><span class="line">mysql&gt; GRANT ALL ON maxwell.* TO &#x27;maxwell&#x27;@&#x27;%&#x27;;</span><br><span class="line">mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO &#x27;maxwell&#x27;@&#x27;%&#x27;;</span><br></pre></td></tr></table></figure></li>
<li><p>测试：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/maxwell --user=&#x27;maxwell&#x27; --password=&#x27;XXXXXX&#x27; --host=&#x27;127.0.0.1&#x27; --producer=stdout</span><br></pre></td></tr></table></figure></li>
<li><p>在Mysql中创建测试表：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table emp (</span><br><span class="line">    id numeric(4) primary key,</span><br><span class="line">    name varchar(10)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li>
<li><p>insert测试数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into emp (id,name) values (1,&#x27;ZhangSan&#x27;);</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="on-Kafka"><a href="#on-Kafka" class="headerlink" title="on Kafka"></a>on Kafka</h4><p>on Kafka其实也很简单，主要是注意下版本兼容问题。</p>
<ol>
<li><p>创建一个测试Topic：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --create --zookeeper 192.168.136.1:2181/kafka --replication-factor 1 --partitions 3 --topic maxwell</span><br><span class="line">Created topic &quot;maxwell&quot;.</span><br><span class="line">[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --list --zookeeper 192.168.136.1:2181/kafka</span><br><span class="line">__consumer_offsets</span><br><span class="line">maxwell</span><br></pre></td></tr></table></figure></li>
<li><p>启动一个Consumer接受信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper 192.168.136.1:2181/kafka --topic maxwell --from-beginning</span><br></pre></td></tr></table></figure></li>
<li><p>启动Maxwell</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/maxwell --user=&#x27;maxwell&#x27; --password=&#x27;XXXXXX&#x27; --host=&#x27;127.0.0.1&#x27; \</span><br><span class="line">   --producer=kafka --kafka.bootstrap.servers=192.168.136.1:9092 --kafka_topic=maxwell</span><br></pre></td></tr></table></figure></li>
<li><p>剩下同上。</p>
</li>
</ol>
<h3 id="Maxwell的过滤："><a href="#Maxwell的过滤：" class="headerlink" title="Maxwell的过滤："></a>Maxwell的过滤：</h3><p>参考：<a href="http://maxwells-daemon.io/filtering/">Basic Filters</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/maxwell --user=&#x27;maxwell&#x27; --password=&#x27;maxwell&#x27; \</span><br><span class="line">--host=&#x27;127.0.0.1&#x27; --filter = &#x27;exclude: foodb.*, include: foodb.tbl, include: foodb./table_\d+/ \</span><br><span class="line">--producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.136.1:9092 --kafka_topic=maxwell</span><br></pre></td></tr></table></figure>

<p>这个filter是白名单，这样就可以只检测部分表了。</p>
<h3 id="Maxwell的Bootstrap："><a href="#Maxwell的Bootstrap：" class="headerlink" title="Maxwell的Bootstrap："></a>Maxwell的Bootstrap：</h3><p>参考：<a href="http://maxwells-daemon.io/bootstrapping/">Using the maxwell-bootstrap utility</a></p>
<p>HBase平台刚刚搭建起来时，涉及部分数据的全量刷，这也是选择Maxwell的理由：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/maxwell-bootstrap --database fooDB --table barTable --where &quot;my_date &gt;= &#x27;2017-01-07 00:00:00&#x27;&quot;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>MaxWell</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建Harbor</title>
    <url>/2017/09/16/da-jian-harbor/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>请先安装Docker。本文主要针对离线部署。</p>
<h3 id="1）Https："><a href="#1）Https：" class="headerlink" title="1）Https："></a>1）Https：</h3><p>本步骤可参考：<a href="https://github.com/goharbor/harbor/blob/master/docs/configure_https.md">configure_https</a></p>
<ol>
<li><p>安装证书：（Getting Certificate Authority）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl genrsa -out ca.key 4096</span><br><span class="line">主要修改后三项：</span><br><span class="line">openssl req -x509 -new -nodes -sha512 -days 3650 \</span><br><span class="line">-subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com&quot; \</span><br><span class="line">-key ca.key \</span><br><span class="line">-out ca.crt</span><br></pre></td></tr></table></figure></li>
<li><p>获取服务器证书：（Getting Server Certificate）</p>
<ol>
<li><p>生成私钥：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl genrsa -out yourdomain.com.key  4096</span><br></pre></td></tr></table></figure></li>
<li><p>生成签名请求：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl req -sha512 -new \</span><br><span class="line">-subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com&quot; \</span><br><span class="line">-key yourdomain.com.key \</span><br><span class="line">-out yourdomain.com.csr</span><br></pre></td></tr></table></figure></li>
<li><p>生成注册表主机证书：（v3.ext）（DNS按需写）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; v3.ext &lt;&lt;-EOF</span><br><span class="line">authorityKeyIdentifier=keyid,issuer</span><br><span class="line">basicConstraints=CA:FALSE</span><br><span class="line">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment</span><br><span class="line">extendedKeyUsage = serverAuth</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1=yourdomain.com</span><br><span class="line">DNS.2=yourdomain</span><br><span class="line">DNS.3=hostname</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl x509 -req -sha512 -days 3650 \</span><br><span class="line">-extfile v3.ext \</span><br><span class="line">-CA ca.crt -CAkey ca.key -CAcreateserial \</span><br><span class="line">-in yourdomain.com.csr \</span><br><span class="line">-out yourdomain.com.crt</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>配置安装证书：（Configuration and Installation）</p>
<ol>
<li><p>为Harbor配置证书（文件夹不存在则创建）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp yourdomain.com.crt /data/cert/</span><br><span class="line">cp yourdomain.com.key /data/cert/</span><br></pre></td></tr></table></figure></li>
<li><p>为Docker配置：（文件夹不存在则创建）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl x509 -inform PEM -in yourdomain.com.crt -out yourdomain.com.cert</span><br><span class="line">---</span><br><span class="line">cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/</span><br><span class="line">cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/</span><br><span class="line">cp ca.crt /etc/docker/certs.d/yourdomain.com/</span><br></pre></td></tr></table></figure></li>
<li><p>配置damon.jason：（使用ip登陆必须）</p>
<ol>
<li>位置：/etc/docker/daemon.json</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;:[&quot;https://hadoop001&quot;],</span><br><span class="line">&quot;insercure-registries&quot;:[&quot;ip&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>​    </p>
</li>
</ol>
<h3 id="2）配置harbor-yml"><a href="#2）配置harbor-yml" class="headerlink" title="2）配置harbor.yml"></a>2）配置harbor.yml</h3><p>主要配置：</p>
<ul>
<li>hostname：hostname或ip</li>
<li>certificate：/data/cert/<strong>yourdomain.com</strong>.crt</li>
<li>private_key：/data/cert/<strong>yourdomain.com</strong>.key</li>
</ul>
<h3 id="3）-安装Docker-compose"><a href="#3）-安装Docker-compose" class="headerlink" title="3） 安装Docker compose"></a>3） 安装Docker compose</h3><p>参考：<a href="https://docs.docker.com/compose/install/">安装Docker compose</a></p>
<h3 id="4）-安装Harbor"><a href="#4）-安装Harbor" class="headerlink" title="4） 安装Harbor"></a>4） 安装Harbor</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./install.sh</span><br></pre></td></tr></table></figure>

<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>对Harbor修改配置时，需要重启Docker及Harbor，建议如下操作：</p>
<ul>
<li><p><strong>docker-compose需要与harbor.yml同一目录下执行</strong>：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>sudo docker-compose down -v</td>
<td>停止所有容器，并删除容器。并删除挂载卷和volunme的链接（-v）</td>
</tr>
<tr>
<td>vim harbor.yml</td>
<td>编辑配置</td>
</tr>
<tr>
<td>sudo ./prepare</td>
<td>执行一个准备脚本。</td>
</tr>
<tr>
<td>sudo docker-compose up -d</td>
<td>在后台（-d）构建，（重新）创建，启动，链接一个服务相关的容器。</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="生命周期管理"><a href="#生命周期管理" class="headerlink" title="生命周期管理"></a>生命周期管理</h3><ol>
<li><p>停止Harbor（会保留数据）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker-compose down -v</span><br></pre></td></tr></table></figure></li>
<li><p>删除数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm -r /data/database</span><br><span class="line">rm -r /data/registry</span><br></pre></td></tr></table></figure></li>
<li><p>后台启动Harbor：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker-compose up -d</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>本地spark+idea访问云主机上的hive</title>
    <url>/2018/03/31/ben-di-spark-idea-fang-wen-yun-zhu-ji-shang-de-hive/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录下本人使用本地spark+idea访问云主机遇到过的一些小问题。<br>仅供参考。欢迎讨论。</p>
<blockquote>
<p>spark版本：2.1.1 </p>
</blockquote>
<h2 id="使用sparkSQL-idea访问云主机HIVE"><a href="#使用sparkSQL-idea访问云主机HIVE" class="headerlink" title="使用sparkSQL+idea访问云主机HIVE"></a>使用sparkSQL+idea访问云主机HIVE</h2><h3 id="准备hive-site-xml"><a href="#准备hive-site-xml" class="headerlink" title="准备hive-site.xml"></a>准备hive-site.xml</h3><p>官网是这么描述的：</p>
<blockquote>
<p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.</p>
</blockquote>
<p>实际上，一般只需要准备hive-site.xml这个文件即可，因为spark访问Hive主要是通过访问hive的原数据库来完成的：<br><img src="https://img-blog.csdnimg.cn/20191014173213417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Ntb2tlcml1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以最重要的就是让spark程序能够访问到原数据库。本例是使用mysql。<br>贴一下hive-site.xml供参考。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hadoop换成云主机ip就好了，不过mysql需要注意开启远程访问权限 --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--客户端显示当前查询表的头信息 --&gt;</span> </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--客户端显示当前数据库名称信息 --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><p>主要是三个依赖：sparksql、mysql、spark-hive。<br>请根据需要进行更改。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="获取sparkSession"><a href="#获取sparkSession" class="headerlink" title="获取sparkSession"></a>获取sparkSession</h3><p>新版的spark，通过sparkSession来连接hive，不过有几个参数需要注意，先贴代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val spark = SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;hiveApp&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .config(<span class="string">&quot;dfs.client.use.datanode.hostname&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>需要<code>enableHiveSupport</code>来对hive进行支持。<br>访问云主机，需要<code>.config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</code>，因为hadoop返回对ip是内网地址，通过这个设置让hadoop返回主机名，这样我们配置下hosts就能够访问了。</p>
<h3 id="读取hive表"><a href="#读取hive表" class="headerlink" title="读取hive表"></a>读取hive表</h3><p>拿到正确的sparksession，就能对hive进行各种操作了。<br>这里贴一下完整代码方便查阅。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object hiveApp &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;hiveApp&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .config(<span class="string">&quot;dfs.client.use.datanode.hostname&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val df = spark.table(<span class="string">&quot;spark.person&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>初识Kubernetes</title>
    <url>/2021/05/29/chu-shi-kubernetes/</url>
    <content><![CDATA[<h2 id="何为容器"><a href="#何为容器" class="headerlink" title="何为容器"></a>何为容器</h2><ul>
<li><p>容器允许开发者在同一台物理机上运行多个服务。且将他们相互隔离。</p>
</li>
<li><p>容器中运行的进程，实际就是运行在物理机上。</p>
</li>
<li><p>相比虚拟机，减少资源消耗。</p>
</li>
<li><p>同一台物理机上的容器，调用的是同一个内核。</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210529160509.png" alt="image-20210529160459962"></p>
</blockquote>
</li>
</ul>
<h3 id="容器如何隔离"><a href="#容器如何隔离" class="headerlink" title="容器如何隔离"></a>容器如何隔离</h3><p>主要可以利用两个机制：</p>
<ul>
<li>Linux的命名空间：<ul>
<li>命名空间被用来隔离一组特定的资源，如进程能够使用哪些网络接口等。</li>
</ul>
</li>
<li>Linux的控制组（cgroups）：<ul>
<li>控制组能够限制进程使用的资源情况（CPU、内存、网络带宽等）</li>
</ul>
</li>
</ul>
<h2 id="关于Docker"><a href="#关于Docker" class="headerlink" title="关于Docker"></a>关于Docker</h2><ul>
<li>Docker是容器的一个具体实现项目/平台。</li>
<li>Docker主要由三个概念组成：<ul>
<li>镜像：Docker镜像里包含了开发者打包的应用程序及其所依赖的环境。<ul>
<li>Docker的镜像与虚拟机镜像有一个很大不同，容器镜像是由多层构成的。<ul>
<li>如果镜像B、C都依赖镜像A，则镜像A只会被存储一份。</li>
</ul>
</li>
<li>容器镜像层是只读的。</li>
</ul>
</li>
<li>镜像仓库：用于存放Docker镜像。</li>
<li>容器：Docker容器通常是一个Linux容器，基于Docker镜像被创建。<ul>
<li>一个运行中的容器是一个运行在Docker主机上的进程，但它和主机、以及所有其他进程都是隔离的。</li>
<li>容器运行时，一个新的可写层会在镜像层之上被创建。相当于进程写的都是拷贝。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Docker运行容器背后的原理"><a href="#Docker运行容器背后的原理" class="headerlink" title="Docker运行容器背后的原理"></a>Docker运行容器背后的原理</h3><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210529164800.png" alt="image-20210529164800241"></p>
<h2 id="关于Kubernetes"><a href="#关于Kubernetes" class="headerlink" title="关于Kubernetes"></a>关于Kubernetes</h2><h3 id="Kubernetes的核心功能"><a href="#Kubernetes的核心功能" class="headerlink" title="Kubernetes的核心功能"></a>Kubernetes的核心功能</h3><ul>
<li>整个系统由一个主节点和若干工作节点组成。</li>
<li>组件被部署在哪个节点对于开发者和系统管理员来说都不用关心。</li>
</ul>
<h3 id="Kubernetes架构"><a href="#Kubernetes架构" class="headerlink" title="Kubernetes架构"></a>Kubernetes架构</h3><p>硬件级别上，一个Kubernetes集群由很多节点组成：</p>
<ul>
<li><p>主节点：它承载着K8s控制和管理整个集群系统的<strong>控制面板</strong></p>
</li>
<li><p>工作节点：它们运行用户实际部署的应用</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210529163842.png" alt="image-20210529163842048"></p>
</blockquote>
</li>
</ul>
<h4 id="控制面板"><a href="#控制面板" class="headerlink" title="控制面板"></a>控制面板</h4><p>它包含多个组件。这些组件可运行在单个主节点上，或通过副本部署在多个主节点上（高可用）。</p>
<ul>
<li>Kubernetes API服务器：开发者客户端和其他控制面板组件都要和它通信。</li>
<li>Scheculer：用于调度应用（为应用的每个可部署组件分配一个工作节点）</li>
<li>ControllerManager：执行集群级别的功能，如复制组件、持续跟踪工作节点、处理失败节点等。</li>
<li>etcd：分布式数据存储。用于持久化集群的配置。</li>
</ul>
<p>控制面板持有并控制集群状态。</p>
<h4 id="工作节点"><a href="#工作节点" class="headerlink" title="工作节点"></a>工作节点</h4><p>工作节点是运行容器化应用的机器。</p>
<ul>
<li>运行的容器，如Docker</li>
<li>Kubelet：它与API服务器通信。管理它所在节点的容器。</li>
<li>Kube-proxy：负责组件之间的负载均衡网络流量</li>
</ul>
<h3 id="POD"><a href="#POD" class="headerlink" title="POD"></a>POD</h3><p>POD是k8s的概念。一个pod是一组紧密相关的容器。它们总是一起运行在同一个工作节点上。每个pod就像一个独立的逻辑机器，拥有自己的IP、主机名、进程等。</p>
<ul>
<li><p>一个pod可以运行多个容器。</p>
</li>
<li><p>一个pod的所有容器都运行在同一个<strong>逻辑</strong>机器上。</p>
</li>
<li><p>其他pod中的容器，即使运行在一个工作节点上，也会出现在不同的节点上。</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/podpng" alt="image-20210529165205700"></p>
</blockquote>
</li>
</ul>
<h3 id="运行容器镜像"><a href="#运行容器镜像" class="headerlink" title="运行容器镜像"></a>运行容器镜像</h3><p>通过kubectl run可以运行docker镜像，其背后流程如下：</p>
<ol>
<li><p>kubectl向API服务器发送HTTP请求。创建一个ReplicationController对象。</p>
</li>
<li><p>RC创建一个新的POD，调度器将其调度到一个工作节点上。</p>
</li>
<li><p>kubelet看到pod已经被调度到指定节点上后，通知docker从镜像中心拉去指定的镜像。并创建运行容器。</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/k8s%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.png" alt="image-20210529165547735"></p>
</blockquote>
</li>
</ol>
<h3 id="外部访问"><a href="#外部访问" class="headerlink" title="外部访问"></a>外部访问</h3><p>由于每个POD都有自己的IP地址，所以如果要从集群外部访问，需要通过服务对象公开他。创建一个特殊的LoadBalancer类型的<strong>服务</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl expose rc kubia --type=LoadBalancer --name kubia-http</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里是对名为kubia的replicationcontroller(rc)，创建一个loadbalancer服务，名字叫kubia-http</span></span><br></pre></td></tr></table></figure>

<p>不同于pod，示例中创建的kubia-http属于K8s中的服务（sercvice），通过<code>kubectl get services</code>能够看到。</p>
<h3 id="ReplicationController与POD"><a href="#ReplicationController与POD" class="headerlink" title="ReplicationController与POD"></a>ReplicationController与POD</h3><ul>
<li>用户创建的是ReplicationController。</li>
<li>POD是ReplicationController创建并管理。</li>
<li>ReplicationController可以创建并管理多个POD副本。<ul>
<li>负责重启消失的POD</li>
</ul>
</li>
</ul>
<h3 id="为什么需要服务"><a href="#为什么需要服务" class="headerlink" title="为什么需要服务"></a>为什么需要服务</h3><ul>
<li><p>POD的存在是短暂的。消失的POD会被RC替换为新的POD。</p>
</li>
<li><p>通过服务，能够保证外部访问并不会因为POD的更换而变化。</p>
</li>
<li><p>通过服务，能够将请求平均分配到多个POD副本上，实现负载均衡。</p>
</li>
<li><blockquote>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/pod%E4%B8%8E%E6%9C%8D%E5%8A%A1%E4%B8%8Erc.png" alt="image-20210529172109644"></p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>聊聊spark-submit的常用参数</title>
    <url>/2020/02/24/liao-liao-submit-de-chang-yong-can-shu/</url>
    <content><![CDATA[<p>在使用spark-submit在yarn跑应用程序时，我们除了设置参数，一般还会附加jar包，file文件等。这些东西实际去哪了？<br>答案就是我们配置的<code>yarn.nodemanager.local-dirs</code>tmp目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/data/tmp/nm-local-dir&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="这些文件会放到这里？"><a href="#这些文件会放到这里？" class="headerlink" title="这些文件会放到这里？"></a>这些文件会放到这里？</h3><p>其实这个问题只需要简单实验下，我们可以写一个sleep很长的程序，也可以直接使用spark提供的示例程序。<br>我们现在本地单节点尝试下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit --master yarn --class org.apache.spark.examples.SparkPi --files scopt-2.11-3.7.0.jar --jars ~/lib/okio-1.17.2.jar spark-examples_2.11-2.4.1.jar 10000</span><br></pre></td></tr></table></figure>
<p>在<code>yarn.nodemanager.local-dirs</code>目录下，可以看到生成了一个以application开头的目录，一路进去进到container，可以看到我们的文件已经放在了里面。<br>// TODO 图。</p>
<h3 id="集群情况？"><a href="#集群情况？" class="headerlink" title="集群情况？"></a>集群情况？</h3>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>通过BaseRelation自定义SparkSQL数据源</title>
    <url>/2018/08/05/tong-guo-baserelation-zi-ding-yi-sparksql-shu-ju-yuan/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用<code>org.apache.spark.sql.sources.interfaces</code>中的接口可以实现自定义SparkSql数据源。不过对此在官网和相关书籍中没有找到相关信息，使用网上资料实际使用时其实遇到很多坑，特此记录下。<br>本身是用Spark中的源码JdbcRelationProvider作为参考。这部分解析就不写了。<br>本文章主要记录整体流程和使用过程中遇到的问题。</p>
<blockquote>
<p>Spark版本：spark2.1.1</p>
</blockquote>
<h1 id="DefaultSource类"><a href="#DefaultSource类" class="headerlink" title="DefaultSource类"></a>DefaultSource类</h1><p>实现自定义数据源，第一步是新建一个名为<code>DefaultSource</code>的类，并需要实现一些Trait，其中<code>RelationProvider</code>是必须实现的。</p>
<h2 id="实现RelationProvider"><a href="#实现RelationProvider" class="headerlink" title="实现RelationProvider"></a>实现RelationProvider</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span></span>&#123;...&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1）DefaultSource必须一字不差。除非我们在META-INFO中注册。<br>2）必须实现RelationProvider接口。<br>3）其他常用接口：SchemaRelationProvider、DataSourceRegister。</p>
</blockquote>
<h2 id="实现SchemaRelationProvider（Optional）"><a href="#实现SchemaRelationProvider（Optional）" class="headerlink" title="实现SchemaRelationProvider（Optional）"></a>实现SchemaRelationProvider（Optional）</h2><p>实际使用的时候，为了方便，我们还会实现SchemaRelationProvider。<br>所以常见构造其实是这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span> <span class="title">with</span> <span class="title">SchemaRelationProvider</span></span>&#123;</span><br><span class="line">	<span class="comment">// RelationProvider </span></span><br><span class="line">	<span class="function">override def <span class="title">createRelation</span><span class="params">(sqlContext:SQLContext,parameters:Map[String,String])</span>:BaseRelation</span>=&#123;...&#125;</span><br><span class="line">	<span class="comment">// SchemaRelationProvider</span></span><br><span class="line">	<span class="function">override def <span class="title">createRelation</span><span class="params">(sqlContext:SQLContext,parameters:Map[String,String],schema:StructType)</span>:BaseRelation</span>=&#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>两个Provider会提供同名的方法，不过通过SchemaRelationProvider我们能够轻松的拿到用户想要传入的Schema信息。parameters则是用户在<code>.option</code>传入的一些映射数据。</p>
<h2 id="实现DataSourceRegister（Optional）"><a href="#实现DataSourceRegister（Optional）" class="headerlink" title="实现DataSourceRegister（Optional）"></a>实现DataSourceRegister（Optional）</h2><p>如果没有实现DataSourceRegister，我们使用时就必须要写完整包名，并且<strong>类名只能为DefaultSource</strong>。该Trait只有一个方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">override def <span class="title">shortName</span><span class="params">()</span>:String </span>= <span class="string">&quot;yourName&quot;</span></span><br></pre></td></tr></table></figure>
<p>系统自带的数据源，比如说jdbc，csv等，都是通过这个方法定义的。不过，实现这个方法还不能被系统识别，我们需要在META-INFO中进行注册，具体位置：<code>spark/sql/core/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code>：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223839.png" alt="包位置"></p>
<h2 id="附：source的名字在哪检测？"><a href="#附：source的名字在哪检测？" class="headerlink" title="附：source的名字在哪检测？"></a>附：source的名字在哪检测？</h2><p>debug源码，可以很清楚的看到整个逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/** Given a provider name, look up the data source class definition. */</span></span><br><span class="line"> <span class="function">def <span class="title">lookupDataSource</span><span class="params">(provider: String, conf: SQLConf)</span>: Class[_] </span>= &#123;</span><br><span class="line">   val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) match &#123;...&#125;</span><br><span class="line">   <span class="comment">// DefaultSource的由来</span></span><br><span class="line">   val provider2 = s<span class="string">&quot;$provider1.DefaultSource&quot;</span></span><br><span class="line">   val loader = Utils.getContextOrSparkClassLoader</span><br><span class="line">   val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这边会进行查找，先回找别称，没找到就会去找xxx.DefaultSource</span></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match &#123;</span><br><span class="line">       <span class="comment">// the provider format did not match any given registered aliases</span></span><br><span class="line">       <span class="keyword">case</span> Nil =&gt;</span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">           Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match &#123;</span><br></pre></td></tr></table></figure>
<p>provider就是我们<code>.format(&quot;xxx&quot;)</code>传入的xxx<br>系统首先会先去找别称，没找到再去补充找全称。</p>
<h1 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h1><p>观察DefaultSource，可以发现其需要返回BaseRelation，这是一个抽象类，所以我们第二步就是实现它。这个类倒是没有什么命名要求。</p>
<h2 id="继承BaseRelation"><a href="#继承BaseRelation" class="headerlink" title="继承BaseRelation"></a>继承BaseRelation</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ETLRelation</span>(...) <span class="keyword">extends</span> <span class="title">BaseRelation</span></span>&#123;</span><br><span class="line">	override def schema:StructType=&#123;...&#125;</span><br><span class="line">	override def sqlContext:SQLContext=&#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>继承该类，需要实现两个方法，即拿到schema和sqlContext。这个时候观察DefaultSource，schema和sqlContext其实可以传进来。所以这部分其实常常为这个样子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ETLRelation</span>(<span class="title">userSchema</span>:<span class="title">StructType</span>)(<span class="title">override</span> <span class="title">val</span> <span class="title">sqlContext</span>:<span class="title">SQLContext</span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span></span>&#123;</span><br><span class="line">	override def schema:StructType=&#123;</span><br><span class="line">		<span class="keyword">if</span>(userSchema==<span class="keyword">null</span>)&#123;</span><br><span class="line">			userSchema</span><br><span class="line">		&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="comment">//TODO.. StructType(StructField()::Nil)</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这边会给schema设置一个默认值，当然也可以不设，仿照sqlContext的写法就好了。为了方便使用，我一般会将override的参数与一般参数分开放。构造体中我们可以传入其他需要的参数。<br>Relation的框已经搭起来了，下一步就是实现读取功能了。</p>
<h2 id="TableScan"><a href="#TableScan" class="headerlink" title="TableScan"></a>TableScan</h2><p>Spark提供了几种Trait供使用：<br>| trait              | 用途                                                         |<br>| :—————– | :———————————————————– |<br>| TableScan          | 将所有列进行读入                                             |<br>| PrunedScan         | 不需要的列不会从外部数据源加载。                             |<br>| PrunedFilteredScan | 在PrunedScan的基础上，并且加入Filter，在加载数据也的时候就进行过滤 |<br>| CatalystScan       | Catalyst的支持传入expressions来进行Scan。支持列裁剪和Filter。 |<br>三个Trait均有一个同名方法：buildScan</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">trait TableScan &#123;</span><br><span class="line">  <span class="function">def <span class="title">buildScan</span><span class="params">()</span>: RDD[Row]</span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function">trait PrunedScan </span>&#123;</span><br><span class="line">  <span class="function">def <span class="title">buildScan</span><span class="params">(requiredColumns: Array[String])</span>: RDD[Row]</span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function">trait PrunedFilteredScan </span>&#123;</span><br><span class="line">  <span class="function">def <span class="title">buildScan</span><span class="params">(requiredColumns: Array[String], filters: Array[Filter])</span>: RDD[Row]</span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function">trait CatalystScan </span>&#123;</span><br><span class="line">  <span class="function">def <span class="title">buildScan</span><span class="params">(requiredColumns: Seq[Attribute], filters: Seq[Expression])</span>: RDD[Row]</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<p>requiredColumns与filters两个参数是在我们操作DataFrame时传入的，如使用filter算子时。以PrunedFilteredScan 为例，实现这个Trait的好处时Spark能够将我们的filter和select实现下压操作。这样我们就无需将所有数据都读进来供后面的运算了。</p>
<blockquote>
<p>不实现PrunedFilteredScan我们仍然能够使用filter、select等算子，只是不支持下压操作。</p>
</blockquote>
<p>本文就以实现TableScan 为例继续：（假设我们需要读取一个文件）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myRelation</span>(<span class="title">schema</span>:<span class="title">StructType</span>,<span class="title">parameters</span>:<span class="title">Map</span>[<span class="title">String</span>,<span class="title">String</span>])(<span class="title">override</span> <span class="title">val</span> <span class="title">sqlContext</span>:<span class="title">SQLContext</span>)</span></span><br><span class="line"><span class="class">	<span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="title">with</span> <span class="title">TableScan</span> <span class="title">with</span> <span class="title">Loggin</span></span>&#123;</span><br><span class="line">	<span class="function">override def <span class="title">builScan</span><span class="params">()</span>:RDD[Row]</span>=&#123;</span><br><span class="line">		val triedGetPath = Try(parameters.get(<span class="string">&quot;path&quot;</span>))</span><br><span class="line">		val path = triedGetPath match &#123;</span><br><span class="line">			<span class="function"><span class="keyword">case</span> <span class="title">Failure</span><span class="params">(exception)</span> </span>=&gt; logError(<span class="string">&quot;Path must set&quot;</span>);sys.exit(<span class="number">1</span>)<span class="comment">//TODO..</span></span><br><span class="line">			<span class="function"><span class="keyword">case</span> <span class="title">Success</span><span class="params">(value)</span> </span>=&gt; value.get</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// rdd是Tuple类型，(文件地址,文件内容)。</span></span><br><span class="line">		val rdd = sqlContext.sparkContext.wholeTextFiles(path)</span><br><span class="line">		<span class="comment">// 我们只需要文件内容，先分行，再拆分出单词。</span></span><br><span class="line">		val res = rdd.map(_._2)</span><br><span class="line">			.flatMap(_.split(<span class="string">&quot;\n&quot;</span>))</span><br><span class="line">			.map(_.split(<span class="string">&quot;\t&quot;</span>).toSeq)</span><br><span class="line">		<span class="comment">// 构成Row类型，并返回。注意要和Schema对应上。</span></span><br><span class="line">		res.map(Row.fromSeq(_))</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为节省页面，上面代码只有builScan这个函数。<br>其实我们就是将rdd的处理进行了一层封装。返回SparkSql能够识别的数据。 所以其他Rdd的操作在这里都是允许的，我们需要按照需求选取即可</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val session = SparkSession.builder().....</span><br><span class="line">val df = session.read.format(<span class="string">&quot;com.bigdata.csdn&quot;</span>).option(<span class="string">&quot;path&quot;</span>,<span class="string">&quot;xxx&quot;</span>).load</span><br><span class="line">df.show</span><br><span class="line">session.stop()</span><br></pre></td></tr></table></figure>
<p>format里需要写DefaultSource所在的包名。系统会自动加上DefaultSource。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>通过Curator API操作zookeeper的学习与简单使用介绍</title>
    <url>/2019/10/27/tong-guo-curator-api-cao-zuo-zookeeper-de-xue-xi-yu-jian-dan-shi-yong-jie-shao/</url>
    <content><![CDATA[<p>主要记录些使用Curator的学习记录，方便随时查阅。</p>
<h2 id="Curator"><a href="#Curator" class="headerlink" title="Curator"></a>Curator</h2><blockquote>
<p>Curator框架在zookeeper原生API接口上进行了包装，解决了很多ZooKeeper客户端非常底层的细节开发。提供ZooKeeper各种应用场景(recipe， 比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装，实现了Fluent风格的API接口,是最好用，最流行的zookeeper的客户端。包括以下特性：</p>
<ol>
<li>自动化的连接管理。</li>
<li>简化了原生的ZooKeeper的方法，事件等</li>
<li>Curator初始化之后会一直对zk连接进行监听，一旦发现连接状态发生变化将会作出相应的处理。</li>
<li>内部实现了诸如Session超时重连，Watcher反复注册等功能，</li>
</ol>
<p>更多相关信息请前往：<a href="http://curator.apache.org/index.html">http://curator.apache.org</a></p>
</blockquote>
<h2 id="使用概述："><a href="#使用概述：" class="headerlink" title="使用概述："></a>使用概述：</h2><p>curator是Fluent风格API，创建会话方式与原生的API创建方式区别很大。<br>Curator创建客户端为CuratorFramework，是由CuratorFrameworkFactory工厂类来实现的，注：CuratorFramework是线程安全的，要连接的每个ZooKeeper集群只需要一个 CuratorFramework对象就可以了。</p>
<h3 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-recipes<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;curator.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>一般添加这个就够用了，这个依赖背后依赖了client和framework。<br>client是zookeeper client的封装，提供有用的客户端特性。<br>framework是api的高层封装，添加了连接管理，重试机制等。<br>本次使用版本：<br>curator：4.0.1<br>zookeeper：3.4.6（3.4版本需要排除zk的依赖包。）</p>
<h3 id="获取client"><a href="#获取client" class="headerlink" title="获取client"></a>获取client</h3><p>总体而言，通过CuratorFrameworkFactory来获取Client对象。一种则是使用.builder()。另一种使用构造方法直接new，其实内部走的一条线：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 参数：</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> connectString       list of servers to connect to（zk地址）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> retryPolicy         retry policy to use（重试策略）</span></span><br><span class="line"><span class="comment"> * 可选参数：</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sessionTimeoutMs    session timeout（会话超时时间）默认60000</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> connectionTimeoutMs connection timeout（连接超时时间）默认15000</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace           每个curatorFramework可以设置独立命名空间，之后操作基于该命名空间。</span></span><br><span class="line"><span class="comment"> *                            比如操作/test/abc =&gt;实际是操作：/zk1/test/abc.注意使用时前面不要加/号</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">cf2 = CuratorFrameworkFactory.builder().connectString(<span class="string">&quot;hadoop:2181&quot;</span>)</span><br><span class="line">        .sessionTimeoutMs(<span class="number">60000</span>)</span><br><span class="line">        .connectionTimeoutMs(<span class="number">15000</span>)</span><br><span class="line">        .retryPolicy(retryPolicy)</span><br><span class="line">        .namespace(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//也可以采用常规的newClient的形式，不过无法支持命名空间。</span></span><br><span class="line"><span class="comment">//其实内部还是使用的builder，给出其中一个构造方法。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CuratorFramework <span class="title">newClient</span><span class="params">(String connectString, <span class="keyword">int</span> sessionTimeoutMs, <span class="keyword">int</span> connectionTimeoutMs, RetryPolicy retryPolicy)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> builder().</span><br><span class="line">        connectString(connectString).</span><br><span class="line">        sessionTimeoutMs(sessionTimeoutMs).</span><br><span class="line">        connectionTimeoutMs(connectionTimeoutMs).</span><br><span class="line">        retryPolicy(retryPolicy).</span><br><span class="line">        build();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码中有使用到一个实例，<code>retryPolicy</code>，即重试策略。<br>Curator提供了多种重试策略，后边会进行一个简单介绍，选择合适的即可。<br>不过目前为止，我们已经拿到了client。这时候需要我们调用<code>start()</code>来使用他。程序结束时我们也应该要调用<code>close()</code>来关闭他。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cf2.start()</span><br><span class="line">cf2.close()</span><br></pre></td></tr></table></figure>

<h4 id="重试策略简介"><a href="#重试策略简介" class="headerlink" title="重试策略简介"></a>重试策略简介</h4><p>会给出构造方法即部分介绍，一般看看名字和参数就知道有啥区别了。<br>他们都是继承自SleepingRetry<br>常用的有：<br>ExponentialBackoffRetry<br>ExponentialBackoffRetry被BoundedExponetialBackoffRetry继承，参数作用都差不多，不做赘述。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> baseSleepTimeMs initial amount of time to wait between retries（每次重试会增加重试时间baseSleepTimeMs）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxRetries max number of times to retry（最大重试次数）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSleepMs max time in ms to sleep on each retry（最大重试时间）</span></span><br><span class="line"><span class="comment"> *                   如果超过，将使用warn级别log，并使用最大重试时间</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExponentialBackoffRetry</span><span class="params">(<span class="keyword">int</span> baseSleepTimeMs, <span class="keyword">int</span> maxRetries, <span class="keyword">int</span> maxSleepMs)</span></span></span><br></pre></td></tr></table></figure>

<p>RetryForever（一直重试）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> retryIntervalMs	重试间隔时间</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">RetryForever</span><span class="params">(<span class="keyword">int</span> retryIntervalMs)</span></span></span><br></pre></td></tr></table></figure>
<p>RetryNTimes（重试N次）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> retryIntervalMs	重试间隔时间</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> n					重试N次</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">RetryNTimes</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> sleepMsBetweenRetries)</span></span></span><br></pre></td></tr></table></figure>
<p>剩下还有RetryOneTime（重试一次）、RetryUntilElaspsed（一直重试，知道超过指定时间）</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="常用的API"><a href="#常用的API" class="headerlink" title="常用的API"></a>常用的API</h3><p>这边大家直接看代码注释。因为使用的是zookeeper3.4。部分3.5的特性不支持。</p>
<h4 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h4><p>可以允许递归创建，即父节点不存在时可以帮你创建父节点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建节点。</span></span><br><span class="line"><span class="comment"> * 常见节点类型：</span></span><br><span class="line"><span class="comment"> *      永久：PERSISTENT</span></span><br><span class="line"><span class="comment"> *      临时（当前连接有效）：EPHEMERAL</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    cf2.create().creatingParentsIfNeeded()<span class="comment">//允许递归创建</span></span><br><span class="line">            .withMode(CreateMode.PERSISTENT)<span class="comment">//持久绩点</span></span><br><span class="line">            .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE)</span><br><span class="line">            .forPath(<span class="string">&quot;/test2&quot;</span>,<span class="string">&quot;data&quot;</span>.getBytes());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="获取节点数据："><a href="#获取节点数据：" class="headerlink" title="获取节点数据："></a>获取节点数据：</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取节点信息</span></span><br><span class="line"><span class="comment"> * data会作为byte数组返回。</span></span><br><span class="line"><span class="comment"> * 节点信息则会放入stat中。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getInfo</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Stat stat = <span class="keyword">new</span> Stat();</span><br><span class="line">    <span class="keyword">byte</span>[] bytes = cf2.getData()</span><br><span class="line">            .storingStatIn(stat)<span class="comment">//将节点信息传入stat中（和zk自带的stat一样）</span></span><br><span class="line">            .forPath(<span class="string">&quot;/a&quot;</span>);</span><br><span class="line">    System.out.println(<span class="keyword">new</span> String(bytes));</span><br><span class="line">    System.out.println(stat.getNumChildren());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取节点信息，并使用watcher</span></span><br><span class="line"><span class="comment"> * 默认只能监听本节点的修改、删除</span></span><br><span class="line"><span class="comment"> * 获取children可以用getChildren。</span></span><br><span class="line"><span class="comment"> * 初次运行节点不存在会报错。watcher只能监听一次。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getInfoWithWatcher</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Stat stat = <span class="keyword">new</span> Stat();</span><br><span class="line">    <span class="keyword">byte</span>[] bytes = cf2.getData().storingStatIn(stat)</span><br><span class="line">            .usingWatcher(<span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">switch</span> (watchedEvent.getType())&#123;</span><br><span class="line">                        <span class="keyword">case</span> NodeDataChanged:</span><br><span class="line">                            System.out.println(<span class="string">&quot;change!&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> NodeDeleted:</span><br><span class="line">                            System.out.println(<span class="string">&quot;node delete!&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> NodeChildrenChanged:</span><br><span class="line">                            System.out.println(<span class="string">&quot;child!&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> NodeCreated:</span><br><span class="line">                            System.out.println(<span class="string">&quot;Create!&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).forPath(<span class="string">&quot;/test&quot;</span>);</span><br><span class="line">    System.out.println(stat.getVersion());</span><br><span class="line">    System.out.println(<span class="keyword">new</span> String(bytes));</span><br><span class="line">    Thread.sleep(<span class="number">60</span>*<span class="number">1000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="设置-修改节点数据"><a href="#设置-修改节点数据" class="headerlink" title="设置/修改节点数据"></a>设置/修改节点数据</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 设置节点</span></span><br><span class="line"><span class="comment"> * 不指定会使用defaultData。默认为ip</span></span><br><span class="line"><span class="comment"> * 返回设置后的stat</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNode</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Stat stat = cf2.setData().withVersion(-<span class="number">1</span>).forPath(<span class="string">&quot;/test&quot;</span>);</span><br><span class="line">    System.out.println(stat.getVersion());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 删除节点</span></span><br><span class="line"><span class="comment"> * deletingChildrenIfNeeded：子节点存在则删除子节点，再删除父节点。</span></span><br><span class="line"><span class="comment"> * guaranteed：如果删除失败，后端会一直尝试删除知道成功。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteNode</span><span class="params">()</span><span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Void aVoid = cf2.delete()</span><br><span class="line">            .guaranteed()<span class="comment">//可省略，保证删除节点（删除失败后段会一直删除知道成功）</span></span><br><span class="line">            .deletingChildrenIfNeeded()<span class="comment">//可省略，开启递归删除。</span></span><br><span class="line">            .forPath(<span class="string">&quot;/test&quot;</span>);</span><br><span class="line">    System.out.println(aVoid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h3 id="Cache（watcher-N次）"><a href="#Cache（watcher-N次）" class="headerlink" title="Cache（watcher N次）"></a>Cache（watcher N次）</h3><p>原生的zk，有个比较蛋疼的地方就是watcher只能使用一次，如果需要重复使用，常见的可以采用生产消费者模式来重复触发注册，不过还是比较繁琐的。<br>幸运的是，Curator自带了一套名为Cache的东西，能够帮助我们完成类似功能：<br>注：上面有使用过usingWatcher的方法，不过也是一次性的。</p>
<p>Cache一般有三种：NodeCache、PathChildrenCache、TreeCache。<br>NodeCache是监视当前节点。<br>PathChildrenCache则是监视该节点的子节点。（不能递归监视孙节点哦）<br>TreeCache则是前两者的结合，即监视当前节点+当前节点的子节点。<br>他们使用上其实是差不多的，所有这边就只介绍一种，需要使用另外一种时，稍微看看源码就会用了。</p>
<h4 id="创建Cache"><a href="#创建Cache" class="headerlink" title="创建Cache"></a>创建Cache</h4><p>PathChildrenCache有6种构造方法，比另外两个加起来还多，这边介绍下常用的参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> client           传入client</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> path             监视的path</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cacheData        是否缓存data，写false，你会得不到节点的数据。只能拿到stat。</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> dataIsCompressed 是否压缩</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">PathChildrenCache pathChildrenCache = <span class="keyword">new</span> PathChildrenCache(cf2,<span class="string">&quot;/test&quot;</span>,<span class="keyword">true</span>);</span><br><span class="line">pathChildrenCache.start();</span><br></pre></td></tr></table></figure>
<h4 id="StartMode"><a href="#StartMode" class="headerlink" title="StartMode"></a>StartMode</h4><p>PathChildrencache有三种启动模式：(另外两种Cache则没这么丰富)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* The cache will be primed (in the backgrouinitial values.</span></span><br><span class="line"><span class="comment">* Events for existing and new nodes will be posted.</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* 默认的启动模式。异步初始化</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">StartMode.NORMAL</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* The cache will be primed (in the foregrouinitial values.</span></span><br><span class="line"><span class="comment">* &#123;<span class="doctag">@link</span> PathChildrenCache#rebuild()&#125; will be called before</span></span><br><span class="line"><span class="comment">* the &#123;<span class="doctag">@link</span> PathChildrenCache#start(Stamethod returns</span></span><br><span class="line"><span class="comment">* in order to get an initial view of the node.</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* 同步初始化Cache。创建后就从服务器拉数据。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">StartMode.BUILD_INITIAL_CACHE</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* After cache is primed with initial values (in the background) a</span></span><br><span class="line"><span class="comment">* &#123;<span class="doctag">@link</span> PathChildrenCacheEvent.Type#INITIALIZED&#125; will be posted.</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* 异步初始化，Cache准备好后，会触发INITIALIZED类型的事件。Listerner会收到该通知，</span></span><br><span class="line"><span class="comment">* 通过pathChildrenCacheEvent.getType()就能拿到。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">StartMode.POST_INITIALIZED_EVENT</span><br></pre></td></tr></table></figure>
<p>其实就是一个异步/同步初始化、是否有通知的区别。</p>
<h4 id="Listerner"><a href="#Listerner" class="headerlink" title="Listerner"></a>Listerner</h4><p>通过给Listerner设置监听器，我们就能够对设置的节点进行监听工作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设置为true才能够达到节点的data。</span></span><br><span class="line">PathChildrenCache pathChildrenCache = <span class="keyword">new</span> PathChildrenCache(cf2,<span class="string">&quot;/test&quot;</span>,<span class="keyword">true</span>);</span><br><span class="line">pathChildrenCache.start();</span><br><span class="line"></span><br><span class="line">pathChildrenCache.getListenable().addListener((curatorFramework, pathChildrenCacheEvent) -&gt; &#123;</span><br><span class="line">    String nodeName = ZKPaths.getNodeFromPath(pathChildrenCacheEvent.getData().getPath());</span><br><span class="line">    String path2 = pathChildrenCacheEvent.getData().getPath();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (pathChildrenCacheEvent.getType())&#123;</span><br><span class="line">        <span class="keyword">case</span> CHILD_ADDED:</span><br><span class="line">            System.out.println(nodeName+<span class="string">&quot; :add: &quot;</span>+path2);<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> CHILD_REMOVED:</span><br><span class="line">            System.out.println(<span class="string">&quot;remove&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> CHILD_UPDATED:</span><br><span class="line">            System.out.println(<span class="string">&quot;changed&quot;</span>);</span><br><span class="line">            System.out.println(<span class="keyword">new</span> String(pathChildrenCacheEvent.getData().getData()));</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>我们能够获取到节点对type，并作出对应对处理。</p>
]]></content>
      <categories>
        <category>Utils</category>
      </categories>
      <tags>
        <tag>Utils</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink核心抽象概念</title>
    <url>/2021/04/12/flink-he-xin-chou-xiang-gai-nian/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Flink采用流水线的形式，通过Transformation的概念记录上游的数据来源，将代码逻辑组织成流水线。</p>
<h1 id="环境对象"><a href="#环境对象" class="headerlink" title="环境对象"></a>环境对象</h1><ul>
<li><code>StreamExecutionEnvironment</code>是Flink开发时的入口，表示流应用的执行环境。</li>
<li><code>Environment</code>是运行时作业级别的概念。<ul>
<li><code>Environment</code>从<code>StreamExecutionEnvironment</code>的配置信息衍生而来。作为运行时刻的上下文。</li>
</ul>
</li>
<li><code>RuntimeContext</code>是运行时Task实例级别的概念。</li>
</ul>
<blockquote>
<p>对于开发而言：</p>
<ul>
<li><code>StreamExecutionEnvironment</code>在main方法中使用。</li>
<li><code>RuntimeContext</code>在UDF开发中使用。</li>
<li>Environment则起到衔接StreamExecutionEnvironment和RuntimeContext的作用。</li>
</ul>
</blockquote>
<h2 id="运行时环境"><a href="#运行时环境" class="headerlink" title="运行时环境"></a>运行时环境</h2><p>即<code>Environment</code>。</p>
<p>定义了运行时Task所需的所有配置信息，包括静态配置和调度器调度后生成的动态配置信息。主要由两个实现类：<code>RuntimeEnvironment</code>和<code>SavepointEnvironment</code>。后者主要是<code>state-processor-api</code>使用</p>
<blockquote>
<p>A minimally implemented Environment that provides the functionality required to run the state-processor-api.</p>
</blockquote>
<h3 id="RuntimeEnvironment"><a href="#RuntimeEnvironment" class="headerlink" title="RuntimeEnvironment"></a>RuntimeEnvironment</h3><p>在Task开始执行时进行初始化。将Task运行相关的信息都封装到该对象中，包括：<code>jobId</code>,<code>executionId</code>等。</p>
<h2 id="运行时上下文"><a href="#运行时上下文" class="headerlink" title="运行时上下文"></a>运行时上下文</h2><p>即<code>RuntimeContext</code></p>
<p>是<code>Function</code>运行时的上下文，封装了<code>Function</code>运行时可能需要的所有信息，使得<code>Function</code>能够获取到作业级别的信息，如作业并行度等。在<code>RichFunction</code>中，可以通过<code>getRuntimeContext</code>得到。不同的地方会得到不同的RuntimeContext</p>
<ul>
<li>StreamingRuntimeContext：流计算的UDF</li>
<li>DistributedRuntimeUDFContext：由运行时UDF所在的批处理算子创建。</li>
<li>RuntimeUDFContext：批处理UDF中使用</li>
<li>SavepointRuntimeContext：支持读取变更写入<code>checkpoint</code>和<code>savepoint</code>。在<code>state-processor-api</code>使用。</li>
<li>CepRuntimeContext：CEP中使用</li>
</ul>
<h1 id="数据流元素"><a href="#数据流元素" class="headerlink" title="数据流元素"></a>数据流元素</h1><p>数据流元素在Flink中被称为StreamElement。由四类组成：</p>
<ul>
<li>StreamRecord。数据记录。<ul>
<li>即数据流中的一条记录。包含：<ul>
<li>数据本身。</li>
<li>数据的时间戳（可选）</li>
</ul>
</li>
</ul>
</li>
<li>LatencyMarker。延迟标记。<ul>
<li>用来近似评估延迟。</li>
<li>在<code>Source</code>中创建，并向下游发送。</li>
<li>在<code>Sink</code>节点用来估计整个DAG图中花费的时间，用来评估整体的处理延迟。</li>
<li>包含：<ul>
<li>周期性在数据源中创造出来的时间戳。</li>
<li>算子编号。</li>
<li>数据源算子所在的Task编号。</li>
</ul>
</li>
</ul>
</li>
<li>Watermark。水位线<ul>
<li>是一个时间戳。</li>
<li>用来告诉算子所有时间早于水位线的数据已经到达。</li>
</ul>
</li>
<li>StreamStatus。流状态标记。<ul>
<li>用来通知Task是否会继续接收到上游的记录或者水位线。</li>
<li>在<code>Source</code>中生成，并沿着Dataflow向下游传播。</li>
<li>包含两种状态<ul>
<li>IDLE（空闲）</li>
<li>ACTIVE（活动）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h1><p><code>Transformation</code>分为两类：物理和虚拟。在运行时，DataStream的API调用都会被转换为<code>Transformation</code>。然后从<code>Transformation</code>转换成实际运行的算子。而虚拟的<code>Transformation</code>则不会转换为具体的算子。</p>
<p>所有的物理<code>Transformation</code>继承<code>PhysicalTransformation</code>。其他的都是虚拟<code>Transformation</code>。<code>Transformation</code>包含了Flink运行时的一些关键参数：</p>
<ul>
<li>name：转换器的名字。一般是用户指定。</li>
<li>uid：用户指定的uid。用于从savePoint重启时恢复状态。</li>
<li>bufferTimeout：buffer超时时间。</li>
<li>parallelism：并行度。</li>
<li>id：自动生成的唯一id。</li>
<li>outputType：输出类型。用于进行序列化数据。（<code>TypeInformation&lt;T&gt;类型</code>）</li>
<li>slotSharingGroup：给当前的Transformation设置Slot共享组。</li>
</ul>
<h2 id="物理Transformation"><a href="#物理Transformation" class="headerlink" title="物理Transformation"></a>物理Transformation</h2><ul>
<li>SourceTransformation<ul>
<li>Flink作业的起点。读取数据。</li>
</ul>
</li>
<li>SinkTransformation<ul>
<li>Flink作业的终点。写出数据。</li>
</ul>
</li>
<li>OneInputTransformation<ul>
<li>只接收一个输入流。需要input和operator参数。</li>
</ul>
</li>
<li>TwoInputTransformation<ul>
<li>接收两种流作为输入。</li>
</ul>
</li>
</ul>
<h2 id="虚拟Transformation"><a href="#虚拟Transformation" class="headerlink" title="虚拟Transformation"></a>虚拟Transformation</h2><ul>
<li>SideOutputTransformation</li>
<li>SplitTransformation</li>
<li>SelectTransformation</li>
<li>PartitionTransformation</li>
<li>UnionTransformation</li>
<li>FeedbackTransformation<ul>
<li>表示FlinkDAG中的一个反馈点。把符合条件的数据重新发回上游Transformation中处理。</li>
<li>一个反馈点可以连接一个或者多个上游的Transformation。</li>
<li>包含两个重要参数：<ul>
<li>input：上有输入StreamTransformation。</li>
<li>waitTime：默认为0，即永远等待。如果超出等待时间，则计算结束不再接收数据</li>
</ul>
</li>
<li>FeedbackTransformation要与待加入的StreamTransformation实例的并行度一致。</li>
</ul>
</li>
<li>CoFeedbackTransformation</li>
</ul>
<h1 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h1><p>即<code>StreamOperator</code></p>
<p>Flink运行时由Task组成一个Dataflow，<strong>每个Task中包含了一个或多个算子</strong>，一个算子就是一个计算步骤。使用算子需要实现对应的函数<code>Function</code></p>
<h2 id="算子行为"><a href="#算子行为" class="headerlink" title="算子行为"></a>算子行为</h2><p>所有算子都包含了生命周期管理、状态与容错管理、数据处理三个方面。</p>
<h3 id="生命周期管理"><a href="#生命周期管理" class="headerlink" title="生命周期管理"></a>生命周期管理</h3><ul>
<li>setup：初始化环境、时间服务、注册监控等。</li>
<li>open：由具体算子负责实现。包含初始化逻辑。</li>
<li>close：数据处理完后关闭算子。确保所有缓存数据向下游发送。</li>
<li>dispose：进行资源释放。</li>
</ul>
<h3 id="状态与容错管理"><a href="#状态与容错管理" class="headerlink" title="状态与容错管理"></a>状态与容错管理</h3><p>提供状态存储。在出发checkpoint时，保存状态快照。当作业失败时从保存的快照中恢复状态。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>按照单流输入和双流输入定义了不同的行为接口：</p>
<ul>
<li>OneInputStreamOperator<ul>
<li><code>void processElement(StreamRecord&lt;IN&gt; element)</code></li>
<li><code>void processWatermark(Watermark mark)</code></li>
<li><code>void processLatencyMaker(LatencyMarker latencyMarker)</code></li>
</ul>
</li>
<li>TowInputStreamOperator。<ul>
<li>相当于单输入所有方法翻倍</li>
<li><code>void processElement1(StreamRecord&lt;IN&gt; element)</code></li>
<li><code>void processElement2(StreamRecord&lt;IN&gt; element)</code></li>
<li><code>void processWatermark1(Watermark mark)</code></li>
<li><code>void processWatermark2(Watermark mark)</code></li>
<li><code>void processLatencyMaker1(LatencyMarker latencyMarker)</code></li>
<li><code>void processLatencyMaker2(LatencyMarker latencyMarker)</code></li>
</ul>
</li>
<li>算子融合优化策略<ul>
<li>OperatorChain的策略。具体规则参考JobGraph优化。</li>
</ul>
</li>
</ul>
<h2 id="Flink算子"><a href="#Flink算子" class="headerlink" title="Flink算子"></a>Flink算子</h2><blockquote>
<p>Flink算子在设计时主要分为两条线</p>
</blockquote>
<ol>
<li>数据处理。主要是<code>OneInputStreamOperator</code>和<code>TwoInputStreamOperator</code>接口。</li>
<li>生命周期。主要是<code>AbstractStreamOperator抽象类及其子实现类</code>。</li>
</ol>
<h2 id="Blink算子"><a href="#Blink算子" class="headerlink" title="Blink算子"></a>Blink算子</h2><blockquote>
<p>目前BlinkRuntime中重新实现了很多与SQL相关的算子，但仍然使用了Flink-Streaming-java中定义的Transformation来包装这些算子</p>
</blockquote>
<blockquote>
<p>Blink Runtime中的算子并不是完全批流通用。支持批的算子都实现了BoundedOneInput接口或者BoundedMultiInput接口。通过<code>endInput</code>方法来标识数据是否有界。</p>
</blockquote>
<p>目前用到的算子可以分为3类</p>
<ul>
<li>Blink Runtime内置。</li>
<li>通过模块内置的算子，如CEP算子（CepOperator）、ProcessOperator等。</li>
<li>通过动态代码生成的算子。</li>
</ul>
<h2 id="异步算子"><a href="#异步算子" class="headerlink" title="异步算子"></a>异步算子</h2><blockquote>
<p>Flink在异步算子中提供了两种输出模式：顺序输出和无序输出模式。</p>
</blockquote>
<ul>
<li>顺序输出模式<ul>
<li>先收到的元素先输出。后续元素的异步函数调用无论是否先完成，都等待。</li>
<li>保证消息不乱序。</li>
<li>可能增加延迟、降低算子的吞吐量。</li>
<li><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210721195127.png" alt="image-20210721195122585"><ul>
<li>R1是第一个到达的元素。必定比R2先输出。</li>
<li>这种模式的异步主要体现在处理R1，R2是异步的，并不会等待R1处理完才去处理R2。</li>
</ul>
</li>
</ul>
</li>
<li>无序输出模式<ul>
<li><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210721195440.png" alt="image-20210721195440859"></li>
<li>R2的结果可能早于R1进行输出。</li>
<li>无序输出并不是完全无序，<strong>仍然要保持Watermark不能超越前面元素的原则</strong>。即等待队列中将按照Watermark进行分组。组内无序。组与组之间有序。<ul>
<li>对于途中，需要按照<code>[R1,R2,R3]、W1、[R4]</code>的顺序输出。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="函数体系"><a href="#函数体系" class="headerlink" title="函数体系"></a>函数体系</h1><blockquote>
<p>函数是算子具体功能/计算逻辑的实现</p>
</blockquote>
<p>一般包含三类Function：</p>
<ul>
<li>SourceFunction<ul>
<li>直接从外部存储读取数据。</li>
</ul>
</li>
<li>SinkFunction<ul>
<li>将数据写入外部存储。</li>
</ul>
</li>
<li>一般Function<ul>
<li>中间的处理逻辑。</li>
<li>包含单流输入和双流输入两种。</li>
</ul>
</li>
</ul>
<h2 id="函数层次"><a href="#函数层次" class="headerlink" title="函数层次"></a>函数层次</h2><p>UDF在<code>DataStream API</code>层使用。从高阶到低阶Function如下图所示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20210721200310.png" alt="image-20210721200310714"></p>
<p>其中，RichFunction相较于无状态Function，有两个方面的增强：</p>
<ul>
<li>增加了<code>open</code>和<code>close</code>方法来管理Function的生命周期。<ul>
<li>Function启动时执停止时行open方法。停止时执行close方法</li>
</ul>
</li>
<li>增加了<code>getRuntimeContext</code>和<code>setRuntimeContext</code><ul>
<li>可以拿到执行时作业级别的参数信息。</li>
</ul>
</li>
</ul>
<h2 id="处理函数"><a href="#处理函数" class="headerlink" title="处理函数"></a>处理函数</h2><p>ProcessFunction可以访问流应用程序所有基本构建块：</p>
<ul>
<li>事件。即流元素。</li>
<li>状态。容错和一致性。</li>
<li>定时器。事件时间和处理时间。</li>
</ul>
<h2 id="广播函数"><a href="#广播函数" class="headerlink" title="广播函数"></a>广播函数</h2><p>需要继承<code>BroadcastProcessFunction</code>或<code>KeyedBroadcastProcessFunction</code>。相比于一般的<code>ProcessFunction</code>，主要多了<code>processBroadcastElement</code>方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(<span class="keyword">final</span> IN2 value, <span class="keyword">final</span> Context ctx, <span class="keyword">final</span> Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure>

<p>通过Context就能够得到广播出去的数据。<code>IN2 value</code>则表示数据记录。所以对于<code>BroadcastProcessFunction</code>来说，其实数据会被<code>processBroadcastElement</code>和<code>processElement</code>各处理一次。</p>
<h2 id="异步函数"><a href="#异步函数" class="headerlink" title="异步函数"></a>异步函数</h2><p>异步函数是对Java异步框架的封装。包括了<code>RichAsyncFunction</code>和<code>AsyncFunction</code>。异步函数接口定义了两种行为：</p>
<ul>
<li>将调用结果封装到ResultFurtrue中。<ul>
<li><code>void asyncInvoke(IN input, ResultFuture&lt;OUT&gt; resultFuture)</code></li>
</ul>
</li>
<li>提供调用超时的处理，防止不释放资源。<ul>
<li><code>default void timeout(IN input, ResultFuture&lt;OUT&gt; resultFuture)</code></li>
</ul>
</li>
</ul>
<p>###官方的使用用例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这个例子使用 Java 8 的 Future 接口（与 Flink 的 Future 相同）实现了异步请求和回调。</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实现 &#x27;AsyncFunction&#x27; 用于发送请求和设置回调。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AsyncDatabaseRequest</span> <span class="keyword">extends</span> <span class="title">RichAsyncFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 能够利用回调函数并发发送请求的数据库客户端 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> DatabaseClient client;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        client = <span class="keyword">new</span> DatabaseClient(host, post, credentials);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(String key, <span class="keyword">final</span> ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送异步请求，接收 future 结果</span></span><br><span class="line">        <span class="keyword">final</span> Future&lt;String&gt; result = client.query(key);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置客户端完成请求后要执行的回调函数</span></span><br><span class="line">        <span class="comment">// 回调函数只是简单地把结果发给 future</span></span><br><span class="line">        CompletableFuture.supplyAsync(<span class="keyword">new</span> Supplier&lt;String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="keyword">return</span> result.get();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">                    <span class="comment">// 显示地处理异常。</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).thenAccept( (String dbResult) -&gt; &#123;</span><br><span class="line">            resultFuture.complete(Collections.singleton(<span class="keyword">new</span> Tuple2&lt;&gt;(key, dbResult)));</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建初始 DataStream</span></span><br><span class="line">DataStream&lt;String&gt; stream = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 应用异步 I/O 转换操作</span></span><br><span class="line"><span class="comment">// 注：AsyncDataStream是一个位于org.apache.flink.streaming.api.datastream包下的帮助类，用于实现异步调用。并非之前产生的某个流</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream =</span><br><span class="line">    AsyncDataStream.unorderedWait(stream, <span class="keyword">new</span> AsyncDatabaseRequest(), <span class="number">1000</span>, TimeUnit.MILLISECONDS, <span class="number">100</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注：</p>
<ul>
<li>第一次调用 <code>ResultFuture.complete</code> 后 <code>ResultFuture</code> 就完成了。 后续的 <code>complete</code> 调用都将被忽略。</li>
</ul>
<h3 id="结果的顺序"><a href="#结果的顺序" class="headerlink" title="结果的顺序"></a>结果的顺序</h3><p>对应于异步算子中提到的两种输出模式</p>
<ul>
<li><p>顺序输出模式：</p>
<ul>
<li>异步请求一结束就立刻发出结果记录。 流中记录的顺序在经过异步 I/O 算子之后发生了改变。 </li>
<li>此模式使用 <code>AsyncDataStream.unorderedWait(...)</code> 方法</li>
</ul>
</li>
<li><p>无序输出模式：</p>
<ul>
<li>保持了流的顺序。发出结果记录的顺序与触发异步请求的顺序（记录输入算子的顺序）相同。</li>
<li>此模式使用 <code>AsyncDataStream.orderedWait(...)</code> 方法。</li>
</ul>
</li>
</ul>
<p>###使用提示</p>
<p>在实现使用 <em>Executor</em>（或者 Scala 中的 <em>ExecutionContext</em>）和回调的 <em>Futures</em> 时，建议使用 <code>DirectExecutor</code>，因为通常回调的工作量很小，<code>DirectExecutor</code> 避免了额外的线程切换开销。回调通常只是把结果发送给 <code>ResultFuture</code>，也就是把它添加进输出缓冲。从这里开始，包括发送记录和与 chenkpoint 交互在内的繁重逻辑都将在专有的线程池中进行处理。</p>
<p><code>DirectExecutor</code> 可以通过 <code>org.apache.flink.runtime.concurrent.Executors.directExecutor()</code> 或 <code>com.google.common.util.concurrent.MoreExecutors.directExecutor()</code> 获得。</p>
<p><strong>Flink 不以多线程方式调用 AsyncFunction</strong></p>
<p><strong>目前，出于一致性的原因，AsyncFunction 的算子（异步等待算子）必须位于算子链的头部</strong></p>
<h2 id="数据源函数"><a href="#数据源函数" class="headerlink" title="数据源函数"></a>数据源函数</h2><blockquote>
<p><code>SourceFunction</code>，一般实现<code>RichSourceFunction</code>或<code>RichParallelSourceFunction</code>，后者可以增加读取时的并行度</p>
</blockquote>
<blockquote>
<p><code>RichParallelSourceFunction</code>本身并没有增加其他方法，如果要实现数据源的并行读取，一般需要通过<code>getRuntimeContext().getIndexOfThisSubtask();</code>来获取subTask的index值，通过index值来判断每个Task的读取范围，实现不同Task读取不同范围的数据，实现并行读取。</p>
</blockquote>
<p>SourceFunction有几个比较关键的行为：</p>
<ul>
<li>生命周期管理<ul>
<li>主要通过open、close、cancle三个方法控制初始化、清理。</li>
</ul>
</li>
<li>读取数据<ul>
<li>通过run方法，从数据源读取数据。</li>
</ul>
</li>
<li>向下游发送数据<ul>
<li>将读取的数据传递给<code>SourceContext&lt;OUT&gt; ctx</code></li>
</ul>
</li>
<li>发送Watermark<ul>
<li>生成Watermark并向下游发送，发送逻辑定义在<code>SourceContext&lt;OUT&gt; ctx</code>中</li>
</ul>
</li>
<li>空闲标记<ul>
<li>如果读取不到数据，则将该Task标记为空闲，阻止Watermark向下游传递。发送逻辑定义在<code>SourceContext&lt;OUT&gt; ctx</code>中。</li>
</ul>
</li>
</ul>
<p><code>StreamSourceContext</code>提供了生成不同类型<code>SourceContext</code>实例的方法：<code>NonTimestampContext</code>和<code>WaterMarkContext</code></p>
<p>Flink会根据时间进行判断并使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">switch</span> (timeCharacteristic) &#123;</span><br><span class="line">    <span class="keyword">case</span> EventTime:</span><br><span class="line">        ctx =</span><br><span class="line">                <span class="keyword">new</span> ManualWatermarkContext&lt;&gt;(</span><br><span class="line">                        output,</span><br><span class="line">                        processingTimeService,</span><br><span class="line">                        checkpointLock,</span><br><span class="line">                        streamStatusMaintainer,</span><br><span class="line">                        idleTimeout);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> IngestionTime:</span><br><span class="line">        ctx =</span><br><span class="line">                <span class="keyword">new</span> AutomaticWatermarkContext&lt;&gt;(</span><br><span class="line">                        output,</span><br><span class="line">                        watermarkInterval,</span><br><span class="line">                        processingTimeService,</span><br><span class="line">                        checkpointLock,</span><br><span class="line">                        streamStatusMaintainer,</span><br><span class="line">                        idleTimeout);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> ProcessingTime:</span><br><span class="line">        ctx = <span class="keyword">new</span> NonTimestampContext&lt;&gt;(checkpointLock, output);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(String.valueOf(timeCharacteristic));</span><br></pre></td></tr></table></figure>

<h3 id="NonTimestampContext"><a href="#NonTimestampContext" class="headerlink" title="NonTimestampContext"></a>NonTimestampContext</h3><p>不会向下游发送Watermark（所有元素时间戳都是-1）。使用<code>process Time</code>时使用。因为使用事件时间时，Watermark没有意义。</p>
<h3 id="WatermarkContext"><a href="#WatermarkContext" class="headerlink" title="WatermarkContext"></a>WatermarkContext</h3><p>定义了与Watermark相关的行为，包括：</p>
<ul>
<li>负责管理当前的StreamStatus，确保StreamStatus向下游传递。</li>
<li>负责空闲检测的逻辑，当超过设定的事件间隔而没有收到数据或者Watermark时，认为Task处于空闲状态。</li>
</ul>
<p>实际实现分为了<code>AutomaticWatermarkContext</code>和<code>ManualWatermarkContext</code>。</p>
<p>####AutomaticWatermarkContext</p>
<p>使用<code>ingestionTime</code>时使用，自动生成Watermark。持续的自动注册定时器。</p>
<p>其相关方法实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitWatermark</span><span class="params">(Watermark mark)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (allowWatermark(mark)) &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">            streamStatusMaintainer.toggleStreamStatus(StreamStatus.ACTIVE);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (nextCheck != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">this</span>.failOnNextCheck = <span class="keyword">false</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                scheduleNextIdleDetectionTask();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            processAndEmitWatermark(mark);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">allowWatermark</span><span class="params">(Watermark mark)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// allow Long.MAX_VALUE since this is the special end-watermark that for example the</span></span><br><span class="line">    <span class="comment">// Kafka source emits</span></span><br><span class="line">    <span class="keyword">return</span> mark.getTimestamp() == Long.MAX_VALUE &amp;&amp; nextWatermarkTime != Long.MAX_VALUE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processAndCollect</span><span class="params">(T element)</span> </span>&#123;</span><br><span class="line">    lastRecordTime = <span class="keyword">this</span>.timeService.getCurrentProcessingTime();</span><br><span class="line">    output.collect(reuse.replace(element, lastRecordTime));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// this is to avoid lock contention in the lockingObject by</span></span><br><span class="line">    <span class="comment">// sending the watermark before the firing of the watermark</span></span><br><span class="line">    <span class="comment">// emission task.</span></span><br><span class="line">    <span class="keyword">if</span> (lastRecordTime &gt; nextWatermarkTime) &#123;</span><br><span class="line">        <span class="comment">// in case we jumped some watermarks, recompute the next watermark time</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> watermarkTime = lastRecordTime - (lastRecordTime % watermarkInterval);</span><br><span class="line">        nextWatermarkTime = watermarkTime + watermarkInterval;</span><br><span class="line">        output.emitWatermark(<span class="keyword">new</span> Watermark(watermarkTime));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we do not need to register another timer here</span></span><br><span class="line">        <span class="comment">// because the emitting task will do so.</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于来说，一般不会调用<code>processAndEmitWatermark</code>方法，而是通过<code>processAndCollect</code>来创建Watermark</p>
<p>####ManualWatermarkContext</p>
<p>使用<code>EventTime</code>时使用，不会产生WaterMark，而是向下游发送透传(只负责将传输的内容由源地址传输到目的地址)上游的Watermark。</p>
<p>其相关方法实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">allowWatermark</span><span class="params">(Watermark mark)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processAndEmitWatermark</span><span class="params">(Watermark mark)</span> </span>&#123;</span><br><span class="line">    output.emitWatermark(mark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>两者的区别其实相关注释有提及</p>
<blockquote>
<p>This method is only relevant when running on TimeCharacteristic.EventTime. On TimeCharacteristic.ProcessingTime,Watermarks will be ignored. On TimeCharacteristic.IngestionTime, the Watermarks will be replaced by the automatic ingestion time watermarks.</p>
</blockquote>
<h2 id="输出函数"><a href="#输出函数" class="headerlink" title="输出函数"></a>输出函数</h2><p>Connector在实现Sink时，基本都是从<code>RichSinkFunction</code>和<code>TwoPhaseCommitSinkFunction</code>继承。故并没有Function的生命周期管理行为。</p>
<p><code>TwoPhaseCommitSinkFunction</code>是Flink实现端到端Exactly-Once的关键函数。需要配合检查点机制结合实现。</p>
<h2 id="检查点函数"><a href="#检查点函数" class="headerlink" title="检查点函数"></a>检查点函数</h2><p>用来实现函数级别的State管理。Flink设计了<code>CheckpointedFunction</code>和<code>ListCheckpointed</code>接口。</p>
<h3 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h3><ul>
<li>当保存状态后，<code>snapshotStat</code>会被调用，用来备份保存状态到外部存储。</li>
<li><code>initializeState</code>方法负责<strong>初始化State</strong>以及<strong>从上一个检查点恢复状态</strong>的逻辑。<ul>
<li>函数初始化时也会调用该方法。</li>
</ul>
</li>
</ul>
<h3 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h3><blockquote>
<p>在1.13中已经被标记为过时：If you need to do non-keyed state snapshots of your operator, use CheckpointedFunction. This should only be needed in rare cases, though.</p>
</blockquote>
<ul>
<li><code>snapshotState</code>方法，方法有个checkpointId参数，是唯一单调递增的数字，而timestamp则是master触发checkpoint的时间戳，该方法要返回当前的state(<code>List结构</code>)。</li>
<li><code>restoreState</code>方法会在failure recovery的时候被调用，传递的参数为List类型的state，方法里头可以将state恢复到本地。</li>
</ul>
<p><code>ListCheckpointed</code>是<code>CheckpointedFunction</code>的限制版，它只能支持<strong>Even-split redistribution模式的list-style state</strong>。</p>
<blockquote>
<ul>
<li><strong>Even-split redistribution:</strong> 每个算子都保存一个列表形式的状态集合，整个状态由所有的列表拼接而成。当作业恢复或重新分配的时候，<strong>整个状态会按照算子的并发度进行均匀分配</strong>。 比如说，算子 A 的并发读为 1，包含两个元素 <code>element1</code> 和 <code>element2</code>，当并发读增加为 2 时，<code>element1</code> 会被分到并发 0 上，<code>element2</code> 则会被分到并发 1 上。</li>
<li><strong>Union redistribution:</strong> 每个算子保存一个列表形式的状态集合。整个状态由所有的列表拼接而成。当作业恢复或重新分配时，<strong>每个算子都将获得所有的状态数据</strong>。</li>
</ul>
</blockquote>
<p>在限制的同时，省略了初始化State的步骤。</p>
<h1 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h1><p>数据分区在Flink中叫做Partition。抽象接口时StreamPartitioner，决定了实际运行中数据流分发模式。所有的数据分区器(包括<code>StreamPartitioner</code>)都实现了<code>ChannelSelector</code>接口，<code>ChannelSelector</code>接口定义了负载均衡选择行为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ChannelSelector</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">IOReadableWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化下游可选Channel数量</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每条记录的选择方法</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(T record)</span></span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 是否向下游广播。即决定是否将记录写入到下游的所有channel</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在API层面上，分区器作用于DataStream上，并且会生成一个新的DataStream。</p>
<h1 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h1><p>Flink内置了一批连接器，Github的Bahir项目实现了一部分其他数据库的连接器。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的时间语义及简单使用</title>
    <url>/2021/01/17/flink-de-shi-jian-yu-yi-ji-jian-dan-shi-yong/</url>
    <content><![CDATA[<blockquote>
<p>本文Flink API版本：java-1.12</p>
</blockquote>
<blockquote>
<p>注1 ： Flink在1.11后，通过WatermarkStrategy来指定水位线及事件时间。将AssignerWithPeriodicWatermarks<code>和</code>AssignerWithPunctuatedWatermarks标记为过时。本文也主要是记录新接口的使用方法。</p>
<p>注2 ： Flink在1.12（大概）后，默认的时间策略为事件时间。</p>
<p>注3 ： 本文主要以Kafka作为输入元为例。仅简单介绍用法，共后续使用时的资料翻阅。</p>
<p>注4 ： 为了前后的连贯性，本文将不讨论在1.12中已被标记为过时的方法。若需使用早先的API，可翻阅其他资料。</p>
</blockquote>
<h2 id="分配时间戳和生成水位线"><a href="#分配时间戳和生成水位线" class="headerlink" title="分配时间戳和生成水位线"></a>分配时间戳和生成水位线</h2><p>使用事件时间，需要提供数据中<strong>代表时间的字段</strong>，并指定水位线。在新的FlinkAPI中，提供了常见的默认实现，来将时间戳的分配简单化：</p>
<h3 id="默认实现"><a href="#默认实现" class="headerlink" title="默认实现"></a>默认实现</h3><p>简单使用：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> WatermarkStrategy&lt;ObjectNode&gt; watermarkStrategy = WatermarkStrategy.</span><br><span class="line">				&lt;ObjectNode&gt;forBoundedOutOfOrderness(Duration.ofMillis(<span class="number">10</span>*<span class="number">1000</span>))</span><br><span class="line">				.withTimestampAssigner((SerializableTimestampAssigner&lt;ObjectNode&gt;) (element, recordTimestamp) -&gt; element.get(<span class="string">&quot;value&quot;</span>).get(<span class="string">&quot;log_time&quot;</span>).longValue());</span><br></pre></td></tr></table></figure>

<h4 id="水位线分配"><a href="#水位线分配" class="headerlink" title="水位线分配"></a>水位线分配</h4><p>通过调用WatermarkStrategy接口的默认方法，可以直接使用内置的通用策略。分别是</p>
<ul>
<li><p><code>forMonotonousTimestamps()</code>：单调递增的水位线分配器。</p>
<ul>
<li><p>当前时间的时间戳就充当watermark，如果后续数据时间戳更小，则认为是过期数据。</p>
</li>
<li><p>对于并行数据源。则是每个分区独立计算。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a watermark strategy for situations with monotonously ascending timestamps.<br>The watermarks are generated periodically and tightly follow the latest timestamp in the data. The delay introduced by this strategy is mainly the periodic interval in which the watermarks are generated.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>forBoundedOutOfOrderness(Duration.ofSeconds(10))</code></p>
<ul>
<li><p>参数表示数据之间允许的最大延迟。</p>
</li>
<li><p>如果新数据的时间戳 比 <strong>水位线-最大延迟 还要小</strong>。则认为是过期数据</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a watermark strategy for situations where records are out of order, but you can place an upper bound on how far the events are out of order. An out-of-order bound B means that once the an event with timestamp T was encountered, no events older than T - B will follow any more.<br>The watermarks are generated periodically. The delay introduced by this watermark strategy is the periodic interval length, plus the out of orderness bound.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>默认实现的水位线是周期性分配的。</p>
<ul>
<li>分配的周期取决于取决于<code>ExecutionConfig.getAutoWatermarkInterval()</code>。</li>
<li>该间隔可通过<code>env.getConfig().setAutoWatermarkInterval(long interval_ms)</code>设置。</li>
</ul>
</li>
</ul>
<h4 id="分配事件时间戳"><a href="#分配事件时间戳" class="headerlink" title="分配事件时间戳"></a>分配事件时间戳</h4><p>通过*.withTimestampAssigner*,可指定如何将数据的某个字段分配为时间戳，对于Kafka来说，若不指定则是将Kafka的record-time作为事件时间。</p>
<p>Flink提供两种方式来指定：</p>
<ul>
<li><p>TimestampAssignerSupplier<T>：为时间分配器提供更多上下文。</p>
<ul>
<li><p>实现方法需要返回一个TimestampAssigner。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>You can use this when a TimestampAssigner needs additional context, for example access to the metrics system.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>SerializableTimestampAssigner<T> ：指定如何将数据的某个字段分配为时间戳</p>
<ul>
<li><p>实现extractTimestamp方法，用来指定时间戳</p>
</li>
<li><p>SerializableTimestampAssigner实现的就是TimestampAssigner，并增加了序列化。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>You can use this in case you want to specify a TimestampAssigner via a lambda function.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="空闲数据"><a href="#空闲数据" class="headerlink" title="空闲数据"></a>空闲数据</h4><blockquote>
<p>摘自官网：</p>
<p>如果数据源中的某一个分区/分片在一段时间内未发送事件数据，则意味着 <code>WatermarkGenerator</code> 也不会获得任何新数据去生成 watermark。在这种情况下，当某些其他分区仍然发送事件数据的时候就会出现问题。由于下游算子 watermark 的计算方式是取所有不同的上游并行数据源 watermark 的最小值，则其 watermark 将不会发生变化。</p>
<p>故可以使用 <code>WatermarkStrategy</code> 来检测空闲输入并将其标记为空闲状态。</p>
</blockquote>
<p>WatermarkStrategy接口提供了<code>withIdleness(Duration idleTimeout)</code>默认方法。</p>
<p>即当某一个分区超过<em>多长时间</em>没有数据时，则标记为空闲。就算该分区的水位线没有变化，也不会影响整体水位线的变化。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a new enriched WatermarkStrategy that also does idleness detection in the created WatermarkGenerator.<br>Add an idle timeout to the watermark strategy. If no records flow in a partition of a stream for that amount of time, then that partition is considered “idle” and will not hold back the progress of watermarks in downstream operators.<br>Idleness can be important if some partitions have little data and might not have events during some periods. Without idleness, these streams can stall the overall event time progress of the application</p>
</blockquote>
</li>
</ul>
<h4 id="注意点："><a href="#注意点：" class="headerlink" title="注意点："></a>注意点：</h4><ul>
<li>需<code>&lt;ObjectNode&gt;forBoundedOutOfOrderness</code>这般显示指定数据类型。</li>
</ul>
<h3 id="自定义实现"><a href="#自定义实现" class="headerlink" title="自定义实现"></a>自定义实现</h3><p>可以实现WatermarkStrategy接口，来自定义水位线策略：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> WatermarkStrategy&lt;ObjectNode&gt;() &#123;</span><br><span class="line">	<span class="comment">// 必须实现</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> WatermarkGenerator&lt;ObjectNode&gt; <span class="title">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> WatermarkGenerator&lt;ObjectNode&gt;() &#123;</span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(ObjectNode event, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;;</span><br><span class="line">	&#125;;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 选择实现</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> TimestampAssigner&lt;ObjectNode&gt; <span class="title">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h4><ul>
<li><p><code>createWatermarkGenerator</code>：必须实现</p>
<ul>
<li><p>方法需要返回一个<code>WatermarkGenerator</code>。可以通过实现对应接口自定义。</p>
<ul>
<li><code>onEvent(T event, long eventTimestamp, WatermarkOutput output)</code><ul>
<li>每条记录都会调用。根据记录记住事件时间戳，或更新水位线。</li>
</ul>
</li>
<li><code>onPeriodicEmit(WatermarkOutput output)</code><ul>
<li>定期调用。可能生成新的时间戳（也可能不会）生成间隔取决于ExecutionConfig.getAutoWatermarkInterval()</li>
</ul>
</li>
</ul>
</li>
<li><p>watermark 的生成方式本质上是有两种：<em>周期性生成</em> 和 <em>标记生成</em>。</p>
</li>
<li><p><strong>周期性</strong>生成器通常的实现逻辑：</p>
<ul>
<li><p>通过 <code>onEvent()</code>更新事件时间戳。</p>
</li>
<li><p>通过<code>onPeriodicEmit()</code>发出水位线。注意：<em>onPeriodicEmit方法的调用是定时调用的</em></p>
</li>
<li><blockquote>
<p> 官网示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessGenerator</span> <span class="keyword">implements</span> <span class="title">WatermarkGenerator</span>&lt;<span class="title">MyEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness = <span class="number">3500</span>; <span class="comment">// 3.5 秒</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(MyEvent event, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 发出的 watermark = 当前最大时间戳 - 最大乱序时间</span></span><br><span class="line">        output.emitWatermark(<span class="keyword">new</span> Watermark(currentMaxTimestamp - maxOutOfOrderness - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</blockquote>
</li>
</ul>
</li>
<li><p><strong>标记</strong>生成器通常的实现逻辑：</p>
<ul>
<li><p>通过 <code>onEvent()</code>判断是否需要更新（如数据中明确的字段值），如果需要则直接更新。</p>
</li>
<li><blockquote>
<p>官网示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class PunctuatedAssigner implements WatermarkGenerator&lt;MyEvent&gt; &#123;</span><br><span class="line"></span><br><span class="line"> @Override</span><br><span class="line"> public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) &#123;</span><br><span class="line">     if (event.hasWatermarkMarker()) &#123;</span><br><span class="line">         output.emitWatermark(new Watermark(event.getWatermarkTimestamp()));</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> @Override</span><br><span class="line"> public void onPeriodicEmit(WatermarkOutput output) &#123;</span><br><span class="line">     // onEvent 中已经实现</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>createTimestampAssigner</code>：选择实现</p>
<ul>
<li>参考：[分配事件时间戳](# 分配事件时间戳)</li>
</ul>
</li>
</ul>
<h3 id="为数据源指定事件时间"><a href="#为数据源指定事件时间" class="headerlink" title="为数据源指定事件时间"></a>为数据源指定事件时间</h3><p>通过上述过程，我们很容易得到一个<strong>WatermarkStrategy</strong>，通过这个，我们就能够用来指定数据中的事件时间戳和水位线了。</p>
<p>以Kafka为例，如果要修改默认的水位线策略，只需通过<code>.assignTimestampsAndWatermarks()</code>方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">myConsumer.assignTimestampsAndWatermarks(watermarkStrategy);</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>.assignTimestampsAndWatermarks</code>方法同样是DataStream中提供的方法。</p>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="使用事件时间及水位线"><a href="#使用事件时间及水位线" class="headerlink" title="使用事件时间及水位线"></a>使用事件时间及水位线</h2><p>todo 概述</p>
<h3 id="处理函数"><a href="#处理函数" class="headerlink" title="处理函数"></a>处理函数</h3><p>上述内容中得到的时间戳和水位线存放于程序的上下文<code>(Context ctx)</code>中，一般而言可以通过Process及相关的函数进行访问。</p>
<blockquote>
<p>处理函数都实现了RichFunction接口</p>
</blockquote>
<p>对于ProcessFunction，一般提供下述两个方法：</p>
<ul>
<li><code>processElement(I value, Context ctx, Collector&lt;O&gt; out)</code>：必须实现<ul>
<li>每条记录调用一次。</li>
<li>通过<code> Context ctx</code>访问<strong>时间戳</strong>。（也可以访问一些其他的上下文信息，如TimerService，将结果发往副结果）</li>
</ul>
</li>
<li><code>onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</code>：选择实现<ul>
<li>会在计时器触发时调用。<strong>故一般计时器会在<code>processElement</code>中设置</strong></li>
<li><code>timestamp</code>参数指的是触发计时器的时间戳。</li>
<li>用在<code>KeyedProcessFunction</code>中，用来在某些键值不再使用后，清除分区状态/或实现一些基于时间的自定义窗口逻辑。</li>
</ul>
</li>
<li>计时器：通过ctx的<code>timerService</code>使用<ul>
<li><strong>只能在键值流中使用计时器</strong>（*Setting timers is only supported on a keyed streams.*）</li>
<li>每个时间戳可以拥有一个计时器。一个key可以有多个计时器（key不同时，计时器的时间戳允许重复）</li>
<li><strong>不应该注册过多的计时器。建议在每个processFunction通过维护状态来管理计时器（注册+删除）。</strong></li>
<li>计时器触发后，就自动删除了？（todo）</li>
<li><code>timerService</code>：<ul>
<li><code>registerEventTimeTimer(long time)</code>：注册一个事件时间计时器。<ul>
<li>参数就相当于计时器id及出发条件。</li>
<li>当水位线时间戳 大于或等于计时器时间戳，则触发。</li>
</ul>
</li>
<li><code>deleteEventTimeTimer(long time)</code>：删除一个事件时间计时器</li>
<li><code>currentProcessingTime()</code>返回当前的处理时间</li>
<li><code>currentWatermark()</code>返回当前水位线</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="窗口算子"><a href="#窗口算子" class="headerlink" title="窗口算子"></a>窗口算子</h3><blockquote>
<p>1、本文只考虑基于时间的窗口。</p>
<p>2、窗口可用于键值与非键值数据流。本文主要以键值数据流为例进行说明。</p>
<p>3、对于键值数据流，窗口算子为并行计算。对于非键值，考虑到需要处理整个窗口的所有元素，需要单线程处理。</p>
</blockquote>
<p>实际使用窗口算子时，必需要指定两个窗口组件：</p>
<ul>
<li>窗口分配器<code>WindowAssigner</code>，得到<code>WindowedStream</code>：决定了元素如何划分到不同的窗口中。</li>
<li>作用于<code>WindowedStream</code>上的处理函数：如何处理单个窗口中的数据。</li>
</ul>
<p>如果需要自定义某些实现逻辑，还可以指定其他窗口组件：</p>
<ul>
<li>触发器<code>trigger</code>：定义了窗口何时准备好执行计算、何时需要清除自身内容。</li>
<li>移除器<code>evictor</code>：用来从窗口中删除已收集元素。可以在ProcessWindowFunc前/后使用。</li>
</ul>
<p>简单示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">baseInput.keyBy(((KeySelector&lt;Tuple4&lt;Integer, Long, Long, String&gt;, Integer&gt;) value -&gt; value.f0))</span><br><span class="line">				.window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>)))</span><br><span class="line">				.process(<span class="keyword">new</span> ProcessWindowFunction&lt;Tuple4&lt;Integer, Long, Long, String&gt;, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">					<span class="meta">@Override</span></span><br><span class="line">					<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Integer integer, Context context, Iterable&lt;Tuple4&lt;Integer, Long, Long, String&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">						StringBuffer buffer = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">						buffer.append(integer).append(<span class="string">&quot;!&quot;</span>);</span><br><span class="line">						elements.forEach(ele -&gt; buffer.append(ele.f3).append(<span class="string">&quot;-&quot;</span>));</span><br><span class="line">						out.collect(buffer.toString());</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;);</span><br></pre></td></tr></table></figure>

<h4 id="窗口分配器"><a href="#窗口分配器" class="headerlink" title="窗口分配器"></a>窗口分配器</h4><h5 id="内置窗口分配器"><a href="#内置窗口分配器" class="headerlink" title="内置窗口分配器"></a>内置窗口分配器</h5><ul>
<li><p>TumblingWindow（滚动窗口）：下一个窗口的开始为：这个窗口的结束时间。</p>
<ul>
<li><p>提供两个内置实现（事件时间和处理时间各一个）：</p>
<ul>
<li><p><code>TumblingEventTimeWindows</code>：基于事件时间的滚动窗口。</p>
</li>
<li><p><code>of</code>方法接收参数：</p>
<ul>
<li><p>size : 窗口大小</p>
</li>
<li><p>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</p>
</li>
<li><p>windowStagger：交错策略。实际源码中表现为会影响offset的效果。</p>
<ul>
<li><blockquote>
<p>The utility that produces staggering offset in runtime. </p>
<p>有三类：</p>
<p>​    ALIGNED：默认。不产生任何影响。所有分区的窗口开始时间都是一样的</p>
<p>​    RANDOM：加入一个随机值作为交错参数。即每个分区的第一个窗口的开始时间会随机开始。</p>
<p>​    NATURAL：根据每个线程得到的第一个元素，来作为该分区的第一个窗口的开始时间。</p>
<p>（待验证）</p>
<p>Flink会在WindowOperator中的processElement方法中，对每个接收到的流元素进行窗口画风。而这个参数会影响到最后生成的elementWindows，windowStagger主要就是通过影响offset，来影响elementWindows的生成。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>窗口大小决定了多久会生成一个新的窗口。</p>
</li>
</ul>
</li>
<li><p>SlidingTimeWindow（滑动窗口）：下一个窗口开始时间为：这个窗口开始时间+滑动距离。</p>
<ul>
<li>提供两个内置实现（事件时间和处理时间各一个）：<ul>
<li><code>SlidingEventTimeWindows</code>：基于事件时间的滑动窗口。</li>
<li><code>of</code>方法接收参数：<ul>
<li>size : 窗口大小。</li>
<li>slide：滑动距离。</li>
<li>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</li>
</ul>
</li>
</ul>
</li>
<li>滑动距离决定了多久会生成一个新的窗口。</li>
</ul>
</li>
<li><p>sessionWindow（会话窗口）：下一个窗口开始时间为：新元素到达 且距离上一个元素到达时间超过阈值。</p>
<ul>
<li><p>提供两类(动态和固定)，供四个（事件时间和处理时间各两个）内置实现：</p>
<ul>
<li><p><code>EventTimeSessionWindows</code>：基于事件时间的会话窗口。</p>
<ul>
<li>方法：<ul>
<li><code>withGap(Time size)</code>：间隔阈值。</li>
</ul>
</li>
</ul>
</li>
<li><p><code>DynamicEventTimeSessionWindows</code>：基于事件时间的动态会话窗口。</p>
<ul>
<li><p>方法：</p>
<ul>
<li><p><code>withDynamicGap(SessionWindowTimeGapExtractor&lt;T&gt; sessionWindowTimeGapExtractor)</code>：</p>
<ul>
<li><blockquote>
<p>SessionWindowTimeGapExtractor能够从数据中提取超时间隔。</p>
<p>即数据中可以有一个字段，来作为这条数据距离上一条数据多久。<code>SessionWindowTimeGapExtractor</code>接口需要手动实现。且只有一个方法，从数据中拿到这他的间隔<code>long extract(T element);</code></p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>两者的主要区别：</p>
<ul>
<li>非动态：超时间隔是固定的。</li>
<li>动态：有数据自己决定是否划分新窗口。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>globalWindow（全局窗口）: 只会产生一个窗口</p>
<ul>
<li>不会自动触发，必须手动指定<em>触发器</em>。</li>
<li>必须指定<em>移除器</em>来移除窗口中的元素</li>
</ul>
</li>
</ul>
<h5 id="自定义窗口分配器"><a href="#自定义窗口分配器" class="headerlink" title="自定义窗口分配器"></a>自定义窗口分配器</h5><p><code>WindowAssigner</code>接口提供了四个方法需要实现：</p>
<ul>
<li><code>assignWindows(T element, long timestamp, WindowAssignerContext context)</code>：返回元素归属的窗口集合（这条记录归属哪些窗口）<ul>
<li>由于一般都是针对时间设计，所以一般都是返回TimeWindow。基于数量的窗口可以使用<code>countWindow</code></li>
</ul>
</li>
<li><code>getDefaultTrigger(StreamExecutionEnvironment env)</code>：返回该分配器的默认触发器。<ul>
<li>请参考自定义触发器，或者Trigger子类的内部实现。</li>
</ul>
</li>
<li><code>getWindowSerializer(ExecutionConfig executionConfig)</code>：返回窗口的TypeSerializer<ul>
<li>对于TimeWindow而言，则返回<code>new TimeWindow.Serializer()</code>即可。</li>
</ul>
</li>
<li><code>isEventTime()</code>：是否是事件时间窗口</li>
</ul>
<h4 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h4><p>触发器决定何时对窗口进行计算并发出结果。每次调用触发器都会生成一个<code>TriggerResult</code>，用于决定窗口接下来的行为：</p>
<ul>
<li>CONTINUE：什么都不做。</li>
<li>FIRE：调用<code>ProcessFunction</code>（如果有使用），并发出结果。如果包含增量聚合函数，则直接发出结果。</li>
<li>PURGE：清空窗口内容，并删除窗口、及窗口元数据。并调用<code>ProcessWindowFunction.clear()</code>方法。</li>
<li>FIRE_AND_PURGE：先FIRE，再PURGE。</li>
</ul>
<h5 id="内置触发器"><a href="#内置触发器" class="headerlink" title="内置触发器"></a>内置触发器</h5><p>Flink提供一些内置的触发器，包括（只说明事件时间的，处理时间一样的）：</p>
<ul>
<li><p><code>EventTimeTrigger</code>：水位线大于窗口结束时间，则触发。</p>
</li>
<li><p><code>CountTrigger</code>：当窗口内元素数量大于阈值时则触发。</p>
</li>
<li><p><code>DeltaTrigger</code>：初始化是需要给定一个<code>DeltaFunction</code>，根据接入数据计算出来的Delta指标是否超过指定的Threshold去判断是否触发窗口计算。</p>
<ul>
<li>计算当前元素与上一个触发计算元素的Delta值来与阈值进行比较。</li>
</ul>
</li>
<li><p><code>ContinuousEventTimeTrigger</code>:</p>
<ul>
<li><p>水位线大于窗口结束时间，则触发。</p>
</li>
<li><p>定期设置处理时间计时器。（计时器触发时会调用触发器的<code>onEventTime</code>方法。详见自定义）</p>
<ul>
<li><blockquote>
<p>1、当第一个元素达到时，会根据时间间隔注册一个计时器。触发时间约为开始时间+时间间隔（<code>start = timestamp - (timestamp % interval)+interval</code>）</p>
<p>2、当计时器触发时（），进行判断。</p>
<p>​    2.1、如果触发事件 等于 窗口结束时间，则触发触发器。</p>
<p>​    2.2、其他情况，则判断计时器是否有效，若有效则触发触发器，并根据设定的间隔注册下一个计时器。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>PurgingTrigger</code>：是一个trigger的包装类。具体作用为：如果被包装的trigger触发返回FIRE，则PurgingTrigger将返回修改为FIRE_AND_PURGE，其他的返回值不做处理。</p>
<ul>
<li>通过<code>of(Trigger&lt;T, W&gt; nestedTrigger)</code>来包装其他<code>Trigger</code></li>
</ul>
</li>
<li><p><code>ProcessingTimeoutTrigger</code>是一个trigger的包装类，作用于处理时间。具体作用为：为元素配置处理时间超时。当到达超时时间时，清除触发器状态</p>
</li>
</ul>
<h5 id="自定义触发器"><a href="#自定义触发器" class="headerlink" title="自定义触发器"></a>自定义触发器</h5><p>自定义触发器需要实现触发器API的下述方法：</p>
<ul>
<li><code>onElement(T element, long timestamp, W window, TriggerContext ctx)</code>：每有元素添加到窗口时就调用。<ul>
<li>入参<code>long timestamp</code>为事件的时间属性。</li>
<li>入参<code>W window</code>为时间所属的窗口对象。（对于一个元素所属多个窗口时，会对应有多个触发器，每个触发器只负责一个窗口）</li>
</ul>
</li>
<li><code>onProcessingTime(long time, W window, TriggerContext ctx)</code>：处理时间计时器触发时调用。计时器通过ctx注册/清除。</li>
<li><code>onEventTime(long time, W window, TriggerContext ctx)</code>事件时间计时器触发时调用。计时器通过ctx注册/清除。<ul>
<li>入参<code>long time</code>为触发计时器的时间。等于计时器注册时间。</li>
</ul>
</li>
<li><code>canMerge()</code>：该触发器是否支持合并。</li>
<li><code>onMerge(W window, OnMergeContext ctx)</code>：合并逻辑（将多个窗口合并为一个窗口），需要合并时则调用该方法。<ul>
<li>在触发器与<code>MergingWindowAssigner</code>一起使用时，需要实现该方法。</li>
<li>触发器的自定义状态同样需要合并。</li>
<li>入参<code>W window</code>为合并后的窗口。</li>
</ul>
</li>
<li><code>clear(W window, TriggerContext ctx)</code>：在触发器中清除那些为给定窗口保存的状态。</li>
</ul>
<h4 id="移除器"><a href="#移除器" class="headerlink" title="移除器"></a>移除器</h4><p>用于在窗口执行计算前或计算后删除窗口中的元素。是一个可选组件。</p>
<h5 id="内置移除器"><a href="#内置移除器" class="headerlink" title="内置移除器"></a>内置移除器</h5><p>Flink提供了三类内置的<code>Evictor</code>：</p>
<ul>
<li><code>TimeEvictor</code>：以时间为判断标准，决定元素是否会被移除。<ul>
<li>该移除器初始化时需要赋予一个时间size大小。移除比 最大时间-size 还小的数据。</li>
</ul>
</li>
<li><code>CountEvictor</code>：以元素计数为标准，决定元素是否会被移除。<ul>
<li>该移除器初始化时需要赋予一个数量size大小。移除多余的元素。从前向后移除。</li>
</ul>
</li>
<li><code>DeltaEvictor</code>：DeltaEvictor通过计算DeltaFunction的值（依次传入每个元素和最后一个元素），并将其与threshold进行对比，如果函数计算结果大于等于threshold，则该元素会被移除。</li>
</ul>
<h5 id="自定义移除器"><a href="#自定义移除器" class="headerlink" title="自定义移除器"></a>自定义移除器</h5><p>自定义移除器需要实现移除器API的下述方法：</p>
<ul>
<li><code>evictBefore(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, int size, W window, EvictorContext ctx)</code>：窗口函数作用于窗口内容前调用。</li>
<li><code>evictAfter(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, int size, W window, EvictorContext ctx)</code>：窗口函数作用于窗口内容后调用。<ul>
<li>入参<code>(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements</code>：窗口中的元素集合。移除操作主要对这个集合处理。满足条件时<code>iterator.remove();</code></li>
<li>入参<code>int size</code>：当前的元素个数。</li>
<li>入参<code>W window</code>：本次处理的窗口。</li>
</ul>
</li>
</ul>
<h2 id="处理迟到数据"><a href="#处理迟到数据" class="headerlink" title="处理迟到数据"></a>处理迟到数据</h2><p>即便设置了水位线，也不可避免的会产生迟到数据。</p>
<blockquote>
<p>迟到数据一般是针对window而言，因为window需要聚合一段时间内的数据触发计算。</p>
<p>不需要window的情况下，在处理过程中其实对迟到数据不需要特殊处理。</p>
</blockquote>
<p>Flink提供了三种处理方案：</p>
<ul>
<li>直接丢弃。默认方案</li>
<li>基于迟到事件更新结果。</li>
<li>旁路输出。</li>
</ul>
<h3 id="基于迟到事件更新结果"><a href="#基于迟到事件更新结果" class="headerlink" title="基于迟到事件更新结果"></a>基于迟到事件更新结果</h3><p>Flink的<code>WindowedStream</code>提供了<code>allowedLateness(Time lateness)</code>方法。入参为允许延迟的时间（默认为0）。这个方法的作用和实现方式：</p>
<ul>
<li>当设置lateness不为0时，窗口被删除的时间会在原本的基础上推迟lateness。</li>
<li>当水位线触发计算后，每当有满足要求的迟到数据达到时，都会再次触发计算。</li>
</ul>
<h3 id="旁路输出"><a href="#旁路输出" class="headerlink" title="旁路输出"></a>旁路输出</h3>]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Time</tag>
        <tag>Window</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-3.0 自适应查询</title>
    <url>/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/</url>
    <content><![CDATA[<h1 id="Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译"><a href="#Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译" class="headerlink" title="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译"></a>Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译</h1><p>原文链接：<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></p>
<blockquote>
<p>Over the years, there’s been an extensive and continuous effort to improve Spark SQL’s query optimizer and planner in order to generate high-quality query execution plans. One of the biggest improvements is the cost-based optimization framework that collects and leverages a variety of data statistics (e.g., row count, number of distinct values, NULL values, max/min values, etc.) to help Spark choose better plans. Examples of these cost-based optimization techniques include choosing the right join type (broadcast hash join vs. sort merge join), selecting the correct build side in a hash-join, or adjusting the join order in a multi-way join. However, outdated statistics and imperfect cardinality estimates can lead to suboptimal query plans. Adaptive Query Execution, new in the upcoming Apache SparkTM 3.0 release and available in the Databricks Runtime 7.0, now looks to tackle such issues by reoptimizing and adjusting query plans based on runtime statistics collected in the process of query execution.</p>
</blockquote>
<p>多年来，为了提供高质量的查询计划，Spark-SQL的query optimizer 和query planner进行了连续且大量的更新，其中最大的进步之一便是基于成本的优化框架，它收集并利用各种数据统计信息(例如，行计数、不同值的数量、空值、最大/最小值等)，来帮助Spark选择更好的计划。</p>
<p>这些基于成本的框架示例包括有：选择right join的类型（hash广播join vs 排序合并join），选择合适的hash-join端，调整join计划的链接顺序等。然而，过时的统计数据和不完美的基数估计可能导致不佳(次优)的查询计划。</p>
<p>自适应查询执行是即将发布的Apache SparkTM 3.0版本中的新增功能，该功能通过基于查询执行<strong>过程中收集的运行时统计信息</strong>，来优化并调整查询计划来解决这些问题。</p>
<h2 id="The-Adaptive-Query-Execution-AQE-framework"><a href="#The-Adaptive-Query-Execution-AQE-framework" class="headerlink" title="The Adaptive Query Execution (AQE) framework"></a>The Adaptive Query Execution (AQE) framework</h2><blockquote>
<p>One of the most important questions for Adaptive Query Execution is when to reoptimize. Spark operators are often pipelined and executed in parallel processes. However, a shuffle or broadcast exchange breaks this pipeline. We call them materialization points and use the term “query stages” to denote subsections bounded by these materialization points in a query. Each query stage materializes its intermediate result and the following stage can only proceed if all the parallel processes running the materialization have completed. This provides a natural opportunity for reoptimization, for it is when data statistics on all partitions are available and successive operations have not started yet.</p>
</blockquote>
<p>AQE中最重要的便是何时去重新优化，Spark任务通常是链式并行计算(pipelined and executed in parallel processes)，不过shuffle和广播会中断上述的链式计算，我们称之为物化点，后续文章中将使用“查询阶段”来表示查询中由这些物化点限定的子节。</p>
<p>每个查询阶段都需要物化其中间结果，只有在运行物化的所有并行进程都已完成时，下一个阶段才能继续。这个节点是天然的能够用来重新优化的节点，因为它有所有之前分区的统计数据，并且后续计划还未开始。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png" alt="优化逻辑"></p>
<blockquote>
<p>When the query starts, the Adaptive Query Execution framework first kicks off all the leaf stages — the stages that do not depend on any other stages. As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.</p>
</blockquote>
<p>当查询开始时，AQE框架会启动所有‘叶’阶段——不需要依赖其他阶段的阶段。一旦这些阶段完成了物化，框架就在物理查询计划中将它们标记为完成，并使用从<strong>完成阶段检索到的运行时统计信息</strong>，来相应地更新逻辑查询计划。</p>
<p>基于这些统计信息，框架会运行优化器(具有选定的逻辑优化规则列表)，物理规划器，物理优化规则。物理优化规则包括常规物理规则和特定于自适应执行的规则，例如合并分区、join时的数据倾斜处理等。</p>
<p>现在我们已经获得了一个新优化的查询计划，其中包含一些已完成的阶段。AQE框架将搜索并执行那些子阶段已经完成物化的新的查询阶段，然后重复上面的执行-重新优化-执行过程，直到完成整个查询。</p>
<blockquote>
<p>In Spark 3.0, the AQE framework is shipped with three features:</p>
<ul>
<li><p>Dynamically coalescing shuffle partitions</p>
</li>
<li><p>Dynamically switching join strategies</p>
</li>
<li><p>Dynamically optimizing skew joins</p>
</li>
</ul>
<p>The following sections will talk about these three features in detail.</p>
</blockquote>
<p>Spark3.0中，AQE附带了下面三个功能：</p>
<ul>
<li>动态合并shuffle分区。简化甚至避免调整 shuffle 分区的数量。</li>
<li>动态调整连接策略。部分避免了由于缺少统计信息或错误估计大小而导致执行次计划的情况</li>
<li>动态优化数据倾斜连接。解决数据倾斜的问题。</li>
</ul>
<p>接下来的章节会详细讨论这些功能：</p>
<h2 id="Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）"><a href="#Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）" class="headerlink" title="Dynamically coalescing shuffle partitions（动态合并shuffle分区）"></a>Dynamically coalescing shuffle partitions（动态合并shuffle分区）</h2><blockquote>
<p>When running queries in Spark to deal with very large data, shuffle usually has a very important impact on query performance among many other things. Shuffle is an expensive operator as it needs to move data across the network, so that data is redistributed in a way required by downstream operators.</p>
<p>One key property of shuffle is the number of partitions. The best number of partitions is data dependent, yet data sizes may differ vastly from stage to stage, query to query, making this number hard to tune:</p>
<ol>
<li>If there are too few partitions, then the data size of each partition may be very large, and the tasks to process these large partitions may need to spill data to disk (e.g., when sort or aggregate is involved) and, as a result, slow down the query.</li>
<li>If there are too many partitions, then the data size of each partition may be very small, and there will be a lot of small network data fetches to read the shuffle blocks, which can also slow down the query because of the inefficient I/O pattern. Having a large number of tasks also puts more burden on the Spark task scheduler.</li>
</ol>
</blockquote>
<p>使用Spark处理大数据时，shuffle是影响性能中最重要的一项。shuffle是一个昂贵的操作，因为需要经过网络传输数据，从而以后续算子需要的样子重新分配分区。</p>
<p>shuffle的关键属性之一是分区数量，分区的最佳数量取决于数据，但是不同阶段、不同查询的数据大小可能会有很大差异，这使得该数量很难调优。</p>
<ol>
<li>如果分区数量过小，单个分区中的数据会很大。处理这些大分区的任务时，可能需要将数据溢出到磁盘。(例如，当涉及排序或聚合时)，因此减慢了查询速度。</li>
<li>如果分区数量过多，单个分区中的数据会很小。这会导致将有大量小型网络数据提取来读取shuffle块，会因为低效的I/O模式而减慢查询速度。并且过多数量的任务也会给Spark任务调度带来更多负担。</li>
</ol>
<blockquote>
<p>To solve this problem, we can set a relatively large number of shuffle partitions at the beginning, then combine adjacent small partitions into bigger partitions at runtime by looking at the shuffle file statistics.</p>
<p>For example, let’s say we are running the query SELECT max(i)FROM tbl GROUP BY j. The input data tbl is rather small so there are only two partitions before grouping. The initial shuffle partition number is set to five, so after local grouping, the partially grouped data is shuffled into five partitions. Without AQE, Spark will start five tasks to do the final aggregation. However, there are three very small partitions here, and it would be a waste to start a separate task for each of them.</p>
<p>Instead, AQE coalesces these three small partitions into one and, as a result, the final aggregation now only needs to perform three tasks rather than five.</p>
</blockquote>
<p>为了解决这些问题。我们可以在任务开始时设置大数量的分区。然后在运行时通过查看统计数据，将相邻的小分区组合成更大的分区。</p>
<p>例如，当我们执行查询下述时：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">max</span>(i)<span class="keyword">FROM</span> tbl <span class="keyword">GROUP</span> <span class="keyword">BY</span> j</span><br></pre></td></tr></table></figure>

<p>输入数据tbl非常小，所以在grouping前只有2个分区。如果初始的shuffle分区数量设置为5，在local grouping后，部分分区的数据会被shuffle到5个分区中。没有AQE时，Spark会启动5个任务来执行最后的聚合计算。但是这里有三个非常小的分区，为每个分区启动单独的任务将是一种浪费：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140533.png" alt="Without AQE"></p>
<p>不过当启用AQE时，AQE会合并这三个小的分区，所以最后的聚合只需要3个任务，而不是5个。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140648.png" alt="enable AQE"></p>
<h2 id="Dynamically-switching-join-strategies（动态调整连接策略）"><a href="#Dynamically-switching-join-strategies（动态调整连接策略）" class="headerlink" title="Dynamically switching join strategies（动态调整连接策略）"></a>Dynamically switching join strategies（动态调整连接策略）</h2><blockquote>
<p>Spark supports a number of join strategies, among which broadcast hash join is usually the most performant if one side of the join can fit well in memory. And for this reason, Spark plans a broadcast hash join if the estimated size of a join relation is lower than the broadcast-size threshold. But a number of things can make this size estimation go wrong — such as the presence of a very selective filter — or the join relation being a series of complex operators other than just a scan.</p>
</blockquote>
<p>Spark支持多种连接策略，例如：如果join的一侧可以放入内存中，则广播hash-join(broadcast hash join)通常是性能最高的。基于上述原因，Spark会在join一侧的预估数据量小于广播阈值（broadcast-size threshold）时，Spark会采用broadcast hash join。不过，很多事情可能会导致这种大小估计出错——例如，存在一个非常有选择性的过滤器、或者连接侧存在一系列复杂的运算符，而不是只有一次扫描（scan）。</p>
<blockquote>
<p>简单来说，就是Spark估算Join两侧数据量大小不准确时导致的，AQE能够动态基于之前物化的数据来调整连接策略，也就减少了这种统计出错带来的影响。</p>
</blockquote>
<blockquote>
<p>To solve this problem, AQE now replans the join strategy at runtime based on the most accurate join relation size. As can be seen in the following example, the right side of the join is found to be way smaller than the estimate and also small enough to be broadcast, so after the AQE reoptimization the statically planned sort merge join is now converted to a broadcast hash join.</p>
<p>For the broadcast hash join converted at runtime, we may further optimize the regular shuffle to a localized shuffle (i.e., shuffle that reads on a per mapper basis instead of a per reducer basis) to reduce the network traffic.</p>
</blockquote>
<p>为了解决这个问题，AQE会根据最准确的连接关系大小，在运行时重新规划join策略。下面的示例中展示了这种情况。</p>
<p>参与join的右侧的数据的实际数据量(8M)，小于它的预估值(15M)。实际的数据量起始能够通过广播写入内存中，所以在AQE的重新调整后，计划从sort merge join调整为了 broadcast hash join。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622141835.png" alt="Dynamically switching join strategies"></p>
<p>对于运行时转换的broadcast hash join，我们能够进一步的将常规的shuffle调整为局部shuffle来减少网络传输（例如：基于mapper来读取shuffle，而不是基于reducer）</p>
<h2 id="Dynamically-optimizing-skew-joins"><a href="#Dynamically-optimizing-skew-joins" class="headerlink" title="Dynamically optimizing skew joins"></a>Dynamically optimizing skew joins</h2><blockquote>
<p>Data skew occurs when data is unevenly distributed among partitions in the cluster. Severe skew can significantly downgrade query performance, especially with joins. AQE skew join optimization detects such skew automatically from shuffle file statistics. It then splits the skewed partitions into smaller subpartitions, which will be joined to the corresponding partition from the other side respectively.</p>
</blockquote>
<p>当数据在集群中分布不均时就会出现数据倾斜（比如某个key的数据特别多时）。严重的偏斜可能会显著降低查询性能——特别是使用join时。AQE能够自动从shuffle 文件的统计中自动检测此类倾斜。之后，它会将倾斜的分区拆分成更小的子分区，这些子分区会连接上另一侧的与其匹配的分区。</p>
<blockquote>
<p>Let’s take this example of table A join table B, in which table A has a partition A0 significantly bigger than its other partitions.</p>
<p>The skew join optimization will thus split partition A0 into two subpartitions and join each of them to the corresponding partition B0 of table B.</p>
<p>Without this optimization, there would be four tasks running the sort merge join with one task taking a much longer time. After this optimization, there will be five tasks running the join, but each task will take roughly the same amount of time, resulting in an overall better performance.</p>
</blockquote>
<p>下述示例是TableA join TableB，其中，TableA中的分区A0明显大于其他分区。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143417.png" alt="优化前"></p>
<p>因此，AQE将分区A0拆分成两个子分区，并将它们中的每一个子分区，连接到表B的相应分区B0。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143433.png" alt="优化后"></p>
<p>如果没有优化器，这个Join会启动4个Task来执行，并且其中一个Task会耗费非常长的时间(A0对应的任务)。</p>
<p>在优化之后，这个Join会启动5个Task来执行，但每个任务都会耗费几乎一样的时间。最终的结果表现会更好。</p>
<h2 id="TPC-DS-performance-gains-from-AQE（性能提升）"><a href="#TPC-DS-performance-gains-from-AQE（性能提升）" class="headerlink" title="TPC-DS performance gains from AQE（性能提升）"></a>TPC-DS performance gains from AQE（性能提升）</h2><blockquote>
<p>In our experiments using TPC-DS data and queries, Adaptive Query Execution yielded up to an 8x speedup in query performance and 32 queries had more than 1.1x speedup Below is a chart of the 10 TPC-DS queries having the most performance improvement by AQE.</p>
<p>Most of these improvements have come from dynamic partition coalescing and dynamic join strategy switching since randomly generated TPC-DS data do not have skew. Yet we’ve seen even greater improvements in production workload in which all three features of AQE are leveraged.</p>
</blockquote>
<p>在我们使用TPC-DS数据和查询的实验中，AQE在查询性能方面获得了高达8倍的提升，32个查询的提升超过了1.1倍。下面是通过AQE获得最大性能提升的10个TPC-DS查询的图表。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143919.png" alt="性能提升"></p>
<p>这些改进大多来自动态分区合并（Dynamically coalescing shuffle partitions）和动态连接策略切换（Dynamically switching join strategies），因为随机生成的TPC-DS数据一般不存在数据倾斜的问题。</p>
<p>然而，我们已经看到，在利用了AQE的所有三个特性之后，生产工作得到了很大的改善。</p>
<h2 id="Enabling-AQE（启用AQE）"><a href="#Enabling-AQE（启用AQE）" class="headerlink" title="Enabling AQE（启用AQE）"></a>Enabling AQE（启用AQE）</h2><blockquote>
<p>AQE can be enabled by setting SQL config spark.sql.adaptive.enabled to true (default false in Spark 3.0), and applies if the query meets the following criteria:</p>
<ul>
<li>It is not a streaming query</li>
<li>It contains at least one exchange (usually when there’s a join, aggregate or window operator) or one subquery</li>
</ul>
</blockquote>
<p>通过将配置<code>spark.sql.adaptive.enabled</code>设置为<code>true</code>，AQE能在Spark3.0中启用，（在Spark3.0中默认是关闭的）。AQE有如下限制：</p>
<ul>
<li>不能是流式查询</li>
<li>至少需要一次数据交换（比如使用join、聚合、窗口函数时会出现）、或者存在一次子查询。</li>
</ul>
<blockquote>
<p>By making query optimization less dependent on static statistics, AQE has solved one of the greatest struggles of Spark cost-based optimization — the balance between the stats collection overhead and the estimation accuracy. To achieve the best estimation accuracy and planning outcome, it is usually required to maintain detailed, up-to-date statistics and some of them are expensive to collect, such as column histograms, which can be used to improve selectivity and cardinality estimation or to detect data skew. AQE has largely eliminated the need for such statistics as well as for the manual tuning effort. On top of that, AQE has also made SQL query optimization more resilient to the presence of arbitrary UDFs and unpredictable data set changes, e.g., sudden increase or decrease in data size, frequent and random data skew, etc. There’s no need to “know” your data in advance any more. AQE will figure out the data and improve the query plan as the query runs, increasing query performance for faster analytics and system performance.</p>
</blockquote>
<p>通过减少查询优化对静态统计数据的依赖，AQE解决了基于Spark Cost的优化的最大难题之一 — 统计数据的开销和评估精度之间的平衡。为了达到最佳的估计精度和规划结果，通常需要维护详细的最新统计数据，其中一些统计数据的收集成本很高，就例如柱状图，其可用于提高选择性、基数估计、检测数据倾斜。</p>
<p>AQE在很大程度上消除了对此类信息的统计，以及手动调优工作的需要。最重要的是，AQE还使SQL查询优化对任意UDF的不可预测的数据集更改具有更强的弹性，例如：数据大小的突然增加或减少、频繁和随机的数据歪斜等。</p>
<p>现在不再需要事先“了解”您的数据。AQE将在查询运行时找出数据并改进查询计划，从而提高查询性能以实现更快的分析和系统性能。</p>
]]></content>
      <categories>
        <category>Spark3</category>
      </categories>
      <tags>
        <tag>Spark3</tag>
      </tags>
  </entry>
</search>
