<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>spark-3.0 Hadoop2.6-CDH编译的问题</title>
      <link href="/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/"/>
      <url>/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<p>使用cdh5版本编译Spark3时，会出现如下问题：</p><pre><code>&gt; [ERROR] [Error]&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:298:&gt; value setRolledLogsIncludePattern is not a member of&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext&gt; [ERROR] [Error]&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:300:&gt; value setRolledLogsExcludePattern is not a member of&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext</code></pre><p>这个问题起始官方曾经修复过，不过后来可能改过这部分代码，导致编译报错：</p><p><a href="https://github.com/apache/spark/pull/16884" target="_blank" rel="noopener">Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p><p>这是实际解决的连接：</p><p><a href="https://github.com/apache/spark/pull/16884/commits/d7c7e81bafa229bb6083ed5b29789b3f9cb78bf7" target="_blank" rel="noopener">[SPARK-19545][YARN]Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p><p>我们仔细对比下Spark3中的代码，这段代码亦和修复前的代码一样（只是spark3中多了try-catch）</p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> logAggregationContext <span class="token operator">=</span> Records<span class="token punctuation">.</span>newRecord<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>LogAggregationContext<span class="token punctuation">]</span><span class="token punctuation">)</span>logAggregationContext<span class="token punctuation">.</span>setRolledLogsIncludePattern<span class="token punctuation">(</span>includePattern<span class="token punctuation">)</span>sparkConf<span class="token punctuation">.</span>get<span class="token punctuation">(</span>ROLLED_LOG_EXCLUDE_PATTERN<span class="token punctuation">)</span><span class="token punctuation">.</span>foreach <span class="token punctuation">{</span> excludePattern <span class="token keyword">=></span>  logAggregationContext<span class="token punctuation">.</span>setRolledLogsExcludePattern<span class="token punctuation">(</span>excludePattern<span class="token punctuation">)</span><span class="token punctuation">}</span>appContext<span class="token punctuation">.</span>setLogAggregationContext<span class="token punctuation">(</span>logAggregationContext<span class="token punctuation">)</span></code></pre><p>然后是当初修复后的代码：</p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> logAggregationContext <span class="token operator">=</span> Records<span class="token punctuation">.</span>newRecord<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>LogAggregationContext<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// These two methods were added in Hadoop 2.6.4, so we still need to use reflection to</span><span class="token comment" spellcheck="true">// avoid compile error when building against Hadoop 2.6.0 ~ 2.6.3.</span><span class="token keyword">val</span> setRolledLogsIncludePatternMethod <span class="token operator">=</span>  logAggregationContext<span class="token punctuation">.</span>getClass<span class="token punctuation">.</span>getMethod<span class="token punctuation">(</span><span class="token string">"setRolledLogsIncludePattern"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span>setRolledLogsIncludePatternMethod<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span>logAggregationContext<span class="token punctuation">,</span> includePattern<span class="token punctuation">)</span>sparkConf<span class="token punctuation">.</span>get<span class="token punctuation">(</span>ROLLED_LOG_EXCLUDE_PATTERN<span class="token punctuation">)</span><span class="token punctuation">.</span>foreach <span class="token punctuation">{</span> excludePattern <span class="token keyword">=></span>  <span class="token keyword">val</span> setRolledLogsExcludePatternMethod <span class="token operator">=</span>    logAggregationContext<span class="token punctuation">.</span>getClass<span class="token punctuation">.</span>getMethod<span class="token punctuation">(</span><span class="token string">"setRolledLogsExcludePattern"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  setRolledLogsExcludePatternMethod<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span>logAggregationContext<span class="token punctuation">,</span> excludePattern<span class="token punctuation">)</span><span class="token punctuation">}</span>appContext<span class="token punctuation">.</span>setLogAggregationContext<span class="token punctuation">(</span>logAggregationContext<span class="token punctuation">)</span></code></pre><p>所以只要把源代码改动下就ok了。</p><p>Spark在<a href="https://issues.apache.org/jira/browse/SPARK-25016" target="_blank" rel="noopener">SPARK-25016</a>中移除了对Hadoop2.6的支持，代码改动也是原来那个时候。</p><p>可能新的代码在hadoop2.7中更加高效？如果仍然需要使用hadoop2.6的集群。就只能自己修改源码然后编译了。</p>]]></content>
      
      
      <categories>
          
          <category> Spark3 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-3.0 自适应查询</title>
      <link href="/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/"/>
      <url>/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/</url>
      
        <content type="html"><![CDATA[<h1 id="Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译"><a href="#Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译" class="headerlink" title="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译"></a>Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译</h1><p>原文链接：<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" target="_blank" rel="noopener">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></p><blockquote><p>Over the years, there’s been an extensive and continuous effort to improve Spark SQL’s query optimizer and planner in order to generate high-quality query execution plans. One of the biggest improvements is the cost-based optimization framework that collects and leverages a variety of data statistics (e.g., row count, number of distinct values, NULL values, max/min values, etc.) to help Spark choose better plans. Examples of these cost-based optimization techniques include choosing the right join type (broadcast hash join vs. sort merge join), selecting the correct build side in a hash-join, or adjusting the join order in a multi-way join. However, outdated statistics and imperfect cardinality estimates can lead to suboptimal query plans. Adaptive Query Execution, new in the upcoming Apache SparkTM 3.0 release and available in the Databricks Runtime 7.0, now looks to tackle such issues by reoptimizing and adjusting query plans based on runtime statistics collected in the process of query execution.</p></blockquote><p>多年来，为了提供高质量的查询计划，Spark-SQL的query optimizer 和query planner进行了连续且大量的更新，其中最大的进步之一便是基于成本的优化框架，它收集并利用各种数据统计信息(例如，行计数、不同值的数量、空值、最大/最小值等)，来帮助Spark选择更好的计划。</p><p>这些基于成本的框架示例包括有：选择right join的类型（hash广播join vs 排序合并join），选择合适的hash-join端，调整join计划的链接顺序等。然而，过时的统计数据和不完美的基数估计可能导致不佳(次优)的查询计划。</p><p>自适应查询执行是即将发布的Apache SparkTM 3.0版本中的新增功能，该功能通过基于查询执行<strong>过程中收集的运行时统计信息</strong>，来优化并调整查询计划来解决这些问题。</p><h2 id="The-Adaptive-Query-Execution-AQE-framework"><a href="#The-Adaptive-Query-Execution-AQE-framework" class="headerlink" title="The Adaptive Query Execution (AQE) framework"></a>The Adaptive Query Execution (AQE) framework</h2><blockquote><p>One of the most important questions for Adaptive Query Execution is when to reoptimize. Spark operators are often pipelined and executed in parallel processes. However, a shuffle or broadcast exchange breaks this pipeline. We call them materialization points and use the term “query stages” to denote subsections bounded by these materialization points in a query. Each query stage materializes its intermediate result and the following stage can only proceed if all the parallel processes running the materialization have completed. This provides a natural opportunity for reoptimization, for it is when data statistics on all partitions are available and successive operations have not started yet.</p></blockquote><p>AQE中最重要的便是何时去重新优化，Spark任务通常是链式并行计算(pipelined and executed in parallel processes)，不过shuffle和广播会中断上述的链式计算，我们称之为物化点，后续文章中将使用“查询阶段”来表示查询中由这些物化点限定的子节。</p><p>每个查询阶段都需要物化其中间结果，只有在运行物化的所有并行进程都已完成时，下一个阶段才能继续。这个节点是天然的能够用来重新优化的节点，因为它有所有之前分区的统计数据，并且后续计划还未开始。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png" alt="优化逻辑"></p><blockquote><p>When the query starts, the Adaptive Query Execution framework first kicks off all the leaf stages — the stages that do not depend on any other stages. As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.</p></blockquote><p>当查询开始时，AQE框架会启动所有‘叶’阶段——不需要依赖其他阶段的阶段。一旦这些阶段完成了物化，框架就在物理查询计划中将它们标记为完成，并使用从<strong>完成阶段检索到的运行时统计信息</strong>，来相应地更新逻辑查询计划。</p><p>基于这些统计信息，框架会运行优化器(具有选定的逻辑优化规则列表)，物理规划器，物理优化规则。物理优化规则包括常规物理规则和特定于自适应执行的规则，例如合并分区、join时的数据倾斜处理等。</p><p>现在我们已经获得了一个新优化的查询计划，其中包含一些已完成的阶段。AQE框架将搜索并执行那些子阶段已经完成物化的新的查询阶段，然后重复上面的执行-重新优化-执行过程，直到完成整个查询。</p><blockquote><p>In Spark 3.0, the AQE framework is shipped with three features:</p><ul><li><p>Dynamically coalescing shuffle partitions</p></li><li><p>Dynamically switching join strategies</p></li><li><p>Dynamically optimizing skew joins</p></li></ul><p>The following sections will talk about these three features in detail.</p></blockquote><p>Spark3.0中，AQE附带了下面三个功能：</p><ul><li>动态合并shuffle分区。简化甚至避免调整 shuffle 分区的数量。</li><li>动态调整连接策略。部分避免了由于缺少统计信息或错误估计大小而导致执行次计划的情况</li><li>动态优化数据倾斜连接。解决数据倾斜的问题。</li></ul><p>接下来的章节会详细讨论这些功能：</p><h2 id="Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）"><a href="#Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）" class="headerlink" title="Dynamically coalescing shuffle partitions（动态合并shuffle分区）"></a>Dynamically coalescing shuffle partitions（动态合并shuffle分区）</h2><blockquote><p>When running queries in Spark to deal with very large data, shuffle usually has a very important impact on query performance among many other things. Shuffle is an expensive operator as it needs to move data across the network, so that data is redistributed in a way required by downstream operators.</p><p>One key property of shuffle is the number of partitions. The best number of partitions is data dependent, yet data sizes may differ vastly from stage to stage, query to query, making this number hard to tune:</p><ol><li>If there are too few partitions, then the data size of each partition may be very large, and the tasks to process these large partitions may need to spill data to disk (e.g., when sort or aggregate is involved) and, as a result, slow down the query.</li><li>If there are too many partitions, then the data size of each partition may be very small, and there will be a lot of small network data fetches to read the shuffle blocks, which can also slow down the query because of the inefficient I/O pattern. Having a large number of tasks also puts more burden on the Spark task scheduler.</li></ol></blockquote><p>使用Spark处理大数据时，shuffle是影响性能中最重要的一项。shuffle是一个昂贵的操作，因为需要经过网络传输数据，从而以后续算子需要的样子重新分配分区。</p><p>shuffle的关键属性之一是分区数量，分区的最佳数量取决于数据，但是不同阶段、不同查询的数据大小可能会有很大差异，这使得该数量很难调优。</p><ol><li>如果分区数量过小，单个分区中的数据会很大。处理这些大分区的任务时，可能需要将数据溢出到磁盘。(例如，当涉及排序或聚合时)，因此减慢了查询速度。</li><li>如果分区数量过多，单个分区中的数据会很小。这会导致将有大量小型网络数据提取来读取shuffle块，会因为低效的I/O模式而减慢查询速度。并且过多数量的任务也会给Spark任务调度带来更多负担。</li></ol><blockquote><p>To solve this problem, we can set a relatively large number of shuffle partitions at the beginning, then combine adjacent small partitions into bigger partitions at runtime by looking at the shuffle file statistics.</p><p>For example, let’s say we are running the query SELECT max(i)FROM tbl GROUP BY j. The input data tbl is rather small so there are only two partitions before grouping. The initial shuffle partition number is set to five, so after local grouping, the partially grouped data is shuffled into five partitions. Without AQE, Spark will start five tasks to do the final aggregation. However, there are three very small partitions here, and it would be a waste to start a separate task for each of them.</p><p>Instead, AQE coalesces these three small partitions into one and, as a result, the final aggregation now only needs to perform three tasks rather than five.</p></blockquote><p>为了解决这些问题。我们可以在任务开始时设置大数量的分区。然后在运行时通过查看统计数据，将相邻的小分区组合成更大的分区。</p><p>例如，当我们执行查询下述时：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token function">max</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token keyword">FROM</span> tbl <span class="token keyword">GROUP</span> <span class="token keyword">BY</span> j</code></pre><p>输入数据tbl非常小，所以在grouping前只有2个分区。如果初始的shuffle分区数量设置为5，在local grouping后，部分分区的数据会被shuffle到5个分区中。没有AQE时，Spark会启动5个任务来执行最后的聚合计算。但是这里有三个非常小的分区，为每个分区启动单独的任务将是一种浪费：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140533.png" alt="Without AQE"></p><p>不过当启用AQE时，AQE会合并这三个小的分区，所以最后的聚合只需要3个任务，而不是5个。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140648.png" alt="enable AQE"></p><h2 id="Dynamically-switching-join-strategies（动态调整连接策略）"><a href="#Dynamically-switching-join-strategies（动态调整连接策略）" class="headerlink" title="Dynamically switching join strategies（动态调整连接策略）"></a>Dynamically switching join strategies（动态调整连接策略）</h2><blockquote><p>Spark supports a number of join strategies, among which broadcast hash join is usually the most performant if one side of the join can fit well in memory. And for this reason, Spark plans a broadcast hash join if the estimated size of a join relation is lower than the broadcast-size threshold. But a number of things can make this size estimation go wrong — such as the presence of a very selective filter — or the join relation being a series of complex operators other than just a scan.</p></blockquote><p>Spark支持多种连接策略，例如：如果join的一侧可以放入内存中，则广播hash-join(broadcast hash join)通常是性能最高的。基于上述原因，Spark会在join一侧的预估数据量小于广播阈值（broadcast-size threshold）时，Spark会采用broadcast hash join。不过，很多事情可能会导致这种大小估计出错——例如，存在一个非常有选择性的过滤器、或者连接侧存在一系列复杂的运算符，而不是只有一次扫描（scan）。</p><blockquote><p>简单来说，就是Spark估算Join两侧数据量大小不准确时导致的，AQE能够动态基于之前物化的数据来调整连接策略，也就减少了这种统计出错带来的影响。</p></blockquote><blockquote><p>To solve this problem, AQE now replans the join strategy at runtime based on the most accurate join relation size. As can be seen in the following example, the right side of the join is found to be way smaller than the estimate and also small enough to be broadcast, so after the AQE reoptimization the statically planned sort merge join is now converted to a broadcast hash join.</p><p>For the broadcast hash join converted at runtime, we may further optimize the regular shuffle to a localized shuffle (i.e., shuffle that reads on a per mapper basis instead of a per reducer basis) to reduce the network traffic.</p></blockquote><p>为了解决这个问题，AQE会根据最准确的连接关系大小，在运行时重新规划join策略。下面的示例中展示了这种情况。</p><p>参与join的右侧的数据的实际数据量(8M)，小于它的预估值(15M)。实际的数据量起始能够通过广播写入内存中，所以在AQE的重新调整后，计划从sort merge join调整为了 broadcast hash join。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622141835.png" alt="Dynamically switching join strategies"></p><p>对于运行时转换的broadcast hash join，我们能够进一步的将常规的shuffle调整为局部shuffle来减少网络传输（例如：基于mapper来读取shuffle，而不是基于reducer）</p><h2 id="Dynamically-optimizing-skew-joins"><a href="#Dynamically-optimizing-skew-joins" class="headerlink" title="Dynamically optimizing skew joins"></a>Dynamically optimizing skew joins</h2><blockquote><p>Data skew occurs when data is unevenly distributed among partitions in the cluster. Severe skew can significantly downgrade query performance, especially with joins. AQE skew join optimization detects such skew automatically from shuffle file statistics. It then splits the skewed partitions into smaller subpartitions, which will be joined to the corresponding partition from the other side respectively.</p></blockquote><p>当数据在集群中分布不均时就会出现数据倾斜（比如某个key的数据特别多时）。严重的偏斜可能会显著降低查询性能——特别是使用join时。AQE能够自动从shuffle 文件的统计中自动检测此类倾斜。之后，它会将倾斜的分区拆分成更小的子分区，这些子分区会连接上另一侧的与其匹配的分区。</p><blockquote><p>Let’s take this example of table A join table B, in which table A has a partition A0 significantly bigger than its other partitions.</p><p>The skew join optimization will thus split partition A0 into two subpartitions and join each of them to the corresponding partition B0 of table B.</p><p>Without this optimization, there would be four tasks running the sort merge join with one task taking a much longer time. After this optimization, there will be five tasks running the join, but each task will take roughly the same amount of time, resulting in an overall better performance.</p></blockquote><p>下述示例是TableA join TableB，其中，TableA中的分区A0明显大于其他分区。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143417.png" alt="优化前"></p><p>因此，AQE将分区A0拆分成两个子分区，并将它们中的每一个子分区，连接到表B的相应分区B0。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143433.png" alt="优化后"></p><p>如果没有优化器，这个Join会启动4个Task来执行，并且其中一个Task会耗费非常长的时间(A0对应的任务)。</p><p>在优化之后，这个Join会启动5个Task来执行，但每个任务都会耗费几乎一样的时间。最终的结果表现会更好。</p><h2 id="TPC-DS-performance-gains-from-AQE（性能提升）"><a href="#TPC-DS-performance-gains-from-AQE（性能提升）" class="headerlink" title="TPC-DS performance gains from AQE（性能提升）"></a>TPC-DS performance gains from AQE（性能提升）</h2><blockquote><p>In our experiments using TPC-DS data and queries, Adaptive Query Execution yielded up to an 8x speedup in query performance and 32 queries had more than 1.1x speedup Below is a chart of the 10 TPC-DS queries having the most performance improvement by AQE.</p><p>Most of these improvements have come from dynamic partition coalescing and dynamic join strategy switching since randomly generated TPC-DS data do not have skew. Yet we’ve seen even greater improvements in production workload in which all three features of AQE are leveraged.</p></blockquote><p>在我们使用TPC-DS数据和查询的实验中，AQE在查询性能方面获得了高达8倍的提升，32个查询的提升超过了1.1倍。下面是通过AQE获得最大性能提升的10个TPC-DS查询的图表。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143919.png" alt="性能提升"></p><p>这些改进大多来自动态分区合并（Dynamically coalescing shuffle partitions）和动态连接策略切换（Dynamically switching join strategies），因为随机生成的TPC-DS数据一般不存在数据倾斜的问题。</p><p>然而，我们已经看到，在利用了AQE的所有三个特性之后，生产工作得到了很大的改善。</p><h2 id="Enabling-AQE（启用AQE）"><a href="#Enabling-AQE（启用AQE）" class="headerlink" title="Enabling AQE（启用AQE）"></a>Enabling AQE（启用AQE）</h2><blockquote><p>AQE can be enabled by setting SQL config spark.sql.adaptive.enabled to true (default false in Spark 3.0), and applies if the query meets the following criteria:</p><ul><li>It is not a streaming query</li><li>It contains at least one exchange (usually when there’s a join, aggregate or window operator) or one subquery</li></ul></blockquote><p>通过将配置<code>spark.sql.adaptive.enabled</code>设置为<code>true</code>，AQE能在Spark3.0中启用，（在Spark3.0中默认是关闭的）。AQE有如下限制：</p><ul><li>不能是流式查询</li><li>至少需要一次数据交换（比如使用join、聚合、窗口函数时会出现）、或者存在一次子查询。</li></ul><blockquote><p>By making query optimization less dependent on static statistics, AQE has solved one of the greatest struggles of Spark cost-based optimization — the balance between the stats collection overhead and the estimation accuracy. To achieve the best estimation accuracy and planning outcome, it is usually required to maintain detailed, up-to-date statistics and some of them are expensive to collect, such as column histograms, which can be used to improve selectivity and cardinality estimation or to detect data skew. AQE has largely eliminated the need for such statistics as well as for the manual tuning effort. On top of that, AQE has also made SQL query optimization more resilient to the presence of arbitrary UDFs and unpredictable data set changes, e.g., sudden increase or decrease in data size, frequent and random data skew, etc. There’s no need to “know” your data in advance any more. AQE will figure out the data and improve the query plan as the query runs, increasing query performance for faster analytics and system performance.</p></blockquote><p>通过减少查询优化对静态统计数据的依赖，AQE解决了基于Spark Cost的优化的最大难题之一 — 统计数据的开销和评估精度之间的平衡。为了达到最佳的估计精度和规划结果，通常需要维护详细的最新统计数据，其中一些统计数据的收集成本很高，就例如柱状图，其可用于提高选择性、基数估计、检测数据倾斜。</p><p>AQE在很大程度上消除了对此类信息的统计，以及手动调优工作的需要。最重要的是，AQE还使SQL查询优化对任意UDF的不可预测的数据集更改具有更强的弹性，例如：数据大小的突然增加或减少、频繁和随机的数据歪斜等。</p><p>现在不再需要事先“了解”您的数据。AQE将在查询运行时找出数据并改进查询计划，从而提高查询性能以实现更快的分析和系统性能。</p>]]></content>
      
      
      <categories>
          
          <category> Spark3 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-3.0 动态分区裁剪</title>
      <link href="/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/"/>
      <url>/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark中的静态分区裁剪"><a href="#Spark中的静态分区裁剪" class="headerlink" title="Spark中的静态分区裁剪"></a>Spark中的静态分区裁剪</h2><p>用过Spark的都知道，其实简单来说就是谓词下推。在Spark执行下述查询时，能够尽可能将谓词下推至扫描文件的阶段。从而减少读取的数据量，实现处理速度的提升：</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> Sales <span class="token keyword">WHERE</span> day_of_week <span class="token operator">=</span> ‘Mon’</code></pre><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150635.png" alt="分区裁剪"></p><h2 id="Spark3-0中的动态分区裁剪"><a href="#Spark3-0中的动态分区裁剪" class="headerlink" title="Spark3.0中的动态分区裁剪"></a>Spark3.0中的动态分区裁剪</h2><p>所谓的<strong>动态分区裁剪</strong>就是基于运行时（run time）推断出来的信息来进一步进行分区裁剪。</p><p>在Join时，如果我们只需要一部分DIM Table中的数据，静态分区裁剪能够将这部分裁剪下推下去。</p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150941.png" style="zoom:50%;" /><p>动态分区裁剪则更进一步，会将这部分裁剪作用到FACT Table前，然后再进行Join。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622151112.png" alt="动态分区裁剪"></p><h2 id="开启动态分区裁剪"><a href="#开启动态分区裁剪" class="headerlink" title="开启动态分区裁剪"></a>开启动态分区裁剪</h2><p>开启动态分区裁剪：</p><p><code>spark.sql.optimizer.dynamicPartitionPruning.enabled</code>设置为<code>true</code>(默认)</p><p>其他参数：</p><p><code>spark.sql.optimizer.dynamicPartitionPruning.useStats</code>，<code>true</code>(默认)：</p><ul><li><p>在动态分区修剪后，将使用DISTINCT COUNT统计信息计算分区表的数据大小，以便在广播重用不适用的情况下，评估是否值得添加额外的子查询作为裁剪筛选器。</p></li><li><blockquote><p>When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p></blockquote></li></ul><p><code>spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio</code>，<code>0.5</code>(默认)：</p><ul><li><p>当统计数据不可用或配置没有使用时，此配置将用作计算动态分区修剪后分区表数据大小的后备筛选器比率，以便在广播重用不适用的情况下评估是否值得添加额外的子查询作为裁剪筛选器。</p></li><li><blockquote><p>When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p></blockquote></li></ul><p><code>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcast</code>，true(默认)：</p><ul><li><p>动态分区裁剪将寻求重用来自broadcast hash join操作的广播结果。</p></li><li><blockquote><p>When true, dynamic partition pruning will seek to reuse the broadcast results from a broadcast hash join operation.</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark3 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH中禁用kerberos</title>
      <link href="/2020/05/28/cdh-zhong-jin-yong-kerberos/"/>
      <url>/2020/05/28/cdh-zhong-jin-yong-kerberos/</url>
      
        <content type="html"><![CDATA[<p>免费版的CDH关闭kerberos会相对复杂。</p><ol><li>关闭所有组件。</li></ol><h2 id="一、操作zookeeper"><a href="#一、操作zookeeper" class="headerlink" title="一、操作zookeeper"></a>一、操作zookeeper</h2><ol><li>关闭zk的认证配置：</li></ol><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095701.png" alt=""></p><ol start="2"><li>找到zk的数据目录：<br>[][1]</li></ol><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528094904.png" alt=""></p><ol start="3"><li>删除目录下的内容：</li></ol><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095524.png" alt=""></p><ol start="4"><li>初始化zk：</li></ol><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095741.png" style="zoom:50%;" /><ol start="5"><li>确认初始化成功：</li></ol><h2 id="HDFS篇"><a href="#HDFS篇" class="headerlink" title="HDFS篇"></a>HDFS篇</h2><ol><li><p>关闭认证相关配置</p><pre><code>hadoop.security.authenticationhadoop.security.authorization</code></pre><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100134.png" alt=""></p></li><li><p>修改权限及其他配置：</p><pre><code>dfs.datanode.data.dir.perm =&gt; 755dfs.datanode.address =&gt; 50010dfs.datanode.http.address =&gt; 50070</code></pre><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100400.png" alt=""></p></li></ol><h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><ol><li>搜索auth选项，关闭即可。一般来说还需要关HBase</li></ol><h2 id="重启、核查："><a href="#重启、核查：" class="headerlink" title="重启、核查："></a>重启、核查：</h2><p>重启，</p><ol><li>核查可发现kerberos已关闭：</li></ol><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101020.png" alt=""></p><ol start="2"><li><p>核查hdfs上数据，发现都在：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101222.png" alt=""></p></li><li><p>核查zk数据，kafka数据，一切正常：</p><p>![image-20200528101328031](/Users/liushengwei/Library/Application Support/typora-user-images/image-20200528101328031.png)</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>structured里恢复应用程序</title>
      <link href="/2020/05/21/structured-li-hui-fu-ying-yong-cheng-xu/"/>
      <url>/2020/05/21/structured-li-hui-fu-ying-yong-cheng-xu/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kafka部分topic无法通过无认证端口查询的问题处理</title>
      <link href="/2020/05/20/kafka-bu-fen-topic-wu-fa-tong-guo-wu-ren-zheng-duan-kou-cha-xun-de-wen-ti-chu-li/"/>
      <url>/2020/05/20/kafka-bu-fen-topic-wu-fa-tong-guo-wu-ren-zheng-duan-kou-cha-xun-de-wen-ti-chu-li/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka部分topic无法通过无认证端口查询的问题处理"><a href="#kafka部分topic无法通过无认证端口查询的问题处理" class="headerlink" title="kafka部分topic无法通过无认证端口查询的问题处理"></a>kafka部分topic无法通过无认证端口查询的问题处理</h1><h2 id="环境概述"><a href="#环境概述" class="headerlink" title="环境概述"></a>环境概述</h2><p>公司一套XX系统，采用无认证的方式访问XX的kafka集群。另外现场环境开启了kerberos认证。</p><h2 id="情况概述"><a href="#情况概述" class="headerlink" title="情况概述"></a>情况概述</h2><p>现场由于还是部署测试环境。忽然有一天部署小哥给我说之前好好的topic，现在在页面上访问不了（页面走的无认证端口去访问的kafka），而新建的topic又能够访问。并且这些所有topic都能够通过kinit认证后的命令行访问。并且log里没有相关的报错信息。</p><h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>本身问题很简单，不过由于我是和现场小哥远程交流，现场，那个小哥也是新来的，我也刚刚接手这个项目，现场还是没用过的hw环境，走了不少弯路。</p><blockquote><p>先上结论，hw的1.1的kafka再删除topic后，acl的信息会保留。导致新建同名topic时，acl认证信息会继承。暂不清楚开源及后续版本有没有类似问题。</p></blockquote><p>心路历程：</p><p>最开始翻代码来定位问题，代码里走的都是adminclient的api，也确实走的kafka的无认证端口。</p><p>1、担心认证问题，由于我们的topic是走无认证页面建立的，随尝试kdestory后，通过producer测试下topic的权限问题。发现确实之前的topic带着认证。不过现场小哥一直说都是通过页面建立的topic。也让我很疑惑。</p><p>2、当时怀疑会不会是代码哪块把认证加进去了，因为刚刚接手还不太熟悉，所以写了个简单的java代码，让前方测试，证实代码本身没问题。</p><p>3、不过原因发现了是认证问题，且代码本身没问题，并且由于是部署环境，为了不耽误进度，表象表示今天new的topic又能正常使用，所以我让现场小哥把之前的topic删掉，重新new topic继续部署测试。</p><p>4、结果小哥反馈给我说，同一时间new的topic，有的正常，有的依旧异常。</p><p>5、最后灵光一闪，找小哥测试核对了下，确实是：依旧异常的topic与之前建立的第一次遇到问题的topic都是重名，正常的topic之前都没见过。当时以为是不是kafka没删干净。（1.1版本），随取zk和log.dir下核实。发现确实删干净了。</p><p>6、最后没包多大希望，让小哥看了下kafka-acl。最终定位问题，将kafka-acl里的topic remove掉就ok了</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink中使用cep</title>
      <link href="/2020/05/18/flink-zhong-shi-yong-cep/"/>
      <url>/2020/05/18/flink-zhong-shi-yong-cep/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink中使用CEP进行复杂计算"><a href="#Flink中使用CEP进行复杂计算" class="headerlink" title="Flink中使用CEP进行复杂计算"></a>Flink中使用CEP进行复杂计算</h1><h2 id="CEP简介："><a href="#CEP简介：" class="headerlink" title="CEP简介："></a>CEP简介：</h2><p>官网介绍: <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/cep.html" target="_blank" rel="noopener">Complex event processing for Flink</a><br>CEP主要用于流式处理过程中的数据状态转移，我们假设一个场景，我们接受的流数据有2两个字段：（id，name），利用cep，我们就可以监控上下文。</p><p>即在流数据场景中，是否存在先达成状态1，后达成状态2的情景。</p><p>简单来说，最初我们在监控id这个字段，并希望得到id=1的数据。这个时候称之为状态1。当我们确实收到了id=1的数据时，就发生了状态转移，这时候我们开始关注name=zhangsan的数据。这称为状态2。在状态2下，我们已经不关心id这个字段是几，只有当name=zhangsan时，才会出发状态2的结束，进入状态3或者完成CEP。<br>当然这是最简单的情景，更加完善的解释可以参见官网和其他博文。</p><h2 id="Flink使用CEP："><a href="#Flink使用CEP：" class="headerlink" title="Flink使用CEP："></a>Flink使用CEP：</h2><h3 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h3><ol><li><p>引入依赖：</p><pre><code> &lt;dependency&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;     &lt;artifactId&gt;flink-cep-scala_${scala.binary.version}&lt;/artifactId&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;     &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt;</code></pre></li><li><p>简单的CEP程序：</p></li></ol><pre class=" language-scala"><code class="language-scala">  <span class="token keyword">val</span> input<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>Event<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token keyword">val</span> pattern <span class="token operator">=</span> Pattern<span class="token punctuation">.</span>begin<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span>_<span class="token punctuation">.</span>getId <span class="token operator">==</span> <span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>optional<span class="token punctuation">.</span>times<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>greedy      <span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token string">"middle"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>subtype<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>SubEvent<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span>_<span class="token punctuation">.</span>name<span class="token punctuation">.</span>equals<span class="token punctuation">(</span><span class="token string">"zhangsan"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span>within<span class="token punctuation">(</span>Time<span class="token punctuation">.</span>seconds<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">val</span> patternStream <span class="token operator">=</span> CEP<span class="token punctuation">.</span>pattern<span class="token punctuation">(</span>input<span class="token punctuation">,</span> pattern<span class="token punctuation">)</span>  <span class="token keyword">val</span> result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>Alert<span class="token punctuation">]</span> <span class="token operator">=</span> patternStream<span class="token punctuation">.</span>process<span class="token punctuation">(</span>      <span class="token keyword">new</span> PatternProcessFunction<span class="token punctuation">[</span>Event<span class="token punctuation">,</span> Alert<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">override</span> <span class="token keyword">def</span> processMatch<span class="token punctuation">(</span>            `<span class="token keyword">match</span>`<span class="token operator">:</span> util<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> util<span class="token punctuation">.</span>List<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            ctx<span class="token operator">:</span> PatternProcessFunction<span class="token punctuation">.</span>Context<span class="token punctuation">,</span>            out<span class="token operator">:</span> Collector<span class="token punctuation">[</span>Alert<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>                out<span class="token punctuation">.</span>collect<span class="token punctuation">(</span>createAlertFrom<span class="token punctuation">(</span>pattern<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>  pattern是CEP最重要的地方，这里定义了如何对Event进行处理。示例中，定义了第一个状态为start，匹配条件是id=42。其中：</p><ul><li><p>Next/followedBy表示转换成下一状态。</p></li><li><p>where表示状态判定</p></li><li><p>subtype的作用是进行类型校验。SubEvent需要时Event的子类。</p></li><li><p>times表示需要匹配上n次才进入下一阶段。</p></li><li><p>within表示需要在n时间内完成所有状态，否则是无效的。</p></li><li><p>optional表示这个匹配条件是可选的。即便没匹配也会进入下一阶段的匹配。</p></li><li><p>greedy表示贪婪，即可以无视当前状态进行匹配。greedy需要跟在times后面，表示取最后n次匹配到的。</p><p>实际计算时，我们将input输入和pattern捆绑在一起。</p><p>最终，在processMatch中就是在我们完成所有状态匹配后出发，match: util.Map[String, util.List[Event]]里保存了所有匹配上的结果，其中key是state的名字，value是匹配上的值。需要注意，如果启用了optional，没匹配的情况下不会存在value中。</p></li></ul><h3 id="定义模式"><a href="#定义模式" class="headerlink" title="定义模式"></a>定义模式</h3><ol><li><p>模式间的联系：</p><p>主要分为三种：严格连续性（next/notNext），宽松连续性（followedBy/notFollowedBy），和非确定宽松连续性（followedByAny）。</p><ul><li>严格连续性：需要消息的顺序到达与模式完全一致。</li><li>宽松连续性：允许忽略不匹配的事件。</li><li>非确定宽松连性：不仅可以忽略不匹配的事件，也可以忽略已经匹配的事件。</li></ul></li><li><p>模式属性：</p><p>正如示例中，分为循环模式times和可选属性。</p><p>循环属性可以定义模式匹配发生固定次数（<strong>times</strong>），匹配发生一次以上（<strong>oneOrMore</strong>），匹配发生多次以上。(<strong>timesOrMore</strong>)。</p><p>可选属性可以设置模式是贪婪的（<strong>greedy</strong>），即匹配最长的串，或设置为可选的（<strong>optional</strong>），有则匹配，无则忽略。</p></li><li><p>模式有效期：</p><p>通过within设置一个全局有效区，防止数据过度缓存。</p></li><li><p>多模式组合：</p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">val</span> start<span class="token operator">:</span> Pattern<span class="token punctuation">[</span>Event<span class="token punctuation">,</span> _<span class="token punctuation">]</span> <span class="token operator">=</span> Pattern<span class="token punctuation">.</span>begin<span class="token punctuation">(</span>    Pattern<span class="token punctuation">.</span>begin<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">.</span>followedBy<span class="token punctuation">(</span><span class="token string">"start_middle"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>可以将pattern如此进行复用</p></li><li><p>处理结果：</p><p>处理匹配的结果主要有四个接口： PatternFlatSelectFunction，PatternSelectFunction，PatternFlatTimeoutFunction和PatternTimeoutFunction。</p><p>输出可以分为两类：</p><ul><li>select和flatSelect指定输出一条还是多条</li><li>timeoutFunction和不带timeout的Function指定可不可以对超时事件进行旁路输出。</li><li></li></ul></li><li><p>关于PatternStream</p><p>Stream和Pattern组合会得到PatternStream，其下主要有3类方法：process，select，flatSelect。以接受函数适应不同的处理需求，所有函数会在完成匹配后触发。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>structured里使用state计算</title>
      <link href="/2020/05/15/structured-li-shi-yong-state-ji-suan/"/>
      <url>/2020/05/15/structured-li-shi-yong-state-ji-suan/</url>
      
        <content type="html"><![CDATA[<h1 id="structured里使用state计算"><a href="#structured里使用state计算" class="headerlink" title="structured里使用state计算"></a>structured里使用state计算</h1><p>structured主要提供了mapGroupsWithState和flatMapGroupsWithState来进行有状态计算，以mapGroupsWithState为例，其主要接受：</p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">def</span> mapGroupsWithState<span class="token punctuation">[</span>S<span class="token operator">:</span> Encoder<span class="token punctuation">,</span> U<span class="token operator">:</span> Encoder<span class="token punctuation">]</span><span class="token punctuation">(</span>      timeoutConf<span class="token operator">:</span> GroupStateTimeout<span class="token punctuation">)</span><span class="token punctuation">(</span>      func<span class="token operator">:</span> <span class="token punctuation">(</span>K<span class="token punctuation">,</span> Iterator<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">,</span> GroupState<span class="token punctuation">[</span>S<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">=></span> U<span class="token punctuation">)</span><span class="token operator">:</span> Dataset<span class="token punctuation">[</span>U<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span></code></pre><p>对于func来说：</p><ul><li>K：Key</li><li>Iterator[V]：Value的迭代器</li><li>GroupState[S]：现在的State</li><li>U：输出的类型。</li></ul><h2 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h2><p>在使用[map|flatMap]GroupsWithState是，可以通过GroupStateTimeout完成对与超时数据的设置。</p><pre class=" language-scala"><code class="language-scala"><span class="token punctuation">.</span>mapGroupsWithState<span class="token punctuation">(</span>GroupStateTimeout<span class="token punctuation">.</span>EventTimeTimeout<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>updateAcrossEvents<span class="token punctuation">)</span></code></pre><p>几个注意点：</p><ol><li><p>GroupStateTimeout设置的参数类型对整个GroupState都有效。但具体的值设置是当group使用setTimeout方法时确定的。</p></li><li><p>对于ProcessingTimeTimeout，可以通过<strong>GroupState.setTimeoutDuration</strong>设置超时值。如果超时时间设置为Dms：</p><ul><li>时钟时间增加Dms前，超时不会发生。</li><li>超时时间没有严格的上限限制，<strong>即如果数据流中没有任何数据，就没有出发超时的机会，知道有数据时才会触发超时处理。</strong></li></ul></li><li><p>对于EventTimeTimeout，需要设置Dataset.withWatermark()，这样，超时的事件时间会被直接过滤。超时值可以通过<strong>GroupState.setTimeoutTimestamp()</strong>来设置。setTimeoutTimestamp有两类版本，一类是设定一个固定时间戳，另一类是在一个指定的时间戳上再指定一个duration：</p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">def</span> setTimeoutTimestamp<span class="token punctuation">(</span>timestampMs<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span><span class="token keyword">def</span> setTimeoutTimestamp<span class="token punctuation">(</span>timestampMs<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">,</span> additionalDuration<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span></code></pre></li></ol><p>   对于事件时间，是否超时将有两个因素控制：水位线和timeout设置：</p><ul><li>当水印时间大于timeout设置时，timeout永远不会发生。</li><li>与ProcessingTimeout一样，<strong>超时时间没有严格的上限限制</strong>。</li></ul><ol start="4"><li>在func被调用时，timeout会被重置。即当有新数据到达，或发生超时时。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>manage sparkTask in code</title>
      <link href="/2020/05/14/manage-sparktask-in-code/"/>
      <url>/2020/05/14/manage-sparktask-in-code/</url>
      
        <content type="html"><![CDATA[<h1 id="Manage-SparkTask-in-code"><a href="#Manage-SparkTask-in-code" class="headerlink" title="Manage SparkTask in code"></a>Manage SparkTask in code</h1><h2 id="submit-Spark"><a href="#submit-Spark" class="headerlink" title="submit Spark"></a>submit Spark</h2><ol><li>Get SparkLauncher:</li></ol><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// init SparkLauncher</span>SparkLauncher sparkLauncher <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkLauncher</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// set SparkLauncher</span>sparkLauncher<span class="token punctuation">.</span><span class="token function">setConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setXXX</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span><span class="token keyword">return</span> sparkLauncher<span class="token punctuation">;</span></code></pre><ol start="2"><li>Start SparkTask, get SparkAppHandle:</li></ol><pre class=" language-java"><code class="language-java">SparkAppHandle handle <span class="token operator">=</span> sparkLauncher<span class="token punctuation">.</span><span class="token function">startApplication</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><ol><li>(optional) 如果使用yarn的cluster模式，在Saprk任务running后，需要切断当前机器与AM的联系，这样可以释放一些资源。</li></ol><pre><code>String appIdOnYarn = handle.getAppId();    if (appIdOnYarn != null) {    handle.disconnect();    handle.kill();}    // 切段后，需要在我们本地管理的中将handle职位null，这有助于之后判断如何去管理这个spark任务。</code></pre><h2 id="stop-Spark"><a href="#stop-Spark" class="headerlink" title="stop Spark"></a>stop Spark</h2><h3 id="Task-have-not-submit-success"><a href="#Task-have-not-submit-success" class="headerlink" title="Task have not submit success."></a>Task have not submit success.</h3><p>use SparkHandle to Stop:</p><pre><code>// get SparkHandle , eg. manage in a map: runningApplications.runningApplications.get(appId).getSparkAppHandle().kill();</code></pre><h3 id="Task-on-Yarn"><a href="#Task-on-Yarn" class="headerlink" title="Task on Yarn"></a>Task on Yarn</h3><p>use YarnClient to Stop:</p><pre><code>// 0. 之前操作有拿到appIdOnYarnString appIdOnYarn = handle.getAppId();// 1. 拼接。为上面拿到的appIdOnYarn增加前缀，并生成ApplicationIdint pos1 = &quot;application_&quot;.length() - 1;int pos2 = appIdStr.indexOf(95, pos1 + 1);if (pos2 &lt; 0) {    throw new IllegalArgumentException(&quot;Invalid ApplicationId: &quot; + appIdStr);} else {    long rmId = Long.parseLong(appIdStr.substring(pos1 + 1, pos2));    int appId = Integer.parseInt(appIdStr.substring(pos2 + 1));    return ApplicationId.newInstance(rmId, appId); =&gt; ApplicationIdInstance}// 3. Stop:yarnClient.killApplication(ApplicationIdInstance);</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kerberos on CDH</title>
      <link href="/2020/04/27/kerberos-on-cdh/"/>
      <url>/2020/04/27/kerberos-on-cdh/</url>
      
        <content type="html"><![CDATA[<h2 id="为CDH提供kerberos认证："><a href="#为CDH提供kerberos认证：" class="headerlink" title="为CDH提供kerberos认证："></a>为CDH提供kerberos认证：</h2><h3 id="0-准备工作："><a href="#0-准备工作：" class="headerlink" title="0.准备工作："></a>0.准备工作：</h3><ol><li>部署JDK、CDH。网上文章很多，暂略。</li><li>安装NTP保证时间同步。网上文章很多，暂略。</li><li>修改hosts，指定域名。.localdomain 就是域名，之后会使用到。<pre><code> 192.168.xx.xx hadoop       hadoop.localdomain</code></pre></li></ol><h3 id="1-kerberos安装与配置："><a href="#1-kerberos安装与配置：" class="headerlink" title="1.kerberos安装与配置："></a>1.kerberos安装与配置：</h3><h4 id="1-1-kdc-server-安装"><a href="#1-1-kdc-server-安装" class="headerlink" title="1.1 kdc server 安装"></a>1.1 kdc server 安装</h4><p>选择一台机器作为kdc server。其他机器就是client。<br>这里用的是MIT，这是官网：<a href="https://web.mit.edu/kerberos/" target="_blank" rel="noopener">Kerberos: The Network Authentication Protocol</a><br>centos貌似会自带，没有带用yum安装一下就好</p><pre><code>yum -y install krb5-server krb5-libs__</code></pre><blockquote><p>tips：生产上一般建议单独提供一台安装kdc server的机器。这边是测试所以会把cm和kdc server装一台机器上</p></blockquote><h4 id="1-2-kdc-server配置"><a href="#1-2-kdc-server配置" class="headerlink" title="1.2 kdc server配置"></a>1.2 kdc server配置</h4><h5 id="1-2-1-配置-etc-krb5-conf，下面是一个示例："><a href="#1-2-1-配置-etc-krb5-conf，下面是一个示例：" class="headerlink" title="1.2.1 配置/etc/krb5.conf，下面是一个示例："></a>1.2.1 配置/etc/krb5.conf，下面是一个示例：</h5><pre><code>[libdefaults]    default_realm = LOCALDOMAIN    dns_lookup_realm = false    dns_lookup_kdc = false    ticket_lifetime = 24h    forwardable = true    udp_preference_limit = 1000000[realms]    LOCALDOMAIN = {        kdc = hadoop        admin_server = hadoop    }[domain_realm]    .localdomain = LOCALDOMAIN    localdomain = LOCALDOMAIN[logging]    kdc = FILE:/var/log/krb5kdc.log    admin_server = FILE:/var/log/kadmin.log    default = FILE:/var/log/krb5lib.log</code></pre><p>几个注意点：</p><blockquote><p>[realms]里的hadoop ，配置成自己的hostname。后面可以跟域名也可以不跟。<br>注意域名LOCALDOMAIN要大写（domain_realm左边要小写）。最好和准备里hosts里的一样。不一样没测试过。</p></blockquote><h5 id="1-2-2-配置-var-kerberos-krb5kdc-kdc-conf，下面是一个示例："><a href="#1-2-2-配置-var-kerberos-krb5kdc-kdc-conf，下面是一个示例：" class="headerlink" title="1.2.2 配置/var/kerberos/krb5kdc/kdc.conf，下面是一个示例："></a>1.2.2 配置/var/kerberos/krb5kdc/kdc.conf，下面是一个示例：</h5><pre><code>default_realm = LOCALDOMAIN[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] LOCALDOMAIN = {  acl_file = /var/kerberos/krb5kdc/kadm5.acl  dict_file = /usr/share/dict/words  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab  supported_enctypes = aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal }</code></pre><p>注意点：</p><blockquote><p>supported_enctypes这一项，默认包含aes256-cts。这个需要在server和client上额外安装。本次示例没使用，直接删去该项。如果需要，可参考：</p><pre><code>sudo wget http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zipsudo unzip jce_policy-8.zipsudo mv UnlimitedJCEPolicyJDK8/*.jar $JAVA_HOME/jre/lib/security pscp -h list_krb_clients $JAVA_HOME/jre/lib/security/US_export_policy.jar /tmp pscp -h list_krb_clients $JAVA_HOME/jre/lib/security/local_policy.jar /tmp pssh -h list -l admin -A &quot;sudo cp /tmp/US_export_policy.jar $JAVA_HOME/jre/lib/security/&quot; pssh -h list -l admin -A &quot;sudo cp /tmp/local_policy.jar $JAVA_HOME/jre/lib/security/&quot;</code></pre></blockquote><h5 id="1-2-3-配置-var-kerberos-krb5kdc-kadm5-acl"><a href="#1-2-3-配置-var-kerberos-krb5kdc-kadm5-acl" class="headerlink" title="1.2.3 配置/var/kerberos/krb5kdc/kadm5.acl"></a>1.2.3 配置/var/kerberos/krb5kdc/kadm5.acl</h5><pre><code>*/admin@LOCALDOMAIN        *</code></pre><h5 id="1-2-4-为kerberos创建数据库"><a href="#1-2-4-为kerberos创建数据库" class="headerlink" title="1.2.4 为kerberos创建数据库"></a>1.2.4 为kerberos创建数据库</h5><ol><li><p>创建数据库：</p><pre><code> kdb5_util create -r CW.COM -s</code></pre><p> 这段代码含义是：-r 指定realm，简单来说就是本例的LOCALDOMAIN。-s表示会给master key在stash file中创建一份copy。<br> 这一步会要求为这个数据库设置密码。</p></li><li><p>创建用户：</p><pre><code> [root@kdc ~]# kadmin.local kadmin.local:  addprinc root/admin  &lt;= 创建一个root/admin用户。接着会让你设置密码 kadmin.local:  ktadd -k /var/kerberos/krb5kdc/root.keytab root/admin  &lt;= 将该用户写入keytab，可选。 kadmin.local:  exit</code></pre></li><li><p>启动服务，并设置自启：</p><pre><code> systemctl start krb5kdc.service systemctl start kadmin.service systemctl enable krb5kdc.service systemctl enable kadmin.service</code></pre></li></ol><h5 id="1-2-5-测试："><a href="#1-2-5-测试：" class="headerlink" title="1.2.5 测试："></a>1.2.5 测试：</h5><pre><code>kinit root/admin  &lt;= 刚刚创建的，回车输入密码。kinit -kt /var/kerberos/krb5kdc/root.keytab root/admin@LOCALDOMAIN &lt;= 使用keytab登陆。密码已存在keytab中。klist  &lt;= 查看当前认证的用户</code></pre><h4 id="1-3-client安装："><a href="#1-3-client安装：" class="headerlink" title="1.3 client安装："></a>1.3 client安装：</h4><h5 id="1-3-1-所有client安装："><a href="#1-3-1-所有client安装：" class="headerlink" title="1.3.1 所有client安装："></a>1.3.1 所有client安装：</h5><pre><code>yum -y install krb5-workstation krb5-libs</code></pre><p>然后将kdc server的/etc/krb5.conf 分发到所有集群上。替换他们的/etc/krb5.conf。</p><pre><code>scp /etc/krb5.conf hadoop002:/etc/</code></pre><h5 id="1-3-2-cm所在节点额外安装："><a href="#1-3-2-cm所在节点额外安装：" class="headerlink" title="1.3.2 cm所在节点额外安装："></a>1.3.2 cm所在节点额外安装：</h5><pre><code>yum -y install openldap-clients</code></pre><h2 id="2-kerberos-on-cdh"><a href="#2-kerberos-on-cdh" class="headerlink" title="2.kerberos on cdh"></a>2.kerberos on cdh</h2><h3 id="2-1-为cm创建账号："><a href="#2-1-为cm创建账号：" class="headerlink" title="2.1 为cm创建账号："></a>2.1 为cm创建账号：</h3><p>和1.2.4中一样，记住这步设置的密码：</p><pre><code>[root@kdc ~]# kadmin.localAuthenticating as principal root/admin@LOCALDOMAIN with password.    kadmin.local:  addprinc cloudera-scm/admin@LOCALDOMAINkadmin.local:  exit</code></pre><h3 id="2-2-cm启用kerberos："><a href="#2-2-cm启用kerberos：" class="headerlink" title="2.2 cm启用kerberos："></a>2.2 cm启用kerberos：</h3><blockquote><p>主要是界面操作。这一步的坑主要是第四步的加密算法。</p></blockquote><ol><li><p>点击Administration -&gt; security -&gt; kerberos -&gt; enable</p></li><li><p>阅读一些提示信息，没问题就下一步。(基本就是让你确认kerberos是否装好了)</p><ul><li>KDC已经安装好并且正在运行</li><li>将KDC配置为允许renewable tickets with non-zerolifetime，我们在之前修改kdc.conf文件的时候已经添加了max_life和max_renewable_life这个2个属性，前者表示服务端允许的Service ticket最大生命周期，后者表示服务端允许的Service ticket更新周期。这2个属性必须分别大于等于客户端对应的配置ticket_lifetime和renew_lifetime。我们假设，不这样进行配置：ticket_lifetime = 8d, max_life = 7d, renew_lifetime = 25h, max_renew_life = 24h，那么可能造成的结果就是当service持有的票据超过24小时没有去更新，在第24.5小时的时候去进行更新，请求会遭到拒绝，报错：Ticket expired while renewing credentials，永远无法进行正常更新。对于Cloudera来说，因为更新机制被透明(Cloudera有renew进程会去定期更新)，即使我们手动使用<code>modprinc -maxrenewlife 1week krbtgt/DOMAIN.COM@DOMAIN.COM</code> 进行更新，也无济于事。</li><li>在Cloudera Manager Server上安装openldap-clients；</li><li>为Cloudera Manager创建了超级管理员principal，使其能够有权限在KDC中创建其他的principals；</li></ul></li><li><p>配置kdc信息：</p><ul><li>注意填写的内容与krb5.conf里一一对应。</li><li>这里的 Kerberos Encryption Types 一定要选择之前krb5.conf或/var/kerberos/krb5kdc/kdc.conf里配置有的。不然可能会报错：<pre><code>KDC has no support for encryption type while getting initial credentials</code></pre></li></ul></li><li><p>尽量不让cdh管理krb5.conf：</p></li><li><p>后面一路continue即可。</p></li></ol><p>这一步cdh实际会进行如下操作：</p><pre><code>1. 集群中有多少个节点，每个账户就会生成对应个数的 principal ;2. 为每个对应的 principal 创建 keytab；3. 部署 keytab 文件到指定的节点中；4. 在每个服务的配置文件中加入有关 Kerberos 的配置；</code></pre><p>到这里，其实集群就已经完成kerberos认证了。不过真正要使用，还需要些其他操作。</p><h3 id="2-3-创建我们使用的认证账户"><a href="#2-3-创建我们使用的认证账户" class="headerlink" title="2.3 创建我们使用的认证账户"></a>2.3 创建我们使用的认证账户</h3><p>之前，我们使用hdfs，都会使用cdh提供的hdfs账户。不过完成kerberos认证后，hdfs账户不能直接使用了，而cdh创建的我们又不知道密码。所以我们可以为其在kerberos生成一个key：</p><pre><code>[root@kdc ~]# kadmin.localAuthenticating as principal root/admin@LOCALDOMAIN with password.    kadmin.local:  addprinc hdfs/test@LOCALDOMAIN  &lt;= 保证/前是hdfs即可kadmin.local:  exit</code></pre><p>之后通过kinit登陆该认证，就可以以hdfs用户的身份操作cdh了。</p><h2 id="3-测试及常见问题："><a href="#3-测试及常见问题：" class="headerlink" title="3.测试及常见问题："></a>3.测试及常见问题：</h2><h3 id="3-1-基本测试："><a href="#3-1-基本测试：" class="headerlink" title="3.1 基本测试："></a>3.1 基本测试：</h3><ol><li><p>登陆：</p><pre><code> kadmin.local &lt;= kdc可以直接使用kadmin.local登陆。client需要先kinit认证后，使用kadmin登陆。 klist &lt;= 登陆后使用klist列出当前的认证账户。</code></pre></li><li><p>hdfs使用：</p><pre><code> kinit后可以直接使用hadoop fs -ls / 查看hdfs的情况。 kdestory后将无法使用hadoop fs -ls / 查看hdfs的情况。</code></pre></li><li><p>运行mr/spark任务：</p><blockquote><p>spark需要额外设置才能使用,点一点界面就能完成很简单,参考：<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/sg_spark_auth.html" target="_blank" rel="noopener">Spark Authentication</a></p></blockquote><pre><code> hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-examples.jar pi 10 10000 spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster $SPARK_HOME/lib/spark-examples.jar 10</code></pre></li><li><p>使用yarn查看任务log：</p><pre><code> yarn logs -applicationId &lt;ID&gt;</code></pre><p> 注意查看log的认证账户要与提交的用户一致，否则看不到log</p><h3 id="3-2-常见问题："><a href="#3-2-常见问题：" class="headerlink" title="3.2 常见问题："></a>3.2 常见问题：</h3></li><li><p>运行mr/spark任务时提示没有用户：<br> 问题：</p><pre><code> Application application_1527494654301_0004 failed 2 times due to AM Container for appattempt_1527494654301_0004_000002 exited with exitCode: -1000For more detailed output, check application tracking page:http://fetch-master:8088/proxy/application_1527494654301_0004/Then, click on links to logs of each attempt.Diagnostics: Application application_1527494654301_0004 initialization failed (exitCode=255) with output: main : command provided 0main : run as user is adminmain : requested hdfs user is adminUser admin not foundFailing this attempt. Failing the application.</code></pre><p> 解决：<br> 参考2.3，创建需要的用户认证。这里就是创建一个admin@xxx</p></li><li><p>运行mr/spark任务时提示id异常：<br> 问题：</p><pre><code> Requested user hdfs is not whitelisted and has id 488,which is below the minimum allowed 1000</code></pre><p> 解决：</p><ol><li>修改用户id。</li><li>或修改Clouder关于这个该项的设置 ：ARN -&gt; NodeManager -&gt; Security -&gt; min.user.id改为0</li></ol></li><li><p>运行mr/spark任务时提示用户被禁用：<br> 问题：</p><pre><code> Requested user hdfs is banned</code></pre><p> 解决：<br> 在CM yarn-&gt; Configuration页面，配置禁止的系统用户banned.users列表。</p></li><li><p>找不到yarn log<br> 问题：</p><pre><code> Can not find the logs for the application with the appOwner: hdfs</code></pre><p> 解决：<br> 用运行这个任务的认证查看yarn log</p></li><li><p>没有账号</p><p>问题：</p><pre><code>Client &#39;USERNAME-REDACTED&#39; not found in Kerberos database while getting initial credentials</code></pre><p>解决：</p><p>创建账号，也可能是账号填错了</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聊聊spark-submit的常用参数</title>
      <link href="/2020/02/24/liao-liao-submit-de-chang-yong-can-shu/"/>
      <url>/2020/02/24/liao-liao-submit-de-chang-yong-can-shu/</url>
      
        <content type="html"><![CDATA[<p>在使用spark-submit在yarn跑应用程序时，我们除了设置参数，一般还会附加jar包，file文件等。这些东西实际去哪了？<br>答案就是我们配置的<code>yarn.nodemanager.local-dirs</code>tmp目录。</p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;    &lt;value&gt;/home/hadoop/data/tmp/nm-local-dir&lt;/value&gt;&lt;/property&gt;</code></pre><h3 id="这些文件会放到这里？"><a href="#这些文件会放到这里？" class="headerlink" title="这些文件会放到这里？"></a>这些文件会放到这里？</h3><p>其实这个问题只需要简单实验下，我们可以写一个sleep很长的程序，也可以直接使用spark提供的示例程序。<br>我们现在本地单节点尝试下：</p><pre class=" language-shell"><code class="language-shell">spark-submit --master yarn --class org.apache.spark.examples.SparkPi --files scopt-2.11-3.7.0.jar --jars ~/lib/okio-1.17.2.jar spark-examples_2.11-2.4.1.jar 10000</code></pre><p>在<code>yarn.nodemanager.local-dirs</code>目录下，可以看到生成了一个以application开头的目录，一路进去进到container，可以看到我们的文件已经放在了里面。<br>// TODO 图。</p><h3 id="集群情况？"><a href="#集群情况？" class="headerlink" title="集群情况？"></a>集群情况？</h3>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的水位线在单线程和并行处理时候的问题记录</title>
      <link href="/2020/01/27/flink-de-shui-wei-xian-zai-dan-xian-cheng-he-bing-xing-chu-li-shi-hou-de-wen-ti-ji-lu/"/>
      <url>/2020/01/27/flink-de-shui-wei-xian-zai-dan-xian-cheng-he-bing-xing-chu-li-shi-hou-de-wen-ti-ji-lu/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink对于eventTime的处理是很妙的一个地方，借助水位线机制，能够帮助Flink处理延迟数据。不过在测试途中却遇到了一些有些奇怪的现象，特此记录。</p><blockquote><p>Flink版本：1.9.x</p></blockquote><h2 id="单线程下的水位线："><a href="#单线程下的水位线：" class="headerlink" title="单线程下的水位线："></a>单线程下的水位线：</h2><p>这篇博客写的很好，基本把水位线的原理讲清楚了。<br>博客地址：<a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">Flink流计算编程–watermark（水位线）简介</a><br>总结一下：</p><blockquote><p>1、在Flink中，水位线 = 最新的事件时间 - 设定的时间间隔。<br>2、当Window结束时间 &lt;= 水位线时间，且该窗口内有数据，则触发计算.<br>3、水位线到底描述了什么：事件时间&lt;水位线的数据都已经到达。未到达的数据利用Window的allowedLateness算子来特殊处理。</p></blockquote><p>不过美中不足的是其并没有提及他的测试环境是在单线程下进行的。在并行处理的情况下，水位线的表现可能和想的会不太一样。</p><h2 id="并行数据中的水位线："><a href="#并行数据中的水位线：" class="headerlink" title="并行数据中的水位线："></a>并行数据中的水位线：</h2><p>直接说结论：<br>Flink会生成多个水位线，取决于并行度。每个水位线会根据发往该分区的数据来独立维护。Window算子的计算触发只关心Long值最小的那个水位线。</p><blockquote><p>简单来说，并行度3的情况下，水位线A=1581507300000，水位线B=1581507301000，水位线C=1581507302000。此时Window结束时间1581507301000&gt;水位线A，故不会触发计算，只有当水位线A&gt;Window结束时间1581507301000，此时所有线程水位线都&gt;Window的结束时间，该Window才会触发计算。</p></blockquote><p>虽然话是这么说，但上面的结论在实际测试后发现并不是特别恰当。<br>对于并行读取的Source（Kafka）：</p><blockquote><p>对于Kafka来说，在没有其他Source的情况下，触发Window计算只关注Kafka的分区里/数的WaterMark。我们设置的整个程序的并行度如果&gt;Kakfa并行度，不会对Kafka的Window计算产生影响。小于时会。</p></blockquote><blockquote><p>从下图的黄色标记也可以看出，并行（Parallel）情况下，取决于小的那个水位线。</p></blockquote><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223730.png" alt="Kafka in WaterMark"></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中的BlockManager</title>
      <link href="/2019/11/17/spark-zhong-de-blockmanager/"/>
      <url>/2019/11/17/spark-zhong-de-blockmanager/</url>
      
        <content type="html"><![CDATA[<h1 id="一、BlockManager的用途。"><a href="#一、BlockManager的用途。" class="headerlink" title="一、BlockManager的用途。"></a>一、BlockManager的用途。</h1><p>BlockManager会运行在所有节点之上（driver+executor），管理整个Spark运行时的数据读写的。</p><blockquote><p>Manager running on every node (driver and executors) which provides interfaces for putting and retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap).</p></blockquote><h1 id="二、BlockManager架构。"><a href="#二、BlockManager架构。" class="headerlink" title="二、BlockManager架构。"></a>二、BlockManager架构。</h1><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223404.png" alt="BlockManager架构"><br>1、每个Blockmanager实例化时都会向BlockManagerMaster进行注册。（实际上是 Executor 中的 BlockManager 注册给 Driver 上的 BlockMangerMasterEndpoiont）<br>2、BlockManagerMaster通过BlockManagerInfo来对Blockmanager进行元数据管理。<br>3、当改变了具体的ExecutorBackend 上的 Block 的信息后，就要向BlockManagerMaster发送消息来更新。<br>4、BlockManager也是典型的主从架构。</p><h1 id="三、BlockManager相关源码。"><a href="#三、BlockManager相关源码。" class="headerlink" title="三、BlockManager相关源码。"></a>三、BlockManager相关源码。</h1><blockquote><p>基于Spark 2.4.x</p></blockquote><h2 id="1-SparkEnv："><a href="#1-SparkEnv：" class="headerlink" title="1. SparkEnv："></a>1. SparkEnv：</h2><p>在这里会初始化Blockmanager的实例，但目前还不能用，只是确定初始化时的一些数据。</p><pre class=" language-Java"><code class="language-Java">// NB: blockManager is not valid until initialize() is called later.val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,      serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,      blockTransferService, securityManager, numUsableCores)</code></pre><h2 id="2-SparkContext："><a href="#2-SparkContext：" class="headerlink" title="2. SparkContext："></a>2. SparkContext：</h2><p>作为App的入口，初始化SparkContext时会根据AppId初始化Driver的BlockManager：<br>SparkContext.scala：</p><pre class=" language-Java"><code class="language-Java">_env.blockManager.initialize(_applicationId)</code></pre><p>Executor的Blockmanager会在Executor上完成初始化。executor的ID会在sparkEnv初始化时就确定了。<br>Executor.scala：</p><pre class=" language-Java"><code class="language-Java">env.blockManager.initialize(conf.getAppId)</code></pre><h2 id="3-初始化时，还会一并初始化blockTransferService与shuffleClient。"><a href="#3-初始化时，还会一并初始化blockTransferService与shuffleClient。" class="headerlink" title="3. 初始化时，还会一并初始化blockTransferService与shuffleClient。"></a>3. 初始化时，还会一并初始化blockTransferService与shuffleClient。</h2><pre><code>前者负责本地的块的存取。后者负责读取shuffle的中间文件。</code></pre><blockquote><p>BlockTransferService：Initialize the transfer service by giving it the BlockDataManager that can be used to fetch local blocks or put local blocks.<br>ShuffleClient：Provides an interface for reading shuffle files, either from an Executor or external service.</p></blockquote><pre class=" language-Java"><code class="language-Java">def initialize(appId: String): Unit = {    blockTransferService.init(this)    shuffleClient.init(appId)    ....}</code></pre><h2 id="4-向BlockManagerMaster注册："><a href="#4-向BlockManagerMaster注册：" class="headerlink" title="4. 向BlockManagerMaster注册："></a>4. 向BlockManagerMaster注册：</h2><p>其本质是向 Driver 上的 BlockManagerMasterEndpoint 注册。其中slaveEndpoint负责接收Driver 中的 BlockManagerMaster 发过来的指令，如删除RDD。</p><pre class=" language-Java"><code class="language-Java">val id = BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None)val idFromMaster = master.registerBlockManager(      id,      maxOnHeapMemory,      maxOffHeapMemory,      slaveEndpoint)</code></pre><p>registerBlockManager方法会向BlockManagerMasterEndpoint 发送注册信息：</p><pre class=" language-Java"><code class="language-Java">val updatedId = driverEndpoint.askSync[BlockManagerId](      RegisterBlockManager(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint))</code></pre><p>消息会在BlockManagerMasterEndpoint 的receiveAndReply方法进行匹配，注册并将ID返回：</p><pre class=" language-Java"><code class="language-Java">override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {    case RegisterBlockManager(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint) =>      context.reply(register(blockManagerId, maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint))</code></pre><p>最后，BlockManagerInfo会被存放在blockManagerInfo(这是一个Map)中进行管理：</p><pre class=" language-Java"><code class="language-Java">blockManagerInfo(id) = new BlockManagerInfo(        id, System.currentTimeMillis(), maxOnHeapMemSize, maxOffHeapMemSize, slaveEndpoint)</code></pre><p>每一个 BlockManager 都会对应一个 BlockManagerInfo，最后，BlockManagerMaster 包含了集群中整个 BlockManager 注册的信息。</p><h1 id="四、BlockManager的具体操作举例。"><a href="#四、BlockManager的具体操作举例。" class="headerlink" title="四、BlockManager的具体操作举例。"></a>四、BlockManager的具体操作举例。</h1><p>前面说过BlockManager是用来管理数据的，具体我们参考remove这个方法看看逻辑是怎么样的。<br>具体来到RDD的unpersist方法，这里明显会删除缓存的RDD：</p><h2 id="1、unpersist算子"><a href="#1、unpersist算子" class="headerlink" title="1、unpersist算子"></a>1、unpersist算子</h2><pre class=" language-Java"><code class="language-Java">  def unpersist(blocking: Boolean = true): this.type = {    logInfo("Removing RDD " + id + " from persistence list")    sc.unpersistRDD(id, blocking)    storageLevel = StorageLevel.NONE    this  }</code></pre><p>进入 sc.unpersistRDD(id, blocking)</p><pre class=" language-Java"><code class="language-Java">private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {    env.blockManager.master.removeRdd(rddId, blocking)    persistentRdds.remove(rddId)    listenerBus.post(SparkListenerUnpersistRDD(rddId))  }</code></pre><p>这里就看到调用了env.blockManager.master.removeRdd(rddId, blocking)。</p><h2 id="2、blockManagerMaster的removeRdd方法"><a href="#2、blockManagerMaster的removeRdd方法" class="headerlink" title="2、blockManagerMaster的removeRdd方法"></a>2、blockManagerMaster的removeRdd方法</h2><pre class=" language-Java"><code class="language-Java">def removeRdd(rddId: Int, blocking: Boolean) {    val future = driverEndpoint.askSync[Future[Seq[Int]]](RemoveRdd(rddId))    future.failed.foreach(e =>      logWarning(s"Failed to remove RDD $rddId - ${e.getMessage}", e)    )(ThreadUtils.sameThread)    if (blocking) {      timeout.awaitResult(future)    }  }</code></pre><p>这里会向BlockManagerSlaveEndpoint发送删除RDD的信息并等待回执，BlockManagerSlaveEndpoint的receiveAndReply方法会接收到，并调用对应的方法，让manager去删除RDD：</p><pre class=" language-Java"><code class="language-Java">override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {    ...    case RemoveRdd(rddId) =>      doAsync[Int]("removing RDD " + rddId, context) {        blockManager.removeRdd(rddId)      }    ...}</code></pre><h2 id="3、Manager中的removeRdd"><a href="#3、Manager中的removeRdd" class="headerlink" title="3、Manager中的removeRdd"></a>3、Manager中的removeRdd</h2><p>根据RDD的id来拿到需要删除的块的集合，并通过foreach来进行删除操作</p><pre class=" language-Java"><code class="language-Java">def removeRdd(rddId: Int): Int = {    ...    val blocksToRemove = blockInfoManager.entries.flatMap(_._1.asRDDId).filter(_.rddId == rddId)    blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster = false) }    ...  }</code></pre><p>判断是否存在，并找到块完成删除操作：</p><pre class=" language-Java"><code class="language-Java">  def removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit = {      ...    blockInfoManager.lockForWriting(blockId) match {      case None =>        // The block has already been removed; do nothing.      case Some(info) =>        removeBlockInternal(blockId, tellMaster = tellMaster && info.tellMaster)        addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty)    }  }</code></pre><p>找到块，并完成删除操作，最后向mater反馈信息。</p><pre class=" language-Java"><code class="language-Java">private def removeBlockInternal(blockId: BlockId, tellMaster: Boolean): Unit = {    ...    val removedFromMemory = memoryStore.remove(blockId)    val removedFromDisk = diskStore.remove(blockId)    if (!removedFromMemory && !removedFromDisk) {      logWarning(s"Block $blockId could not be removed as it was not found on disk or in memory")    }    blockInfoManager.removeBlock(blockId)    if (tellMaster) {      reportBlockStatus(blockId, BlockStatus.empty)    }  }</code></pre><p>最后就来到了具体的删除业务代码了：</p><pre class=" language-Java"><code class="language-Java">def removeBlock(blockId: BlockId): Unit = synchronized {    ...          infos.remove(blockId)          blockInfo.readerCount = 0          blockInfo.writerTask = BlockInfo.NO_WRITER          writeLocksByTask.removeBinding(currentTaskAttemptId, blockId)    notifyAll()    ...  }</code></pre>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过Curator API操作zookeeper的学习与简单使用介绍</title>
      <link href="/2019/10/27/tong-guo-curator-api-cao-zuo-zookeeper-de-xue-xi-yu-jian-dan-shi-yong-jie-shao/"/>
      <url>/2019/10/27/tong-guo-curator-api-cao-zuo-zookeeper-de-xue-xi-yu-jian-dan-shi-yong-jie-shao/</url>
      
        <content type="html"><![CDATA[<p>主要记录些使用Curator的学习记录，方便随时查阅。</p><h2 id="Curator"><a href="#Curator" class="headerlink" title="Curator"></a>Curator</h2><blockquote><p>Curator框架在zookeeper原生API接口上进行了包装，解决了很多ZooKeeper客户端非常底层的细节开发。提供ZooKeeper各种应用场景(recipe， 比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装，实现了Fluent风格的API接口,是最好用，最流行的zookeeper的客户端。包括以下特性：</p><ol><li>自动化的连接管理。</li><li>简化了原生的ZooKeeper的方法，事件等</li><li>Curator初始化之后会一直对zk连接进行监听，一旦发现连接状态发生变化将会作出相应的处理。</li><li>内部实现了诸如Session超时重连，Watcher反复注册等功能，</li></ol><p>更多相关信息请前往：<a href="http://curator.apache.org/index.html" target="_blank" rel="noopener">http://curator.apache.org</a></p></blockquote><h2 id="使用概述："><a href="#使用概述：" class="headerlink" title="使用概述："></a>使用概述：</h2><p>curator是Fluent风格API，创建会话方式与原生的API创建方式区别很大。<br>Curator创建客户端为CuratorFramework，是由CuratorFrameworkFactory工厂类来实现的，注：CuratorFramework是线程安全的，要连接的每个ZooKeeper集群只需要一个 CuratorFramework对象就可以了。</p><h3 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h3><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.curator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>curator-recipes<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${curator.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><p>一般添加这个就够用了，这个依赖背后依赖了client和framework。<br>client是zookeeper client的封装，提供有用的客户端特性。<br>framework是api的高层封装，添加了连接管理，重试机制等。<br>本次使用版本：<br>curator：4.0.1<br>zookeeper：3.4.6（3.4版本需要排除zk的依赖包。）</p><h3 id="获取client"><a href="#获取client" class="headerlink" title="获取client"></a>获取client</h3><p>总体而言，通过CuratorFrameworkFactory来获取Client对象。一种则是使用.builder()。另一种使用构造方法直接new，其实内部走的一条线：</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * 参数： * @param connectString       list of servers to connect to（zk地址） * @param retryPolicy         retry policy to use（重试策略） * 可选参数： * @param sessionTimeoutMs    session timeout（会话超时时间）默认60000 * @param connectionTimeoutMs connection timeout（连接超时时间）默认15000 * @param namespace           每个curatorFramework可以设置独立命名空间，之后操作基于该命名空间。 *                            比如操作/test/abc =>实际是操作：/zk1/test/abc.注意使用时前面不要加/号 */</span>cf2 <span class="token operator">=</span> CuratorFrameworkFactory<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">connectString</span><span class="token punctuation">(</span><span class="token string">"hadoop:2181"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">sessionTimeoutMs</span><span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">connectionTimeoutMs</span><span class="token punctuation">(</span><span class="token number">15000</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">retryPolicy</span><span class="token punctuation">(</span>retryPolicy<span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">namespace</span><span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//也可以采用常规的newClient的形式，不过无法支持命名空间。</span><span class="token comment" spellcheck="true">//其实内部还是使用的builder，给出其中一个构造方法。</span><span class="token keyword">public</span> <span class="token keyword">static</span> CuratorFramework <span class="token function">newClient</span><span class="token punctuation">(</span>String connectString<span class="token punctuation">,</span> <span class="token keyword">int</span> sessionTimeoutMs<span class="token punctuation">,</span> <span class="token keyword">int</span> connectionTimeoutMs<span class="token punctuation">,</span> RetryPolicy retryPolicy<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>        <span class="token function">connectString</span><span class="token punctuation">(</span>connectString<span class="token punctuation">)</span><span class="token punctuation">.</span>        <span class="token function">sessionTimeoutMs</span><span class="token punctuation">(</span>sessionTimeoutMs<span class="token punctuation">)</span><span class="token punctuation">.</span>        <span class="token function">connectionTimeoutMs</span><span class="token punctuation">(</span>connectionTimeoutMs<span class="token punctuation">)</span><span class="token punctuation">.</span>        <span class="token function">retryPolicy</span><span class="token punctuation">(</span>retryPolicy<span class="token punctuation">)</span><span class="token punctuation">.</span>        <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>上面代码中有使用到一个实例，<code>retryPolicy</code>，即重试策略。<br>Curator提供了多种重试策略，后边会进行一个简单介绍，选择合适的即可。<br>不过目前为止，我们已经拿到了client。这时候需要我们调用<code>start()</code>来使用他。程序结束时我们也应该要调用<code>close()</code>来关闭他。</p><pre class=" language-java"><code class="language-java">cf2<span class="token punctuation">.</span><span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span>cf2<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4 id="重试策略简介"><a href="#重试策略简介" class="headerlink" title="重试策略简介"></a>重试策略简介</h4><p>会给出构造方法即部分介绍，一般看看名字和参数就知道有啥区别了。<br>他们都是继承自SleepingRetry<br>常用的有：<br>ExponentialBackoffRetry<br>ExponentialBackoffRetry被BoundedExponetialBackoffRetry继承，参数作用都差不多，不做赘述。</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * @param baseSleepTimeMs initial amount of time to wait between retries（每次重试会增加重试时间baseSleepTimeMs） * @param maxRetries max number of times to retry（最大重试次数） * @param maxSleepMs max time in ms to sleep on each retry（最大重试时间） *                   如果超过，将使用warn级别log，并使用最大重试时间 */</span><span class="token keyword">public</span> <span class="token function">ExponentialBackoffRetry</span><span class="token punctuation">(</span><span class="token keyword">int</span> baseSleepTimeMs<span class="token punctuation">,</span> <span class="token keyword">int</span> maxRetries<span class="token punctuation">,</span> <span class="token keyword">int</span> maxSleepMs<span class="token punctuation">)</span></code></pre><p>RetryForever（一直重试）</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * @param retryIntervalMs    重试间隔时间 */</span><span class="token keyword">public</span> <span class="token function">RetryForever</span><span class="token punctuation">(</span><span class="token keyword">int</span> retryIntervalMs<span class="token punctuation">)</span></code></pre><p>RetryNTimes（重试N次）</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * @param retryIntervalMs    重试间隔时间 * @param n                    重试N次 */</span><span class="token keyword">public</span> <span class="token function">RetryNTimes</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> sleepMsBetweenRetries<span class="token punctuation">)</span></code></pre><p>剩下还有RetryOneTime（重试一次）、RetryUntilElaspsed（一直重试，知道超过指定时间）</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="常用的API"><a href="#常用的API" class="headerlink" title="常用的API"></a>常用的API</h3><p>这边大家直接看代码注释。因为使用的是zookeeper3.4。部分3.5的特性不支持。</p><h4 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h4><p>可以允许递归创建，即父节点不存在时可以帮你创建父节点。</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * 创建节点。 * 常见节点类型： *      永久：PERSISTENT *      临时（当前连接有效）：EPHEMERAL */</span><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>    cf2<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">creatingParentsIfNeeded</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//允许递归创建</span>            <span class="token punctuation">.</span><span class="token function">withMode</span><span class="token punctuation">(</span>CreateMode<span class="token punctuation">.</span>PERSISTENT<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//持久绩点</span>            <span class="token punctuation">.</span><span class="token function">withACL</span><span class="token punctuation">(</span>ZooDefs<span class="token punctuation">.</span>Ids<span class="token punctuation">.</span>OPEN_ACL_UNSAFE<span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">forPath</span><span class="token punctuation">(</span><span class="token string">"/test2"</span><span class="token punctuation">,</span><span class="token string">"data"</span><span class="token punctuation">.</span><span class="token function">getBytes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h4 id="获取节点数据："><a href="#获取节点数据：" class="headerlink" title="获取节点数据："></a>获取节点数据：</h4><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * 获取节点信息 * data会作为byte数组返回。 * 节点信息则会放入stat中。 */</span><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">getInfo</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception<span class="token punctuation">{</span>    Stat stat <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stat</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> bytes <span class="token operator">=</span> cf2<span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">storingStatIn</span><span class="token punctuation">(</span>stat<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//将节点信息传入stat中（和zk自带的stat一样）</span>            <span class="token punctuation">.</span><span class="token function">forPath</span><span class="token punctuation">(</span><span class="token string">"/a"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>stat<span class="token punctuation">.</span><span class="token function">getNumChildren</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">/** * 获取节点信息，并使用watcher * 默认只能监听本节点的修改、删除 * 获取children可以用getChildren。 * 初次运行节点不存在会报错。watcher只能监听一次。 */</span><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">getInfoWithWatcher</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception<span class="token punctuation">{</span>    Stat stat <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stat</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> bytes <span class="token operator">=</span> cf2<span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">storingStatIn</span><span class="token punctuation">(</span>stat<span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">usingWatcher</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Watcher</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token annotation punctuation">@Override</span>                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">process</span><span class="token punctuation">(</span>WatchedEvent watchedEvent<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    <span class="token keyword">switch</span> <span class="token punctuation">(</span>watchedEvent<span class="token punctuation">.</span><span class="token function">getType</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                        <span class="token keyword">case</span> NodeDataChanged<span class="token operator">:</span>                            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"change!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>                        <span class="token keyword">case</span> NodeDeleted<span class="token operator">:</span>                            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"node delete!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>                        <span class="token keyword">case</span> NodeChildrenChanged<span class="token operator">:</span>                            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"child!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>                        <span class="token keyword">case</span> NodeCreated<span class="token operator">:</span>                            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Create!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">forPath</span><span class="token punctuation">(</span><span class="token string">"/test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>stat<span class="token punctuation">.</span><span class="token function">getVersion</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">60</span><span class="token operator">*</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h4 id="设置-修改节点数据"><a href="#设置-修改节点数据" class="headerlink" title="设置/修改节点数据"></a>设置/修改节点数据</h4><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * 设置节点 * 不指定会使用defaultData。默认为ip * 返回设置后的stat */</span><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setNode</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>    Stat stat <span class="token operator">=</span> cf2<span class="token punctuation">.</span><span class="token function">setData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">withVersion</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">forPath</span><span class="token punctuation">(</span><span class="token string">"/test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>stat<span class="token punctuation">.</span><span class="token function">getVersion</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h4 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h4><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * 删除节点 * deletingChildrenIfNeeded：子节点存在则删除子节点，再删除父节点。 * guaranteed：如果删除失败，后端会一直尝试删除知道成功。 */</span><span class="token annotation punctuation">@Test</span><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">deleteNode</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">throws</span> Exception<span class="token punctuation">{</span>    Void aVoid <span class="token operator">=</span> cf2<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">guaranteed</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//可省略，保证删除节点（删除失败后段会一直删除知道成功）</span>            <span class="token punctuation">.</span><span class="token function">deletingChildrenIfNeeded</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//可省略，开启递归删除。</span>            <span class="token punctuation">.</span><span class="token function">forPath</span><span class="token punctuation">(</span><span class="token string">"/test"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>aVoid<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h3 id="Cache（watcher-N次）"><a href="#Cache（watcher-N次）" class="headerlink" title="Cache（watcher N次）"></a>Cache（watcher N次）</h3><p>原生的zk，有个比较蛋疼的地方就是watcher只能使用一次，如果需要重复使用，常见的可以采用生产消费者模式来重复触发注册，不过还是比较繁琐的。<br>幸运的是，Curator自带了一套名为Cache的东西，能够帮助我们完成类似功能：<br>注：上面有使用过usingWatcher的方法，不过也是一次性的。</p><p>Cache一般有三种：NodeCache、PathChildrenCache、TreeCache。<br>NodeCache是监视当前节点。<br>PathChildrenCache则是监视该节点的子节点。（不能递归监视孙节点哦）<br>TreeCache则是前两者的结合，即监视当前节点+当前节点的子节点。<br>他们使用上其实是差不多的，所有这边就只介绍一种，需要使用另外一种时，稍微看看源码就会用了。</p><h4 id="创建Cache"><a href="#创建Cache" class="headerlink" title="创建Cache"></a>创建Cache</h4><p>PathChildrenCache有6种构造方法，比另外两个加起来还多，这边介绍下常用的参数：</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * @param client           传入client * @param path             监视的path * @param cacheData        是否缓存data，写false，你会得不到节点的数据。只能拿到stat。 * @param dataIsCompressed 是否压缩 */</span>PathChildrenCache pathChildrenCache <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">PathChildrenCache</span><span class="token punctuation">(</span>cf2<span class="token punctuation">,</span><span class="token string">"/test"</span><span class="token punctuation">,</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>pathChildrenCache<span class="token punctuation">.</span><span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="StartMode"><a href="#StartMode" class="headerlink" title="StartMode"></a>StartMode</h4><p>PathChildrencache有三种启动模式：(另外两种Cache则没这么丰富)</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/*** The cache will be primed (in the backgrouinitial values.* Events for existing and new nodes will be posted.* * 默认的启动模式。异步初始化*/</span>StartMode<span class="token punctuation">.</span>NORMAL<span class="token comment" spellcheck="true">/*** The cache will be primed (in the foregrouinitial values.* {@link PathChildrenCache#rebuild()} will be called before* the {@link PathChildrenCache#start(Stamethod returns* in order to get an initial view of the node.* * 同步初始化Cache。创建后就从服务器拉数据。*/</span>StartMode<span class="token punctuation">.</span>BUILD_INITIAL_CACHE<span class="token comment" spellcheck="true">/*** After cache is primed with initial values (in the background) a* {@link PathChildrenCacheEvent.Type#INITIALIZED} will be posted.* * 异步初始化，Cache准备好后，会触发INITIALIZED类型的事件。Listerner会收到该通知，* 通过pathChildrenCacheEvent.getType()就能拿到。*/</span>StartMode<span class="token punctuation">.</span>POST_INITIALIZED_EVENT</code></pre><p>其实就是一个异步/同步初始化、是否有通知的区别。</p><h4 id="Listerner"><a href="#Listerner" class="headerlink" title="Listerner"></a>Listerner</h4><p>通过给Listerner设置监听器，我们就能够对设置的节点进行监听工作。</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">//设置为true才能够达到节点的data。</span>PathChildrenCache pathChildrenCache <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">PathChildrenCache</span><span class="token punctuation">(</span>cf2<span class="token punctuation">,</span><span class="token string">"/test"</span><span class="token punctuation">,</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>pathChildrenCache<span class="token punctuation">.</span><span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>pathChildrenCache<span class="token punctuation">.</span><span class="token function">getListenable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">addListener</span><span class="token punctuation">(</span><span class="token punctuation">(</span>curatorFramework<span class="token punctuation">,</span> pathChildrenCacheEvent<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>    String nodeName <span class="token operator">=</span> ZKPaths<span class="token punctuation">.</span><span class="token function">getNodeFromPath</span><span class="token punctuation">(</span>pathChildrenCacheEvent<span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    String path2 <span class="token operator">=</span> pathChildrenCacheEvent<span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">switch</span> <span class="token punctuation">(</span>pathChildrenCacheEvent<span class="token punctuation">.</span><span class="token function">getType</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">case</span> CHILD_ADDED<span class="token operator">:</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>nodeName<span class="token operator">+</span><span class="token string">" :add: "</span><span class="token operator">+</span>path2<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token keyword">case</span> CHILD_REMOVED<span class="token operator">:</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"remove"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token keyword">case</span> CHILD_UPDATED<span class="token operator">:</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"changed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>pathChildrenCacheEvent<span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getData</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>我们能够获取到节点对type，并作出对应对处理。</p>]]></content>
      
      
      <categories>
          
          <category> Utils </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Utils </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase的RowKey设计</title>
      <link href="/2019/09/30/hbase-de-rowkey-she-ji/"/>
      <url>/2019/09/30/hbase-de-rowkey-she-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于hbase，会将一张Table中的数据，按照定义的RowKey，将数据切分到不同的Region中。一般不同的Region会被Master分配到不同的机器上。<br>所以如果RowKey设计良好，能够让集群<code>负载均衡，提高吞吐量。防止出现热点问题</code>。</p><blockquote><p>Hbase版本：1.2.0</p></blockquote><h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><ol><li>RowKey<strong>长度不应该超过16字节</strong>，否则对于HFile和MemStore来说会极大的占用存储空间</li><li>RowKey设计时需要保证<strong>唯一性</strong>。当两个rowKey经过变换可能生产同一个结果时，这种设计就是有问题的。</li><li>Rowkey是按照<a href="https://zh.wikipedia.org/wiki/字典序" target="_blank" rel="noopener">字典序</a><strong>排序</strong>。设计后请确认rowKey的排序能否符合预期。</li><li>RowKey设计的最终目的是<strong>避免热点问题</strong>。设计时需要确认自己的方案对于上游来的数据能够做到均匀分布。<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2>整体来看，有两种情况：</li><li>第一种，在RowKey前增加一个<strong>完全随机</strong>的随机前缀。这样简单，也会带来一些坏处，但其实也有解决方案。</li><li>第二种，我们可以自己设计RowKey。我们可利用数据中的某1个或者某几个字段，<strong>通过一定的处理，组装成我们的RowKey</strong>。这样尽管设计复杂，但能够带来一些好处，但其也会有一些受限情况。</li></ol><p>实际上，上述两种情况虽有些许不同，不过本质都是需要随机。随机意味着数据打散，意味着查找变困难。但Hbase本身就是一个重写轻读的系统。</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><h3 id="加盐前缀"><a href="#加盐前缀" class="headerlink" title="加盐前缀"></a>加盐前缀</h3><p>盐就是<a href="https://zh.wikipedia.org/wiki/%E7%9B%90_(%E5%AF%86%E7%A0%81%E5%AD%A6)" target="_blank" rel="noopener">Salt</a>。<br>我们在创建Hbase表示，就能够人工指定上rowKey的分隔符（而不是等某个Region已经很大了，再让系统自动去划分）：</p><pre class=" language-powershell"><code class="language-powershell">hbase> create <span class="token string">'ns1:t1'</span><span class="token punctuation">,</span> <span class="token string">'f1'</span><span class="token punctuation">,</span> SPLITS => <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">]</span></code></pre><p>这样就能够创建出5个Region：（都是左闭右开的区间）<br>【null—a】、【a—b】、【b—c】、【c—d】、【d—null】<br>假如说我们rowKey本来都是：001，002，003。这样就都会分到第一个Region。这显然有问题。<br>通过给我们RowKey加上a-d的前缀，比如说：a-001，b-002。就能够均匀分到设计的<strong>4</strong>个Region了，而不是说基准到某一个Region。<br>ps：<br>实际上，我们如果使用Phoenix来管理hbase，这里会更加的方便。</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token keyword">table</span> <span class="token punctuation">(</span>a_key <span class="token keyword">VARCHAR</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span> a_col <span class="token keyword">VARCHAR</span><span class="token punctuation">)</span> SALT_BUCKETS <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">;</span></code></pre><p>不过phoenix不在本文章讨论范围，以后有机会再弄phoenix相关的。更多的请参考官网链接：<a href="http://phoenix.apache.org/salted.html" target="_blank" rel="noopener">Salted Tables</a><br>但加盐前缀解决了数据分布的问题，却也有很多缺点：</p><h4 id="加盐前缀的缺点："><a href="#加盐前缀的缺点：" class="headerlink" title="加盐前缀的缺点："></a>加盐前缀的缺点：</h4><ol><li>第一个分区永远没有数据。因为我们的加盐是从a开始加的。</li><li>因为是完全随机，我们查询时并不知道之前加入的随机数是什么。这个问题不仅仅是查询，还会影响到put和delete指令。<h4 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h4>我们可以在Table的基础上，建立二级索引。即通过在外部维护一张小表，将我们查询查用的字段放到这张表中。这个方案可以使用Phoenix实现，也可以使用ES来实现。<br>Phoenix的实现：<pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">INDEX</span> my_index <span class="token keyword">ON</span> my_table <span class="token punctuation">(</span>v1<span class="token punctuation">,</span>v2<span class="token punctuation">)</span> INCLUDE<span class="token punctuation">(</span>v3<span class="token punctuation">)</span></code></pre>由于Phoenix不在本文章讨论范围，可以参考：<a href="http://phoenix.apache.org/secondary_indexing.html" target="_blank" rel="noopener">Secondary Indexing</a><h4 id="关于Phoenix多说一句"><a href="#关于Phoenix多说一句" class="headerlink" title="关于Phoenix多说一句"></a>关于Phoenix多说一句</h4>尽管本文章不会过多的介绍Phoenix，不过使用phoenix管理盐前缀，还有个十分强大的功能。不感兴趣可以直接跳过。<br>前面的加前缀的方法中提到过，我们需要在代码中手工的拼接成新的rowKey。而如果使用phoenix管理，我们可以省略拼接这部工作，phoenix会帮助我们完成：<br>这里我们新建一张表mySalt，并设计成4个分区：</li></ol><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> mysalt <span class="token punctuation">(</span>a_key <span class="token keyword">VARCHAR</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span><span class="token punctuation">,</span> a_col <span class="token keyword">VARCHAR</span><span class="token punctuation">)</span> SALT_BUCKETS <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">;</span></code></pre><p>这是upsert前：所有区域都没有数据</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20191110230534325.png" alt="insert前"></p><p>我们向其中upsert 5条数据，实际上他们的rowKey是十分相似的。但通过查看web界面，我们可以知道他们被发往了不同的Region。</p><pre class=" language-sql"><code class="language-sql">upsert <span class="token keyword">into</span> mysalt <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'111'</span><span class="token punctuation">,</span><span class="token string">'jone'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>upsert <span class="token keyword">into</span> mysalt <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'112'</span><span class="token punctuation">,</span><span class="token string">'jone'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>upsert <span class="token keyword">into</span> mysalt <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'113'</span><span class="token punctuation">,</span><span class="token string">'jone'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>upsert <span class="token keyword">into</span> mysalt <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'114'</span><span class="token punctuation">,</span><span class="token string">'jone'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>upsert <span class="token keyword">into</span> mysalt <span class="token keyword">values</span> <span class="token punctuation">(</span><span class="token string">'115'</span><span class="token punctuation">,</span><span class="token string">'jone'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20191110230733649.png" alt="insert 后"><br>这里不过多的展开。</p><h3 id="加Hash值"><a href="#加Hash值" class="headerlink" title="加Hash值"></a>加Hash值</h3><p>本方案的核心思路是对本来的RowKey去其Hash值，然后构建成新的RowKey。比如：Hash+RowKey。其实Hash我们取前几位就可以了，不然RowKey会太长了。<br>使用本方法时，建表可以使用官方自带的Hash分区规则：</p><pre class=" language-shell"><code class="language-shell">hbase> create 't1', 'f1', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}</code></pre><p>我们在代码中处理时，人为的获取rowKey的hash值，再进行拼接。这样带来了几个好处：</p><ol><li>根据rowKey，我们能够拿到随即后的值，这样我们就能够快速定位到Region了。</li><li>通过这种办法，也能够将数据打散，避免热点问题。</li></ol><h4 id="加hash值的缺点"><a href="#加hash值的缺点" class="headerlink" title="加hash值的缺点"></a>加hash值的缺点</h4><ol><li>只有通过rowKey来索引会快些，我们想<strong>通过其他字段</strong>查找时依旧会面临一样的问题。</li><li>原本可能在一块的数据会被打散，这一定程度上会影响查询效率。而实际业务中，查询一块的数据是非常常见的需求。</li></ol><p>实际上，第二点是无法避免的，毕竟本文的核心就是将原本有规律的rowKey打散。在加盐前缀中，解决方案就是二级索引。其实这个思路是通用的。</p><blockquote><p>注意：生成Hash时，请使用Hbase自己的工具：org.apache.hadoop.hbase.util.Hash</p></blockquote><h3 id="更加复杂的RowKey设计"><a href="#更加复杂的RowKey设计" class="headerlink" title="更加复杂的RowKey设计"></a>更加复杂的RowKey设计</h3><p>在Hash值那个方案的缺点里，有提过一个点：我们想<strong>通过其他字段</strong>查找时，有没有办法加速呢？<br>答案是有的。但会有诸多的限制。没有rowKey设计是完美的，本节会指导一个设计思路，而非通用的办法。</p><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>其实设计思路和加Hash值差不多，简单来说，我们需要<code>利用已有字段的数据，人工拼接成出一个RowKey</code>。并且保证这个RowKey是唯一的。因为我们的RowKey里包含了这部分数据，所以我们就能够快速的找到Region。<br>当然会有很多限制，我们在末尾讨论。</p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>假设我们在生产过程中，需要经常进行下列几种查询：越前面越常用。</p><ol><li>查询某个用户的订单数据。</li><li>获得某个用户最近的订单数据。</li><li>查询过去一段时间的订单数据。</li></ol><p>通过分析，我们知道关键是两个字段：用户，时间。<br>所以我们可以考虑利用用户ID与订单时间，来构建RowKey。比如：<code>[Hash_userID]+[UserID]+[TIME]</code><br>这样，当我们需要使用userId进行查询时，就能够提高查询速度了。</p><blockquote><p>Hash主要是用来打散数据。因为UserID更加重要，所以我们取UserID的hash值作为前缀。</p></blockquote><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>我们知道，RowKey在Region里面的存储默认是按照<strong>字典排序</strong>的，对于我们刚刚创建的RowKey，新的数据反倒会在后面。这在查询时会额外浪费一些时间。所以对于第二种场景，将新数据放在老数据之前是有必要的。</p><blockquote><p>这里虽然说的是新旧数据，但其实是两条数据，不要和Hbase内部的version机制搞混了。</p></blockquote><p>所以可以进一步对RowKey中的Time进行处理，可以找一个很大的值，对于本例的时间戳来说，我们可以找Long_MaxValue来做减法，或者你找2050年的时间戳来做减法。得到新的：<code>[USERID_HASH]+[USERID]+[xxxxxx-TIME]</code>，这样就达到了逆序的效果。</p><blockquote><p>注意：运算后的数据位数可能不一样，可能就达不到我们想要的结果，有必要的情况下，不要忘了补位处理。</p></blockquote><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><p>对于第三种场景，这种rowKey设计是无能为力的，我们仍然需要遍历所有Region。</p><blockquote><p>因为Hbase底部的条件查询其实类似于SQL中的Like ‘XXX%’。现阶段我们无法利用中间的数据完成过滤。</p></blockquote><p>所以需要根据实际业务来设计Rowkey。本节只是讲述一种非常简单的例子。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>RowKey的设计，还有很多，比如反转RowKey等等。<br>但归根结底，就是两大类：</p><ol><li>加盐前缀。前缀与我们的数据毫无关系。这种方案最大的缺点就是会影响查询和数据的删除修改。不过我们可以通过建立二级索引来优化这些问题。Phoenix能够帮助我们更加方便的使用加盐前缀。</li><li>根据某个字段配合Hash来设计我们的RowKey。这个字段最好选用我们查询经常会用到的字段，这样能够提升大部分业务的查询速度。</li></ol><p>我们需要结合我们拿到的数据是什么样的，再去进行设计。<br>归根结底，Hbase是一个重写轻读的系统，设计RowKey的本来目的就是让Hbase能够更好的取写数据。通过将数据打散，肯定会影响读场景的效率。<br>但数据存下来终归是要拿来用的，设计RowKey时也要考虑读场景，尽量优化。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase关键参数调优及建议</title>
      <link href="/2019/08/17/hbase-guan-jian-can-shu-diao-you/"/>
      <url>/2019/08/17/hbase-guan-jian-can-shu-diao-you/</url>
      
        <content type="html"><![CDATA[<h3 id="HBase在哪些地方调优"><a href="#HBase在哪些地方调优" class="headerlink" title="HBase在哪些地方调优"></a>HBase在哪些地方调优</h3><p>HBase用下来主要有两类需要调优的地方，是最容易影响性能，最容易出问题的步骤：Flush和Compaction。</p><p>查资料时也发现很多文章并没有给这两方面的建议，有些参数还已经过时了，所以才写了这篇文章。</p><p>其他的比如网络超时时间等参数，GC选择可以参考其他文章。</p><h3 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h3><h4 id="MemStore级别限制：（主要优化项）"><a href="#MemStore级别限制：（主要优化项）" class="headerlink" title="MemStore级别限制：（主要优化项）"></a>MemStore级别限制：（主要优化项）</h4><p>当Region的任意一个memstore的size 达到阈值时触发。这个是最正常的Flush，建议将参数调大，防止频繁的Flush操作。</p><p>涉及参数：hbase.hregion.memstore.flush.size(默认128M) </p><p>建议：调至256~512M左右。</p><h4 id="Region级别限制："><a href="#Region级别限制：" class="headerlink" title="Region级别限制："></a>Region级别限制：</h4><p>当一个Region所有的memstore的size的和达到阈值时，会触发。</p><p>涉及参数：hbase.hregion.memstore.block.multipiler(默认2) * hbase.hregion.memstore.flush.size(上一项中的参数)。</p><p>建议：（前者）设置得略大于 列簇数（假如设置为4，则建表时列族不要超过3）</p><h4 id="regionServer级别限制"><a href="#regionServer级别限制" class="headerlink" title="regionServer级别限制"></a>regionServer级别限制</h4><p>这个灾难的(<strong>需要调优避免</strong>) (会<strong>影响这台机器上的所有表</strong>)</p><p>当RegionServer所有的memstore的size之和，超过低水位线。<strong>RS强制Flush</strong>，先从MemStore最大的开始，直到<strong>总的大小下降到低水位线以下</strong>。如果此时吞吐量依然很高，达到了高水位线，会触发阻塞Flush。直到大小降低到低水位线。</p><p>这里的参数也和hbase.hregion.memstore.flush.size惜惜相关，所以具体设置时需要一起算一下。</p><p>涉及参数：</p><table><thead><tr><th>参数</th><th>建议</th></tr></thead><tbody><tr><td>JVM堆内存 -Xmx ， -Xms</td><td>(默认50M),RS进程的堆内存.建议物理机32G。非物理机20~30G。<br />如果内存吃紧也可调低。重点调整。</td></tr><tr><td>hbase.regionserver.global.memstore.size<br />或<br />hbase.regionserver.global.memstore.upperlimit（已过期）</td><td>(默认0.4)不调<br />RS进程的堆内存*本参数 ==&gt; 高水位线</td></tr><tr><td>hbase.regionserver.global.memstore.size.lower.limit<br />或<br />hbase.regionserver.global.memstore.lowerLimit（已过期）</td><td>(默认0.95)，设置必须要&gt;=0.9。一般不用管。<br />高水位线*本参数 ==&gt; 低水位线</td></tr></tbody></table><h4 id="HLog级别限制：（重点）"><a href="#HLog级别限制：（重点）" class="headerlink" title="HLog级别限制：（重点）"></a>HLog级别限制：（重点）</h4><p>一个RegionServer上HLog总的大小达到会触发。系统会选取最早的HLog对应的一个或多个Region进行Flush。</p><p>涉及参数：hbase.regionserver.maxlogs（默认32）（一个Region Server中HLog数量）</p><p>建议：当我们调整了hbase.hregion.memstore.flush.size后，也需要调整这个参数，否则这个参数一样会频繁的触发Flush。本参数建议根据hbase.hregion.memstore.flush.size的调整来设置。具体设置可以参考<a href="https://issues.apache.org/jira/browse/HBASE-14951" target="_blank" rel="noopener">HBASE-14951</a></p><h4 id="定期Flush"><a href="#定期Flush" class="headerlink" title="定期Flush"></a>定期Flush</h4><p>如果我们很久没有对 HBase 的数据进行更新，hbase会起一个线程flush所有memstore。默认周期为1h。这里的定期有一定的随机延迟（20000左右）。</p><p>涉及参数：hbase.regionserver.optionalcacheflushinterval（默认3600000）（单位ms）</p><p>建议：建议调大，但这个根据集群的压力来判断，没有一个合适的值，过短的Flush可能会造成小文件问题。</p><h4 id="手动触发flush"><a href="#手动触发flush" class="headerlink" title="手动触发flush"></a>手动触发flush</h4><p>flush ‘table_name’ 刷写单个表</p><p>flush ‘region_name’ 刷写单个region</p><h4 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h4><ol><li>hbase.hregion.memstore.flush.size</li><li>HBase的堆内存</li><li>HLog级别限制。</li></ol><h3 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h3><p>合并在HBase里分为大合并和小合并：</p><ul><li>小合并容易发生，且允许发生。小合并主要用来将数量过多的HFile进行合并。（仅仅合并而已）</li><li>大文件尽量不要发生。大合并主要是用来处理update(超过版本数)，delete，ddl产生的多余文件。</li></ul><p>讨论这块的调优还是跟着触发时机来看：</p><h4 id="小合并"><a href="#小合并" class="headerlink" title="小合并"></a>小合并</h4><p>触发及处理：同一个Region下，HFile数量过多，读取效率低。类似于小文件合并。选择相邻的一部分HFile文件，合成一个文件。(仅仅合并)</p><p>触发条件：</p><ul><li><p>数量出发</p><p>每次memstore级别的flush之后，都要对当前的HFile的<strong>文件数量</strong>进行判断，一旦大于就会触发合并。</p><p>涉及参数：hbase.hstore.compactionThreshold(默认3)</p><p>建议：一般不用调整，如果写入qps较高比较大，可以略微调高至5左右。调整该参数建议同时调整<strong>hbase.hstore.compaction.max</strong>至同样的倍数</p></li><li><p>时间触发</p><p>到达参数设置的时间后，会进行检查。一旦达到要求。就会触发合并。</p><p>涉及参数：hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multiplier</p><p>建议：一般不用调整。</p></li></ul><p>其他参数：</p><p>hbase.hstore.blockingStoreFiles：默认为10，表示一旦某个store中文件数大于该阈值，就会导致所有更新阻塞。建议逐步调大至100，特别是出现“Too many HFiles, delaying flush”时。</p><p>hbase.regionserver.thread.compaction.small：小合并的线程数。默认为1。建议调整为3/5。但不能过大。</p><h4 id="大合并"><a href="#大合并" class="headerlink" title="大合并"></a>大合并</h4><p>一句话：避免自动大合并，所以把<strong>hbase.hregion.majorcompaction</strong>设置为0。写个脚本在业务低谷期去定期大合并。</p><p>大合并主要清除3类数据：</p><ul><li>清理TTL：假如table设置了过期时间，大合并会清理已过期的数据</li><li>清理put造成的多版本。（版本号超过设定）</li><li>清理delete操作造成的多版本。</li></ul>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>updateStateByKey vs mapWithState</title>
      <link href="/2019/07/26/updatestatebykey-vs-mapwithstate/"/>
      <url>/2019/07/26/updatestatebykey-vs-mapwithstate/</url>
      
        <content type="html"><![CDATA[<h2 id="有状态转换"><a href="#有状态转换" class="headerlink" title="有状态转换"></a>有状态转换</h2><p>在SparkStreaming中，有时候我们需要进行聚合操作，且依赖于以前的数据，就需要使用到有状态算子。<br>不过由于SparkStreaming中，有状态算子依赖Checkpoint，而Checkpoint有个很大的弊端就是修改代码后失效，所以实际使用时需要用到他们的初始化值。但目前应该先理解有状态转换算子。</p><h2 id="updateWithStateByKey"><a href="#updateWithStateByKey" class="headerlink" title="updateWithStateByKey"></a>updateWithStateByKey</h2><p>简单来说，这个算子的功能是按照Key，将上一批次得到的State与当前批次的Value进行func操作。</p><h3 id="使用层面："><a href="#使用层面：" class="headerlink" title="使用层面："></a>使用层面：</h3><pre><code>  def updateStateByKey[S: ClassTag](      updateFunc: (Iterator[(K, Seq[V], Option[S])]) =&gt; Iterator[(K, S)],      partitioner: Partitioner,      rememberPartitioner: Boolean): DStream[(K, S)] = ssc.withScope {    val cleanedFunc = ssc.sc.clean(updateFunc)    val newUpdateFunc = (_: Time, it: Iterator[(K, Seq[V], Option[S])]) =&gt; {      cleanedFunc(it)    }    new StateDStream(self, newUpdateFunc, partitioner, rememberPartitioner, None)  }</code></pre><p>使用时主要需要传入一个函数func：</p><ol><li>func传入两个参数：更新值（Seq[V]），累计值（Option[U]）。</li><li>将更新值与累计值进行func操作，返回累计值Option[U]。</li><li>这个返回值会作为下一阶段的累计值使用，如此循环。</li></ol><p>示例：</p><pre><code>val stateDstream = inputKeyDStream    .updateStateByKey((newValues: Seq[Int], oldValue: Option[Int]) =&gt; {        // 更新状态        Some(newValues.foldLeft(oldValue.getOrElse(0))(_ + _))    })    .foreachRDD(rdd =&gt; {        rdd.foreach(pair =&gt; println(s&quot;k=${pair._1} v=${pair._2}&quot;))    })</code></pre><h3 id="实现原理："><a href="#实现原理：" class="headerlink" title="实现原理："></a>实现原理：</h3><ol><li>updateStateByKey方法内部最后会new一个StateDStream。</li><li>StateDStream的compute方法会先获取上一个batch计算出的RDD（历史数据）。然后获取本次batch中StateDStream的父类计算出的RDD（本次数据）。然后调用computeUsingPreviousRDD方法。</li><li>computeUsingPreviousRDD方法会对两个RDD进行cogroup操作（性能低），并应用func。</li></ol><p>逻辑图如下：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581835901663.png" alt="updateStateByKey方法内部最后会new"></p><h2 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState"></a>mapWithState</h2><p>updateWithStateByKey每次更新状态时，都需要对状态数据进行全量的聚合操作，这一步是相当耗费性能的。从Spark-1.6开始，Spark-Streaming引入一种新的状态管理机制mapWithState。<br>不过目前这个算子仍然被标记为：@Experimental。使用时需要注意。</p><h3 id="使用层面"><a href="#使用层面" class="headerlink" title="使用层面"></a>使用层面</h3><pre><code>  def mapWithState[StateType: ClassTag, MappedType: ClassTag](      spec: StateSpec[K, V, StateType, MappedType]    ): MapWithStateDStream[K, V, StateType, MappedType] = {    new MapWithStateDStreamImpl[K, V, StateType, MappedType](      self,      spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]    )  }</code></pre><ol><li>mapWithState接受参数：StateSpec[KeyType,valueType,StateType,MappedType]，其实就是写StateSpec.function</li></ol><pre><code>| 类型       | 意义                                                                             || ---------- | -------------------------------------------------------------------------------- || KeyType    | Key的类型。                                                                               || valueType  | value。可以理解为新数据的类型。                                                        || stateType  | state。可以理解为历史累计数据的类型。                                                   || mappedType | 映射数据。可以理解为Dstream/RDD中的数据类型。只有这个数据可以被其他算子使用到。 |</code></pre><ol start="2"><li><p>StateSpec：</p><table><thead><tr><th>算子</th><th>用途</th></tr></thead><tbody><tr><td>function(func)</td><td>具体如何更新状态。func接受参数：[KeyType , Option[valueType] , State[StateType]] 经过转换返回 [MappedType]。需要在func中通过State.get，State.update等方法来维护State。</td></tr><tr><td>initialState(RDD)</td><td>（可选）初始的RDD[(KeyType,StateType)]</td></tr><tr><td>timeout(Duration)</td><td>（可选）key的过期时间。过期后会将key删除（因为state一旦调用remove，将不能update）</td></tr></tbody></table></li><li><p>返回：MapWithStateDStream[KeyType,valueType,StateType,MappedType]</p></li><li><p>维护State：<br>mapWithState的历史信息是放在StateRDD中维护的，准确说是MapWithStateRDDRecord。<br>内部会将KV保存成对应的StateMap中</p></li></ol><p>示例：</p><pre><code> stream    .map(message =&gt; (message._2, 1))    .mapWithState(StateSpec.function((key: String, value: Option[Int], state: State[Long]) =&gt; {        val sum = value.getOrElse(0).toLong + state.getOption.getOrElse(0L)        val output = (key, sum)        // 更新状态        state.update(sum)        println(s&quot;MapWithState: key=$key value=$value state=$state&quot;)        output    }))     .foreachRDD(rdd =&gt; {        rdd.foreach(pair =&gt; println(s&quot;MapWithState: key=${pair._1} value=${pair._2}&quot;))    })</code></pre><h3 id="实现逻辑："><a href="#实现逻辑：" class="headerlink" title="实现逻辑："></a>实现逻辑：</h3><ol><li><p>mapWithState接受更新函数mappingFunc，该函数会更新指定用户的状态，同时会返回更新后的状态。</p><p> Spark-Streaming通过根据我们定义的更新函数，在每个计算时间间隔内更新内部维护的状态，同时返回经过mappingFunc后的结果数据流。<br> <img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581838901682.png" alt="mapWithState"><br> <font color=blue>蓝色箭头</font>为实时过来的数据流<font color=blue>liveDStream</font>，通过liveDStream.mapWithState的调用，会得到一个<font color=green>MapWithStateDStream</font>（mapWithState算子返回值），为方框中上面<font color=green>浅绿色的箭头</font>。</p></li></ol><ol start="2"><li><p>计算过程中，Spark-Streaming会遍历当前时间间隔内的数据rdd-x，在上一个时间间隔的状态state-(x-1)中查找指定的记录，并更新状态，更新操作就是我们前面定义的mappingFunc函数。</p><p> 这里的状态更新不再需要全量扫描状态数据了，状态数据是存在hashmap中，可以根据过来的数据快速定位到。<br> <img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581839214555.png" alt="mapWithState"><br> 首先通过partitionBy将新来的数据分区到对应的状态分区上，每个状态分区中的仅有一条记录，类型为MapWithStateRDDRecord，它打包了两份数据：stateMap保存当前分区内所有的状态、mappedData保存经过mappingFunc处理后的结果。</p><pre><code> case class MapWithStateRDDRecord[K, S, E](var stateMap: StateMap[K, S], var mappedData: Seq[E])</code></pre><p> 默认输出的是mappedData这份（如果一个key在一个batch中存在多次，会一次输出多分mappedData）<br>如果需要输出全量状态，则可以在mapWithState后调用snapshot函数获取。</p></li></ol><h2 id="updateWithStateByKey-vs-mapWithState"><a href="#updateWithStateByKey-vs-mapWithState" class="headerlink" title="updateWithStateByKey vs mapWithState"></a>updateWithStateByKey vs mapWithState</h2><p>相同点：</p><table><thead><tr><th></th></tr></thead><tbody><tr><td>都是pairDStream的算子。</td></tr><tr><td>都需要checkPoints</td></tr></tbody></table><p>不同点：最主要的差异就是这两个算子作用的key不一样，update需要作用于所有key，mapWith只需要作用于更新的Key</p><table><thead><tr><th></th><th>updateWithStateByKey</th><th>mapWithState</th></tr></thead><tbody><tr><td>保存State</td><td>靠内部专门的MapWithStateRDDRecord来维护</td><td>依靠本身的RDD来维护。</td></tr><tr><td>性能</td><td>只更新需要更新的key，官方称相较于updateStateByKey会有10倍提升</td><td>内部逻辑主要是先对两个Rdd进行cogroup，可能会涉及shuffle。</td></tr><tr><td>返回数据</td><td>key的state是独立保存的，每次调用函数只会作用于当前key，所以只会返回更新的数据部分。</td><td>返回全量数据。</td></tr><tr><td>操作数据</td><td>mapWithState是对单独的一条数据进行操作。当前值是一个Option</td><td>会对一个批次内的K进行操作，当前值是一个Seq。</td></tr></tbody></table><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>实际使用时，我们发现mapWithState与updateWithState存在两个问题：</p><ol><li>依赖于Checkpoint，而Checkpoint目前在修改代码后会失效。</li><li>也是由于状态维护在内存中，太耗内存了。<br>这也可能是mapWithState直到现在依旧是@Experimental的原因吧。</li></ol><h3 id="解决思路："><a href="#解决思路：" class="headerlink" title="解决思路："></a>解决思路：</h3><p>其实状态缓存，无非是要有一个介质去存状态，自然眼睛就来到了Redis。实际使用时，为了减少访问Redis的次数，最好在访问前对Redis进行聚合操作，并使用pipeline的方式批量访问。</p><p>实现参考：</p><pre><code>val updateFunc = (      records: Seq[(Long, Set(Int))],      states: Seq[Response[String]],      pipeline: Pipeline) =&gt; {  pipeline.sync()  var i = 0  while (i &lt; records.size) {    val (userId, values) = records(i)    val oldValues: Set[Int] = parseFrom(states(i).get())    val newValues = values ++ oldValues    pipeline.setex(userId.toString, 3600, toString(newValues))    i += 1  }  pipeline.sync() }val Func = (iter: Iterator[(Long, Iterable[Int])]) =&gt; {  val jedis = ConnectionPool.getConnection()  val pipeline = jedis.pipelined()  val records = ArrayBuffer.empty[(Long, Set(Int))]  val states = ArrayBuffer.empty[Response[String]]  while (iter.hasNext) {    val (userId, values) = iter.next()    records += (userId, values.toSet)    states += pipeline.get(userId.toString)    if (records.size == batchSize) {      updateFunc(records, states, pipeline)      records.clear()      states.clear()    }  }  updateFunc(records, states, pipeline)  Iterator[Int]()}inputDStream.groupByKey()  .mapPartitions(Func)}</code></pre><hr><p>参考文章：</p><ol><li><a href="http://sharkdtu.com/posts/spark-streaming-state.html" target="_blank" rel="noopener">DStream updateStateByKey vs mapWithState</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识MaxWell</title>
      <link href="/2019/06/20/chu-shi-maxwell/"/>
      <url>/2019/06/20/chu-shi-maxwell/</url>
      
        <content type="html"><![CDATA[<h2 id="Maxwell"><a href="#Maxwell" class="headerlink" title="Maxwell"></a>Maxwell</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近需要实时采集MySQL的数据到HBase，找了下目前主要有MaxWell和Canal。介于Maxwell部署简单，且支持断点还原，最后选择了MaxWell。</p><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>MaxWell通过采集MySQL的binlog日志，将详细格式化成Json发往Kafka。SparkStreaming只需要对Json数据进行解析，就能够拼接SQL语句，然后同步到HBase。</p><h3 id="初次使用"><a href="#初次使用" class="headerlink" title="初次使用"></a>初次使用</h3><p>参考：<a href="http://maxwells-daemon.io/quickstart/" target="_blank" rel="noopener">Maxwell’s Daemon</a></p><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>下载、解压MaxWell</p><ol><li><p>配置MySQL：</p><pre><code>[mysqld]server_id=1log-bin=masterbinlog_format=row</code></pre></li><li><p>创建Maxwell相关表、用户：</p><pre><code>mysql&gt; CREATE USER &#39;maxwell&#39;@&#39;%&#39; IDENTIFIED BY &#39;XXXXXX&#39;;mysql&gt; GRANT ALL ON maxwell.* TO &#39;maxwell&#39;@&#39;%&#39;;mysql&gt; GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO &#39;maxwell&#39;@&#39;%&#39;;</code></pre></li><li><p>测试：</p><pre><code>bin/maxwell --user=&#39;maxwell&#39; --password=&#39;XXXXXX&#39; --host=&#39;127.0.0.1&#39; --producer=stdout</code></pre></li><li><p>在Mysql中创建测试表：</p><pre><code>create table emp (    id numeric(4) primary key,    name varchar(10));</code></pre></li><li><p>insert测试数据：</p><pre><code>insert into emp (id,name) values (1,&#39;ZhangSan&#39;);</code></pre></li></ol><h4 id="on-Kafka"><a href="#on-Kafka" class="headerlink" title="on Kafka"></a>on Kafka</h4><p>on Kafka其实也很简单，主要是注意下版本兼容问题。</p><ol><li><p>创建一个测试Topic：</p><pre><code>[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --create --zookeeper 192.168.136.1:2181/kafka --replication-factor 1 --partitions 3 --topic maxwellCreated topic &quot;maxwell&quot;.[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --list --zookeeper 192.168.136.1:2181/kafka__consumer_offsetsmaxwell</code></pre></li><li><p>启动一个Consumer接受信息：</p><pre><code>bin/kafka-console-consumer.sh --zookeeper 192.168.136.1:2181/kafka --topic maxwell --from-beginning</code></pre></li><li><p>启动Maxwell</p><pre><code>bin/maxwell --user=&#39;maxwell&#39; --password=&#39;XXXXXX&#39; --host=&#39;127.0.0.1&#39; \   --producer=kafka --kafka.bootstrap.servers=192.168.136.1:9092 --kafka_topic=maxwell</code></pre></li><li><p>剩下同上。</p></li></ol><h3 id="Maxwell的过滤："><a href="#Maxwell的过滤：" class="headerlink" title="Maxwell的过滤："></a>Maxwell的过滤：</h3><p>参考：<a href="http://maxwells-daemon.io/filtering/" target="_blank" rel="noopener">Basic Filters</a></p><pre><code>bin/maxwell --user=&#39;maxwell&#39; --password=&#39;maxwell&#39; \--host=&#39;127.0.0.1&#39; --filter = &#39;exclude: foodb.*, include: foodb.tbl, include: foodb./table_\d+/ \--producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.136.1:9092 --kafka_topic=maxwell</code></pre><p>这个filter是白名单，这样就可以只检测部分表了。</p><h3 id="Maxwell的Bootstrap："><a href="#Maxwell的Bootstrap：" class="headerlink" title="Maxwell的Bootstrap："></a>Maxwell的Bootstrap：</h3><p>参考：<a href="http://maxwells-daemon.io/bootstrapping/" target="_blank" rel="noopener">Using the maxwell-bootstrap utility</a></p><p>HBase平台刚刚搭建起来时，涉及部分数据的全量刷，这也是选择Maxwell的理由：</p><pre><code>bin/maxwell-bootstrap --database fooDB --table barTable --where &quot;my_date &gt;= &#39;2017-01-07 00:00:00&#39;&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> MaxWell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase的读写流程</title>
      <link href="/2019/05/18/hbase-de-du-xie-liu-cheng/"/>
      <url>/2019/05/18/hbase-de-du-xie-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>HBase虽然也是分布式架构，但相比于HDFS，读写流程有点特殊，简单来说读写操作不需要走Master。Master只负责涉及表结构的变更。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217203305.png" alt="读写流程"></p><h2 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h2><ol><li><p>zookeeper：</p><p>去ZK拿到/hbase/meta-region-server。（<strong>这张表误删后，重启hbase能够自动生成。</strong>）这里面存储着hbase:meta这张表维护在那台RS节点上。（一个集群中，只有一台RS上会有这张表，这张表也是HDFS上的文件）</p><p>注：早期Hbase时-root-这张表。并且会维护多张子表（3层结构）</p></li><li><p>RegionServerA：</p><p>向RS节点获取hbase:meta表的内容，写数据根据RowKey，去meta表找到对应的RS节点和Region。（<strong>hbase:meta表误删后，能够通过命令进行恢复</strong>。）</p><p>这部分元数据会缓存到本地，供之后的读写请求使用。</p><p>meta表存储了table，region，startKey，EndKey，RS节点的映射关系。ROW的格式：table,[start key],xxxxxx.xxxxxx(Region创建时间.RegionID)</p></li><li><p>对应的RegionServerB：</p><p>将写数据请求发送给对应的RS节点。</p><p>1）先写Hlog(WAL)</p><p>2）再写对应的Region列族的MemStore。</p></li><li><p>Flush：</p><p>MemStore容量超过阈值，会异步Flush，将内存数据写入文件，为StoreFile（HFile）</p></li></ol><h2 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h2><ol><li><p>zookeeper：</p><p>拿到/hbase/meta-region-server（与写流程一样，找到hbase:meta在哪）</p></li><li><p>RegionServerA：</p><p>向RS节点获取hbase:meta的内容。根据RowKey，去meta表找到对应的RS节点和Region。</p></li><li><p>RegionServerB：</p><p>将读请求进行封装。与RS建立通信。构建Scanner。查找步骤：</p><ol><li>先去MemStore找。</li><li>再去BlockCache上找。</li><li>再去HFile找。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase概念及架构</title>
      <link href="/2019/05/01/hbase-gai-nian-ji-jia-gou/"/>
      <url>/2019/05/01/hbase-gai-nian-ji-jia-gou/</url>
      
        <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><blockquote><p>HBase版本：1.2.x</p></blockquote><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><p>HBase也是主从结构，本身主要由两部分组成：</p><ul><li>HMaster：主节点，表相关的工作都会经过Hmaster，包括表的管理、Region的管理。</li><li>HRegionServer：从节点，主要关注数据的读写操作</li></ul><h3 id="表概念"><a href="#表概念" class="headerlink" title="表概念"></a>表概念</h3><blockquote><p>HBase中表会进行分布式存储，决定因素主要是：RowKey、Column Family</p></blockquote><p>HBase中的表中一条数据主要由5部分组成：RowKey、Column Family、Column、Version Number、Value</p><ul><li>RowKey：主键。将表按照行进行切分。</li><li>Column Family：列族。将表进行切割 简称CF。一般会将一类信息存放在一个列族里。（如订单id，订单名称放在订单列族里）</li><li>Column：字段名。列。</li><li>Version Number：类型为Long，默认值为系统时间戳。表示这条数据的版本。</li><li>Value：这条数据的value</li></ul><h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p>Hbase中的最小单元，是一段数据的集合，类似于HDFS上的Block。Region存放在RegionServer节点上。</p><blockquote><p>一般RegionSrver要和DataNode在一个节点上。</p></blockquote><h4 id="物理层面的Region"><a href="#物理层面的Region" class="headerlink" title="物理层面的Region"></a>物理层面的Region</h4><p>一个RegionServer管理1<del>n个Region。一个Region管理者1</del>n个FC。</p><h4 id="逻辑层面的Region"><a href="#逻辑层面的Region" class="headerlink" title="逻辑层面的Region"></a>逻辑层面的Region</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194401.png" alt="逻辑层面的Region"></p><p>对于一张表：</p><ul><li>按照rowKey范围（Range），将一张表划分成多个Region。(Region一般会保存在不同的机器上)</li><li>Region再按照列族，分成多个Store。MemStore是Flush入文件的一个缓存区域。</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><h4 id="逻辑视图"><a href="#逻辑视图" class="headerlink" title="逻辑视图"></a>逻辑视图</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194541.png" alt="逻辑视图"></p><p>整个数据可以看作一张表，数据按照RowKey，字典排序。HBase不存储null值，整个表是以稀疏表的方式存储。</p><h4 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217194602.png" alt="物理视图"></p><p>每个数据其实本质都是按照k-v进行存储的，不同的CF存储在不同的文件中。（图中左右是两个文件，但其实是一张表里。）</p><h5 id="多版本"><a href="#多版本" class="headerlink" title="多版本"></a>多版本</h5><p>上图中，row2存在多版本数据。小王是新的数据。</p><p>Hbase中一般只有insert。</p><ul><li><p>put：insert+update</p><ul><li><p>rowKey不存在：插入一条新数据</p></li><li><p>rowKey存在：插入新数据，但保留原来的数据，两者主要是TimeStamp和Value不同。TimeStamp一定程度反映了数据的版本。</p><p>hbase会默认返回新版本的数据，当然，老版本的数据也能查得到</p></li></ul></li><li><p>delete：插入一条打上的delete标签的数据。并将之前版本的数据都置为不可见。合并时才真正删除数据。</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Hbase适合写多读少的业务，适合update少的业务（因为update其实是插入等量的数据）</li><li>Hbase中，同一张表的数据往往会存放在不同的节点上。</li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217200948.png" alt="HBase架构"></p><p>HBase的架构中，有一个至关重要的角色：Zookeeper。</p><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><p><strong>Client的所有操作第一步都是Zookeeper</strong></p><p>功能：</p><ul><li>存储Meta表所在的RS节点。存储Hmaster地址。</li><li>RS会主动向ZK注册，使得HMaster可以随时感知RS的健康状态。</li><li>构建HMaster的HA，避免单点故障。</li></ul><h3 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h3><p>表相关的工作都会经过Hmaster，包括：</p><ul><li>Table的管理：包括建表、修改列族配置等。</li><li>Region的管理：分配Region到具体的RegionServer、Region的分割和合并、rs挂的时候，Region的迁移等。</li></ul><h3 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h3><p>主要负责：数据的路由（数据具体写在哪个Region）、数据的读写、数据的持久化–&gt;数据以HFile存储在HDFS上。</p><p>HRegionServer是内容比较多的一个部分。包含：</p><ul><li>HLog（WAL）：预写日志。</li><li>BlockCache：读缓存。</li><li>HRegion：一个HRegion只属于一张表。<ul><li>Store：由MemStore和StoreFile/Hfile组成。</li></ul></li></ul><h4 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h4><ul><li>一个HRegionServer上，同一时间只能写一份HLog（但可以存多个HLog）（其实就是HDFS上的一个文件）<ul><li>HLog存放在HDFS上，并分成两个文件夹存放：WALs 和 oldWALs。</li></ul></li><li>写数据前，总时先写WAL，再写MemStore。</li><li>充当灾难恢复的作用。</li><li>HLog中的所有日志记录都已经落盘到HFile，则失效，移入oldWALs文件夹中。</li><li>满足确认删除条件，则删除oldWALs下的Hlog<ul><li>HLog是否还在参与主从复制。</li><li>文件存在于oldWALs时间是否已超过10分钟。</li></ul></li></ul><h4 id="BlockCache"><a href="#BlockCache" class="headerlink" title="BlockCache"></a>BlockCache</h4><p>Client的读请求，会将从本RegionServer在Hfile上读的数据写入BlockCache。一个Rs只有一个BlockCache。启动时完成初始化。</p><p>读请求时，BlockCache已经缓存的数据，就不用再去Hfile找了。很久没用的数据会被新数据替换掉。</p><h4 id="HRegion"><a href="#HRegion" class="headerlink" title="HRegion"></a>HRegion</h4><p>Region由许多许多的Store组成，<strong>不同的列簇会拆成不同的Store</strong>。</p><p>Store中，又由MemStore和StoreFile/HFile组成。</p><ul><li>MemStore：写入缓存。写入数据时，先有序写入该缓存，满了后Flush成为一个HFile文件。<ul><li>读的时候也会用！</li></ul></li><li>HFile：其实就是对应于HDFS上的文件。<ul><li>HFile本身只有一份，不过HDFS自己会弄成3副本保存。</li><li>HFile数量增加到阈值时，会触发Compaction合并。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM中的垃圾回收</title>
      <link href="/2019/02/18/jvm-zhong-de-la-ji-hui-shou/"/>
      <url>/2019/02/18/jvm-zhong-de-la-ji-hui-shou/</url>
      
        <content type="html"><![CDATA[<h2 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h2><h3 id="JVM的垃圾回收针对哪些内容"><a href="#JVM的垃圾回收针对哪些内容" class="headerlink" title="JVM的垃圾回收针对哪些内容"></a>JVM的垃圾回收针对哪些内容</h3><p>Java堆、方法栈。</p><h3 id="如何定位垃圾"><a href="#如何定位垃圾" class="headerlink" title="如何定位垃圾"></a>如何定位垃圾</h3><ol><li><p>引用计数法（已经基本不用）</p><p>每个对象都有一个引用计数器。被引用是计数器+1。引用失效时，计数器-1。回收计数器为0的对象。</p><p>存在问题：两个对象相互引用时，无法回收。</p></li><li><p>跟搜索法</p><p>以一系列，名为GC Root的对象作为起始点。GCRoot无法到达的对象（无引用关系的对象），即为可被回收的。</p><p>GCRoot是一种对象，可能是：</p><ul><li>JVM栈中的引用的对象。</li><li>方法区中静态属性（Static）引用的对象。</li><li>等等….</li></ul></li></ol><h3 id="方法区回收了什么"><a href="#方法区回收了什么" class="headerlink" title="方法区回收了什么"></a>方法区回收了什么</h3><ol><li>废弃的常量。如没有任何String对象引用的String常量。</li><li>无用的类：同时满足：<ol><li>Java堆中不存在该类的任何实例对象；</li><li>加载该类的类加载器（ClassLoader）已经被回收；</li><li>该类对应的java.lang.Class对象不在任何地方被引用，且无法在任何地方通过反射访问该类的方法。</li></ol></li></ol><h3 id="垃圾清除算法"><a href="#垃圾清除算法" class="headerlink" title="垃圾清除算法"></a>垃圾清除算法</h3><p>主要由三种算法，在垃圾回收中会结合起来使用。</p><h4 id="标记清除"><a href="#标记清除" class="headerlink" title="标记清除"></a>标记清除</h4><p>标记出需要回收的对象。统一回收标记了的对象。</p><p>缺点：清除后，存活对象所占的内存位置不变，会产生<strong>内存碎片</strong>。new大对象可能会触发full GC</p><h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><p>将整个内存空间均分为两块，同时只使用一块。GC时，存活对象复制到另一块区中，清空正在使用块中的所有对象。这样就解决了内存碎片问题，回收只需要移动堆顶指针，效率高。</p><p>缺点：只能使用一半的内存空间。</p><h4 id="标记整理"><a href="#标记整理" class="headerlink" title="标记整理"></a>标记整理</h4><p>在标记-清除的基础上。将存活对象全部移动到一侧。</p><p>缺点：效率低。</p><hr><h2 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h2><p>垃圾回收器中整体分为老年代和新生代。新生代存放比较新的对象，这部分对象的特点就是经常被回收。老年代的对象一般是很久都没被回收的对象。</p><p>默认情况下，内存比率：老年代:新生代=2:1。新生代内存比率：eden:s0:s1 = 8:1:1。</p><p>新对象会new在eden区。每次新生代回收后，存活的对象年龄+1，进入s0/s1区。年龄到15后，进入老年代。</p><p>一般来说，老年代和新生代会采用不同的垃圾回收器。JDK7以后引入了G1回收，会在最后单独讨论。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218114634.png" alt="垃圾回收对应关系"></p><h3 id="新生代的垃圾回收器"><a href="#新生代的垃圾回收器" class="headerlink" title="新生代的垃圾回收器"></a>新生代的垃圾回收器</h3><h4 id="Serial（单线程）"><a href="#Serial（单线程）" class="headerlink" title="Serial（单线程）"></a>Serial（单线程）</h4><p>基于复制算法。GC靠单线程完成工作。触发时，会<strong>stw</strong>，导致其他工作线程暂停。</p><p>开启：-XX:UseSerialGC（同时老年代会采用Serial Old（单线程的老年代版本））</p><h4 id="ParNew（多线程版Serial）"><a href="#ParNew（多线程版Serial）" class="headerlink" title="ParNew（多线程版Serial）"></a>ParNew（多线程版Serial）</h4><p>基于复制算法。GC靠多线程完成工作。触发时，会<strong>stw</strong>，导致其他工作线程暂停。</p><p>开启：-XX:+UseParNewGC（同时老年代会采用Serial Old（注意，单线程））</p><p>注：但可以和CMS配合使用（需要通过老年代开启CMS收集器，这边开启不行）</p><h4 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h4><p>基于复制算法。可看作ParNew的加强版。<strong>重视吞吐量</strong>的垃圾收集器。吞吐量 = 运行代码时间/ (运行代码时间+GC时间)，适合后台运算而无需太多交互的任务。<strong>是JDK8的默认新生代收集器</strong>。触发时，会“stw”，导致其他工作线程暂停。</p><p>开启：+XX:UseParallelGC（老年代会使用Parallel Old（Parallel Scavenge的老年代版本））</p><h3 id="老年代的垃圾回收器"><a href="#老年代的垃圾回收器" class="headerlink" title="老年代的垃圾回收器"></a>老年代的垃圾回收器</h3><h4 id="Serial-old"><a href="#Serial-old" class="headerlink" title="Serial old"></a>Serial old</h4><p>使用标记整理算法。单线程的。主要是作为CMS的后备方案使用。</p><p>开启参数JDK8已移除。</p><h4 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h4><p>基于标记整理算法。其他基本同：Parallel Scavenge收集器</p><p>开启：-XX:+UseParallelOldGC（会同时让新生代开启Parallel Scavenge收集器）</p><h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h4><p>基于标记清除算法。分为4大阶段：（1,3阶段速度快，会触发stw）</p><pre><code>- 初始标记：仅标记GC ROOT能直接关联的对象。速度很快，会触发stw。- 并发标记：进行GC Root Tracing过程（即跟搜索过程）。速度慢，不会触发stw。- 重新标记：修改并发标记期间，由于程序运行额外产生的变化。速度快，会触发stw。- 并发清除：清除工作。速度慢，不会触发stw。</code></pre><p>重视短暂停的垃圾处理器。可以近似看做是并发的垃圾处理器（用户进程和GC进程能够同时运行）</p><p>开启：-XX:+UseConcMarkSweepGC（会同时让新生代采用ParNewGC）（同时让老年代采用Serial Old作为备选GC）</p><p>缺点：</p><ul><li>GC时耗费CPU性能。</li><li>无法处理浮动垃圾（即GC处理过程中产生的垃圾）</li><li>会产生大量空间碎片。可通过参数缓解：<ul><li>-XX:Usecmscompactfullcollection：Full GC后会进行碎片整理</li><li>-XX:CMSFULLGCSBeforeCompaction：多少次FullGC进行碎片整理。</li></ul></li></ul><h3 id="G1垃圾回收器"><a href="#G1垃圾回收器" class="headerlink" title="G1垃圾回收器"></a>G1垃圾回收器</h3><h4 id="区域划分"><a href="#区域划分" class="headerlink" title="区域划分"></a>区域划分</h4><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218120617.png" alt="G1垃圾回收器"></p><p>G1将整个内存区域划成等大小的块。一般约为2000个。有些块是年轻代的Eden、survivor，有些则是老年代。（当前块所属的代并不是确定的，可能现在是Eden，明天就是老年代，后天就是survivor）</p><p>块之间是不相连的，意味这一个对象不能跨块保存。垃圾回收是按照区域来回收的，即回收区域A的同时，不会回收区域B。（不过在一整个GC中，可能回收多个区域）</p><h4 id="回收阶段划分"><a href="#回收阶段划分" class="headerlink" title="回收阶段划分"></a>回收阶段划分</h4><ol><li><p>年轻代的回收</p><p>所有的eden区满后，触发young GC。young GC会回收eden区，上次存放存活对象的survivor区。young GC后，两个区中存活的对象会被copy到一个新的survivor区，eden区则会被清空。达到年龄阈值的对象则会被送往老年区。<strong>年轻代的回收会stw</strong></p></li><li><p>并发标记周期</p><p>当对象占用空间的总和，达到整个堆的大小的45%时触发该阶段。（45%可调整）</p><p>标记周期分为6个步骤：</p><table><thead><tr><th></th><th></th><th>重要特点</th></tr></thead><tbody><tr><td>初始标记</td><td>标记从GC Root<strong>直接可达</strong>的对象。触发这个步骤时，会<strong>伴随一个youngGC</strong>。</td><td>需要stw</td></tr><tr><td>根区域扫描</td><td>扫描由survivor区域直接可达的老年代区域。并标记这些直接可达的对象。<br />该步骤必须在下一次youngGC前完成（即下一次youngGC会等待该步骤完成再进行。）</td><td>不需要stw<br />必须在下一次youngGC前完成</td></tr><tr><td>并发标记</td><td>扫描<strong>整个堆中</strong>的存活对象，并标记。<br />该步骤可以被young GC打断。</td><td>不需要stw<br />可被youngGC打断</td></tr><tr><td>重新标记</td><td>重新标记并发标记期间产生的变化。<br />采用SATB算法（初始快照-G1标记之初为存货对象创建了一个快照。）</td><td>需要stw</td></tr><tr><td>独占清理</td><td>1）计算每个区域中，存活对象与GC回收对象的比例，并排序，识别出可供混合回收的区域。<br />2）标记需要混合回收的区域。<br />3）更新记忆集(RSet)</td><td>需要stw<br />混合回收阶段依靠与这步的标记</td></tr><tr><td>并发清理</td><td>识别清理完全空闲的区域。</td><td>不需要stw</td></tr></tbody></table><p>并发标记的结果和目的：</p><pre><code>- 并发标记的结果时增加了一些标记为G的区域。这些区域的内部垃圾占比高。会在之后的混合回收中清除。- 这些G区域，被G1记录在Collection Sets（回收集）中。- 并发标记阶段，G1并不会回收老年代的垃圾，只是为之后的混合回收做好标记。</code></pre></li><li><p>混合回收</p><p>混合回收会执行多次，每次也会伴随youngGC，也会有OldGC。可以理解为这个阶段，mixedGC（youngGC+oldGC）替代了youngGC。</p><p>mixedGC后：eden区必被清空，存活对象会前往survivor（youngGC），标记为G的分区会被选取一些来清空。存活对象会移到其他没被标记为G的老年代分区。</p></li><li><p>Full GC：</p><p>当混合GC时空间不足，或youngGC时survivor区和老年代无法容纳幸存对象时，会触发FullGC。FullGC是单线程的stw阶段</p></li></ol><h4 id="常用参数"><a href="#常用参数" class="headerlink" title="常用参数"></a>常用参数</h4><ul><li>-XX:+UseG1GC：开启G1回收器。</li><li>-XX:MaxGCPauseMillis：目标最大停顿时间。过小可能增加fullGC的可能性。</li><li>-XX:ParallelGCThreads：并行回收时，GC的工作现场数量。</li><li>-XX:InitiatingHeapOccupancyPercent：触发并发标记的使用率。默认为45（即堆的使用率达到45%时触发。）</li></ul>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM的运行时数据区</title>
      <link href="/2019/02/02/jvm-de-yun-xing-shi-shu-ju-qu/"/>
      <url>/2019/02/02/jvm-de-yun-xing-shi-shu-ju-qu/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>JDK8中，JVM的运行时数据区逻辑上由5个部分组成：</p><ul><li>程序计数器（PC Register）</li><li>JVM栈</li><li>本地方法栈</li><li>堆</li><li>方法区</li></ul><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218111628.png" alt="逻辑分布"></p><hr><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>主要功能：指向当前线程字节码文件执行到哪了。直白来说，就是指向当前线程执行到哪行代码了。</p><p>独有or共享：每个线程独有</p><p>相关异常：一般无异常</p><h3 id="JVM栈"><a href="#JVM栈" class="headerlink" title="JVM栈"></a>JVM栈</h3><p>主要功能：每个方法在执行时会创建一个<strong>栈帧</strong>。方法的调用和完成，相当于栈帧的入栈和出栈。</p><p>​    栈帧主要存放：<strong>局部变量表</strong>。操作栈，动态链接、方法出口。</p><p>​    局部变量表：</p><pre><code>    - 编译期可知的各种**基本数据类型**（原生类型）（boolean、byte、char、short等等）    - **对象引用**（引用类型）（User A = new User的这个A。）    - returnaddress类型。</code></pre><p>独有or共享：每个线程独有</p><p>相关异常：</p><ul><li>StackOverFlow（栈溢出）：出现于<strong>单线程阶段</strong>，一般是<strong>死循环、递归</strong>导致的方法过多。也可能是单个栈帧过大导致的。<pre><code>- 解决：检查代码，或者调大栈内存（与多线程需要平衡）</code></pre></li><li>OutOfMemoryError（OOM/内存溢出）：出现于<strong>多线程阶段</strong>，栈动态扩展时（新开线程），无法申请到足够的内存。一般可能是单个栈的内存占用过多，<strong>考虑减少堆或栈的内存来换取更多的线程。</strong><pre><code>- 提示：unable to create new native thread</code></pre><ul><li>参数：<pre><code>- -Xmx 和 -Xms。控制堆占空间的上下限- -Xss。**线程栈**的大小。不过过小的栈内存可能导致栈溢出。</code></pre></li></ul></li></ul><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>主要功能：基本与JVM栈一样（独享、异常也一样），不过是服务的方法不一样：为虚拟机使用的Native方法服务。（一般会调用非java代码）</p><hr><h3 id="堆区"><a href="#堆区" class="headerlink" title="堆区"></a>堆区</h3><p>主要功能：存放对象实例。几乎所有的对象实例都在这里分配内存。是垃圾收集器的主要管理区域。</p><p>独有or共享：共享。</p><p>相关异常：OutOfMemoryError（OOM/内存溢出）</p><pre><code>- 一般是**大量的对象**占用了对空间，且未能及时回收。或者创建的一个对象超过了堆允许的最大值。- 提示：Java heap space- 参数：-Xmx 和 -Xms。控制堆占空间的上下限。- 注：出现这种溢出，首先应该考虑代码问题。在Spark中，如果某个算子将过大对象返回给Driver，就可能遇到这种情况。</code></pre><p>​    </p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><blockquote><p>（1.7以前通过永久代实现，1.8+通过元数据区实现）</p></blockquote><p>主要功能：编译后的Class文件会加载到方法区。主要存放：类的信息（代码）、运行时常量信息（即运行时常量池）。</p><p>独有or共享：共享。</p><p>相关异常：OutOfMemoryError（OOM/内存溢出）</p><pre><code>- 一般出现于系统不断生成**新的类**，而不回收。（注意是类，而非对象。）- 提示：MetaSpace（1.8+）- 参数：-XX:MaxMetaspaceSize（不设置则仅受系统内存限制）</code></pre><h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><blockquote><p>（1.8+后，物理上，不同的JVM实现不同，有的放在了堆上）（逻辑上仍然是方法区的一部分）</p></blockquote><p>本质上属于方法区的一块。（线程共享，异常与方法区处理类似（增大方法区空间）），存放<strong>编译期</strong>生成的各种字面量和符号引用。</p><h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>并非运行时数据区的一部分。相当于直接使用系统内存，而非JVM内存。主要是NIO类会直接使用系统内存（一种基于Channel的IO方式）</p><p>相关异常：一般是过度的使用堆外内存造成的。</p><p>参数：-XX:MaxDirectMemorySize。实际使用时，应该限制堆外内存的使用。如果未设置，该数值=堆的最大值</p><hr><h3 id="关于方法区"><a href="#关于方法区" class="headerlink" title="关于方法区"></a>关于方法区</h3><p><strong>方法区</strong>在不同的JDK版本中有不同的实现：</p><ul><li>JDK7以前：永久代，存放在堆上。</li><li>JDK8：原空间，存储在堆外内存上。</li><li>方法区中的<strong>运行时常量池</strong>物理上的存放位置并没有跟着方法区走，在JDK8中，其存放在Java堆上。</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>运行时数据区主要是<strong>逻辑上的分区</strong>。具体实现上会有一些区别（其实主要体现在方法区上。）</li><li><strong>栈管运行，堆管存储</strong>。两者都使用JVM内部虚拟内存。。只有方法区的实现源空间（MetaSpace）会使用JVM外部的物理内存。</li><li>一般涉及多线程的情况会导致OOM（如线程共享的区OOM，线程独立的区在多线程情况下回OOM）。单线程一般会栈溢出。PC计数器是唯一没有异常的区域。</li></ol><p>参考图：</p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200218113235.png" alt="运行时数据区"  />]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中的窗口函数使用</title>
      <link href="/2018/12/31/spark-zhong-de-chuang-kou-han-shu-shi-yong/"/>
      <url>/2018/12/31/spark-zhong-de-chuang-kou-han-shu-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>spark自1.4版本后就支持了window function。与sql里的xxx over xxx类似，便于人们进行聚合参数。<br>本文主要是学习时的一些理解，已经遇到的一些注意点。<br>有错误望指正。</p><h2 id="创建window"><a href="#创建window" class="headerlink" title="创建window"></a>创建window</h2><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">//需要import这个包：</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>expressions<span class="token punctuation">.</span>Window<span class="token comment" spellcheck="true">//按照category分组，按照revenue排序</span>val myWindow <span class="token operator">=</span> Window<span class="token punctuation">.</span><span class="token function">partitionBy</span><span class="token punctuation">(</span>$<span class="token string">"category"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">orderBy</span><span class="token punctuation">(</span>$<span class="token string">"revenue"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//之后在函数后使用.over即可：</span><span class="token function">sum</span><span class="token punctuation">(</span>$<span class="token string">"revenue"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">over</span><span class="token punctuation">(</span>myWindow<span class="token punctuation">)</span></code></pre><p>这里就已经建立了一个简单的window函数了。</p><h2 id="支持的函数："><a href="#支持的函数：" class="headerlink" title="支持的函数："></a>支持的函数：</h2><p>SparkSql支持3类窗口函数：ranking functions、analytic functions、aggregate functions<br>前两者只有某些函数能够支持：</p><h3 id="1-Ranking-functions：（排名）"><a href="#1-Ranking-functions：（排名）" class="headerlink" title="1.Ranking functions：（排名）"></a>1.Ranking functions：（排名）</h3><table><thead><tr><th>SQL</th><th>API</th><th>含义</th></tr></thead><tbody><tr><td>rank</td><td>rank</td><td>排名第几，若值相等，rank相同，下一个值不同的行，rank+n</td></tr><tr><td>dense_rank</td><td>denseRank</td><td>排名第几，若值相等，rank相同，下一个值不同的行，rank+1</td></tr><tr><td>row_number</td><td>rowNumber</td><td>当前数据位于第几行，值相同，也会+1</td></tr><tr><td>percent_rank</td><td>percentRank</td><td>排名百分比，分组内当前行的RANK值-1/分组内总行数-1</td></tr><tr><td>ntile</td><td>ntile</td><td>用于将分组数据按照顺序切分成n片，返回当前切片值</td></tr></tbody></table><h3 id="2-Analytic-functions（解析）"><a href="#2-Analytic-functions（解析）" class="headerlink" title="2.Analytic functions（解析）"></a>2.Analytic functions（解析）</h3><table><thead><tr><th>SQL</th><th>API</th><th>含义</th></tr></thead><tbody><tr><td>fiset_value</td><td>first</td><td>第一条记录的value</td></tr><tr><td>last_value</td><td>last</td><td>最后记录的value</td></tr><tr><td>cume_dist</td><td>cume_dist</td><td>小于等于当前值的行数/分组内总行数</td></tr><tr><td>lag</td><td>lag</td><td>延迟offset位</td></tr><tr><td>lead</td><td>lead</td><td>领先offset位</td></tr><tr><td>Analytic不能理解的可以结合代码和运行结果看看：</td><td></td><td></td></tr><tr><td>```java</td><td></td><td></td></tr><tr><td>//window按照category分组，revenue排序</td><td></td><td></td></tr><tr><td>readDF</td><td></td><td></td></tr><tr><td>.withColumn(“first”,first(“revenue”).over(myWindow))</td><td></td><td></td></tr><tr><td>.withColumn(“last”,last(“revenue”).over(myWindow))</td><td></td><td></td></tr><tr><td>.withColumn(“cumeDist”,cume_dist().over(myWindow))</td><td></td><td></td></tr><tr><td>.withColumn(“lag”,lag(“revenue”,1,0).over(myWindow))</td><td></td><td></td></tr><tr><td>.withColumn(“lead”,lead(“revenue”,1,0).over(myWindow))</td><td></td><td></td></tr><tr><td>.show()</td><td></td><td></td></tr><tr><td>```</td><td></td><td></td></tr><tr><td><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/1.png" alt="Analytic"></td><td></td><td></td></tr></tbody></table><h3 id="3-Aggregate（聚合）"><a href="#3-Aggregate（聚合）" class="headerlink" title="3. Aggregate（聚合）"></a>3. Aggregate（聚合）</h3><p>所有支持的函数都能够使用窗口函数（sum、avg、max、min），一般会结合row/rangeBetween来一起使用。</p><h2 id="window函数"><a href="#window函数" class="headerlink" title="window函数"></a>window函数</h2><p>前两个意义比较简单：</p><h3 id="1-partitionBy"><a href="#1-partitionBy" class="headerlink" title="1. partitionBy"></a>1. partitionBy</h3><p>按照哪个值进行分区。其实就是建立一个窗口，范围是分区。</p><h3 id="2-orderBy"><a href="#2-orderBy" class="headerlink" title="2. orderBy"></a>2. orderBy</h3><p>按照哪个值在分区内进行排序。</p><h3 id="3-rowBetween"><a href="#3-rowBetween" class="headerlink" title="3. rowBetween"></a>3. rowBetween</h3><pre class=" language-java"><code class="language-java">val myWindow <span class="token operator">=</span> Window<span class="token punctuation">.</span><span class="token function">partitionBy</span><span class="token punctuation">(</span>$<span class="token string">"category"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">orderBy</span><span class="token punctuation">(</span>$<span class="token string">"revenue"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">rowsBetween</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>图引用自：<a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">Introducing Window Functions in Spark SQL</a><br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231350.png" alt="rowBetween"><br>以当前行为标准，将前1行和后1行的数据组成一个窗口，size=3。<br>这个比较好理解。<br>示例看下：</p><pre class=" language-java"><code class="language-java">readDF<span class="token punctuation">.</span><span class="token function">withColumn</span><span class="token punctuation">(</span><span class="token string">"sum"</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token string">"revenue"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">over</span><span class="token punctuation">(</span>myWindow<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>结果：sum会计算当前行+前一行+后一行的和。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/2.png" alt="Aggregate"></p><h3 id="4-rangeBetween"><a href="#4-rangeBetween" class="headerlink" title="4. rangeBetween"></a>4. rangeBetween</h3><p>根据当前值，来确定窗口</p><pre class=" language-java"><code class="language-java">val myWindow <span class="token operator">=</span> Window<span class="token punctuation">.</span><span class="token function">partitionBy</span><span class="token punctuation">(</span>$<span class="token string">"category"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">orderBy</span><span class="token punctuation">(</span>$<span class="token string">"revenue"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">rangeBetween</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2000</span><span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">)</span></code></pre><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231424.png" alt="rangeBetween">几个注意点：<br>    1. 因为会使用值来加减确定窗口，所以sum(colA).over(window)，这个sum必须是值类型。<br>    2. 如果前方进行你是倒序排序，整个窗口会反过来。即=&gt;(-1000,2000) 应该理解成 (-2000,1000)的窗口。</p><p>看下示例：</p><pre class=" language-java"><code class="language-java">readDF<span class="token punctuation">.</span><span class="token function">withColumn</span><span class="token punctuation">(</span><span class="token string">"sum"</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token string">"revenue"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">over</span><span class="token punctuation">(</span>myWindow<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//示例的window范围：（-1000，2000）</span></code></pre><p>结果：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/3.png" alt="rangeBetween"></p><p>参考：<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">1.Introducing Window Functions in Spark SQL</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkContext执行流程</title>
      <link href="/2018/10/01/sparkcontext-zhi-xing-liu-cheng/"/>
      <url>/2018/10/01/sparkcontext-zhi-xing-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>其实这个也算是Spark任务提交的内容之一。Spark任务的关键就是构建DAG图，Action触发然后去解析DAG图，将Stage交给Executor处理，executor处理完成后返回结果。<br>Spark程序都是以SparkContext为入口，触发Action算子后执行具体任务，这边也会顺着这条思路写下去。</p><blockquote><p>部分源码为了方便阅读会进行精简或调整，与真实源码会略有不同。但整体思路是一样的。<br>Spark版本参照最新的2.3.x<br>涉及模式主要介绍Yarn和Local</p></blockquote><h1 id="流程图概览"><a href="#流程图概览" class="headerlink" title="流程图概览"></a>流程图概览</h1><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217224954.png" alt="流程图"><br>先看图，整个流程可以分成四个步骤，具体会在源码中有所展现：</p><ol><li>根据我们的各种算子生成DAG图</li><li>解析DAG图，拆分成Stage（Task集合）。将Task集合发往下一步。</li><li>Cluster Manager来发起任务，错误或落伍(比如说数据倾斜导致某一任务始终跑不完)，会进行重试。</li><li>将Task发往具体的Worker/Container执行。</li></ol><h2 id="1）获取SparkContext"><a href="#1）获取SparkContext" class="headerlink" title="1）获取SparkContext"></a>1）获取SparkContext</h2><p>在程序里我们会通过<code>new SparkContext(Conf)</code>来得到一个SparkContext实例，具体到SparkContext里，由于代码很多，这边主要的其实就是初始化三个变量：    </p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">SparkContext</span><span class="token punctuation">(</span>config<span class="token operator">:</span> SparkConf<span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">Logging</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 初始化三个主要变量</span>    <span class="token keyword">private</span> var _schedulerBackend<span class="token operator">:</span> SchedulerBackend <span class="token operator">=</span> _    <span class="token keyword">private</span> var _taskScheduler<span class="token operator">:</span> TaskScheduler <span class="token operator">=</span> _    @<span class="token keyword">volatile</span> <span class="token keyword">private</span> var _dagScheduler<span class="token operator">:</span> DAGScheduler <span class="token operator">=</span> _    <span class="token comment" spellcheck="true">// 之后会对上述三个变量进行初始化：</span>    <span class="token function">val</span> <span class="token punctuation">(</span>sched<span class="token punctuation">,</span> ts<span class="token punctuation">)</span> <span class="token operator">=</span> SparkContext<span class="token punctuation">.</span><span class="token function">createTaskScheduler</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">,</span> master<span class="token punctuation">,</span> deployMode<span class="token punctuation">)</span>    _schedulerBackend <span class="token operator">=</span> sched    _taskScheduler <span class="token operator">=</span> ts    _dagScheduler <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DAGScheduler</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>这里代码根据具体的master、deployMode来对<code>_schedulerBackend</code>和<code>_taskScheduler</code>进行初始化，同时还会初始化<code>_dagScheduler</code></p><h3 id="1-1）其中createTaskScheduler方法"><a href="#1-1）其中createTaskScheduler方法" class="headerlink" title="1.1）其中createTaskScheduler方法"></a>1.1）其中createTaskScheduler方法</h3><p>主要用来初始化<code>_schedulerBackend</code>和<code>_taskScheduler</code>，这里以local为例：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span> def <span class="token function">createTaskScheduler</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    master match <span class="token punctuation">{</span>      <span class="token keyword">case</span> <span class="token string">"local"</span> <span class="token operator">=</span><span class="token operator">></span>        val scheduler <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">TaskSchedulerImpl</span><span class="token punctuation">(</span>sc<span class="token punctuation">,</span> MAX_LOCAL_TASK_FAILURES<span class="token punctuation">,</span> isLocal <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">)</span>        val backend <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LocalSchedulerBackend</span><span class="token punctuation">(</span>sc<span class="token punctuation">.</span>getConf<span class="token punctuation">,</span> scheduler<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        scheduler<span class="token punctuation">.</span><span class="token function">initialize</span><span class="token punctuation">(</span>backend<span class="token punctuation">)</span>        <span class="token punctuation">(</span>backend<span class="token punctuation">,</span> scheduler<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>通过这段代码，我们可以知道，一个是<code>TaskSchedulerImpl</code>的实例，一个是<code>LocalSchedulerBackend</code>的实例。里面主要就是初始化一些参数内容。具体的方法在后续过程会使用到，这里不进行过多赘述。</p><h3 id="1-2）其中new-DAGScheduler-this-里"><a href="#1-2）其中new-DAGScheduler-this-里" class="headerlink" title="1.2）其中new DAGScheduler(this)里"></a>1.2）其中new DAGScheduler(this)里</h3><p>初始化时有两个关键点：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">DAGScheduler</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> val eventProcessLoop <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DAGSchedulerEventProcessLoop</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span>    taskScheduler<span class="token punctuation">.</span><span class="token function">setDAGScheduler</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>这边会生成一个事件队列<code>eventProcessLoop</code>，同时设置我们的<code>taskScheduler</code>。这些内容会在之后使用到。<br>至此，这个阶段SparkContext的主要任务已经基本完成：设置我们上面提到的三个参数。<br>接下来，就要等到Action算子触发实际的作业运行了。</p><h2 id="2）Action"><a href="#2）Action" class="headerlink" title="2）Action"></a>2）Action</h2><p>这里以Collect算子为例，里面会调用<code>runJob</code>这个方法：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">abstract</span> <span class="token keyword">class</span> <span class="token class-name">RDD</span><span class="token punctuation">[</span>T<span class="token operator">:</span> ClassTag<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    def <span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Array<span class="token punctuation">[</span>T<span class="token punctuation">]</span> <span class="token operator">=</span> withScope <span class="token punctuation">{</span>    val results <span class="token operator">=</span> sc<span class="token punctuation">.</span><span class="token function">runJob</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>iter<span class="token operator">:</span> Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> iter<span class="token punctuation">.</span>toArray<span class="token punctuation">)</span>    Array<span class="token punctuation">.</span><span class="token function">concat</span><span class="token punctuation">(</span>results<span class="token operator">:</span> _<span class="token operator">*</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>里面点进去，实际会进行多次跳转，这边把中间过程进行省略，最后我们会来到：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">SparkContext</span><span class="token punctuation">(</span>config<span class="token operator">:</span> SparkConf<span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">Logging</span> <span class="token punctuation">{</span>  def runJob<span class="token punctuation">[</span>T<span class="token punctuation">,</span> U<span class="token operator">:</span> ClassTag<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>      val callSite <span class="token operator">=</span> getCallSite    val cleanedFunc <span class="token operator">=</span> <span class="token function">clean</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">// 前面是一些参数初始化</span>      <span class="token comment" spellcheck="true">// 核心，进入runJob。</span>      dagScheduler<span class="token punctuation">.</span><span class="token function">runJob</span><span class="token punctuation">(</span>rdd<span class="token punctuation">,</span> cleanedFunc<span class="token punctuation">,</span> partitions<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> resultHandler<span class="token punctuation">,</span> localProperties<span class="token punctuation">.</span>get<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">// 其他处理</span>    progressBar<span class="token punctuation">.</span><span class="token function">foreach</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">finishAll</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    rdd<span class="token punctuation">.</span><span class="token function">doCheckpoint</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>我们这里主要需要关注<code>dagScheduler.runJob</code>方法，跟这个这个方法，我们来到了DAGScheduler：</p><h2 id="3）DAGScheduler"><a href="#3）DAGScheduler" class="headerlink" title="3）DAGScheduler"></a>3）DAGScheduler</h2><pre class=" language-java"><code class="language-java">def runJob<span class="token punctuation">[</span>T<span class="token punctuation">,</span> U<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    val start <span class="token operator">=</span> System<span class="token punctuation">.</span>nanoTime    val waiter <span class="token operator">=</span> <span class="token function">submitJob</span><span class="token punctuation">(</span>rdd<span class="token punctuation">,</span> func<span class="token punctuation">,</span> partitions<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> resultHandler<span class="token punctuation">,</span> properties<span class="token punctuation">)</span><span class="token punctuation">{</span>        eventProcessLoop<span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token function">JobSubmitted</span><span class="token punctuation">(</span>            jobId<span class="token punctuation">,</span> rdd<span class="token punctuation">,</span> func2<span class="token punctuation">,</span> partitions<span class="token punctuation">.</span>toArray<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> waiter<span class="token punctuation">,</span>            SerializationUtils<span class="token punctuation">.</span><span class="token function">clone</span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        waiter    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这里我将两段代码进行了合并，runJob会调用<code>submitJob</code>方法，这个方法会返回一个<code>JobWaiter</code>对象，简单来说，就是将我们的Func、Rdd加入之前创建的<code>eventProcessLoop</code>。</p><h3 id="3-1）DAGSchedulerEventProcessLoop"><a href="#3-1）DAGSchedulerEventProcessLoop" class="headerlink" title="3.1）DAGSchedulerEventProcessLoop"></a>3.1）DAGSchedulerEventProcessLoop</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">DAGSchedulerEventProcessLoop</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">/**    * The main event loop of the DAG scheduler.    */</span>    override def <span class="token function">onReceive</span><span class="token punctuation">(</span>event<span class="token operator">:</span> DAGSchedulerEvent<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span>doOnReceive<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    <span class="token keyword">private</span> def <span class="token function">doOnReceive</span><span class="token punctuation">(</span>event<span class="token operator">:</span> DAGSchedulerEvent<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> event match <span class="token punctuation">{</span>        <span class="token keyword">case</span> <span class="token function">JobSubmitted</span><span class="token punctuation">(</span>jobId<span class="token punctuation">,</span> rdd<span class="token punctuation">,</span> func<span class="token punctuation">,</span> partitions<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> listener<span class="token punctuation">,</span> properties<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span>      dagScheduler<span class="token punctuation">.</span><span class="token function">handleJobSubmitted</span><span class="token punctuation">(</span>jobId<span class="token punctuation">,</span> rdd<span class="token punctuation">,</span> func<span class="token punctuation">,</span> partitions<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> listener<span class="token punctuation">,</span> properties<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这个类我同样进行了简化，实际上就是判断我们提交过来的是什么，然后指向对应的方法。</p><blockquote><p>这里我们是JobSubmitted。所以指向dagScheduler.handleJobSubmitted</p></blockquote><h3 id="3-2）handleJobSubmitted"><a href="#3-2）handleJobSubmitted" class="headerlink" title="3.2）handleJobSubmitted"></a>3.2）handleJobSubmitted</h3><p>实际上源码会有很多的try catch，这边进行部分简写方便阅读</p><pre class=" language-java"><code class="language-java">def <span class="token function">handleJobSubmitted</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    var finalStage<span class="token operator">:</span> ResultStage <span class="token operator">=</span>         <span class="token function">createResultStage</span><span class="token punctuation">(</span>finalRDD<span class="token punctuation">,</span> func<span class="token punctuation">,</span> partitions<span class="token punctuation">,</span> jobId<span class="token punctuation">,</span> callSite<span class="token punctuation">)</span>    val job <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ActiveJob</span><span class="token punctuation">(</span>jobId<span class="token punctuation">,</span> finalStage<span class="token punctuation">,</span> callSite<span class="token punctuation">,</span> listener<span class="token punctuation">,</span> properties<span class="token punctuation">)</span>    finalStage<span class="token punctuation">.</span><span class="token function">setActiveJob</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span>    <span class="token function">submitStage</span><span class="token punctuation">(</span>finalStage<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>这里首先会拿到最后一个Stage，并通过它生成running job。最终我们将finalStage提交。</p><blockquote><p>A running job in the DAGScheduler. Jobs can be of two types:<br>1）a result job, which computes a ResultStage to execute an action,<br>2）or a map-stage job, which computes the map outputs for a ShuffleMapStage before any downstream stages are submitted.</p></blockquote><h3 id="3-3）submitStage"><a href="#3-3）submitStage" class="headerlink" title="3.3）submitStage"></a>3.3）submitStage</h3><p>这个部分实际就承担了拆分Stage的职责，其实就是通过递归完成的，为了方便理解就直接搬源码了。大家可以结合提交的作业时打印的log看看。</p><pre class=" language-java"><code class="language-java">  <span class="token keyword">private</span> def <span class="token function">submitStage</span><span class="token punctuation">(</span>stage<span class="token operator">:</span> Stage<span class="token punctuation">)</span> <span class="token punctuation">{</span>    val jobId <span class="token operator">=</span> <span class="token function">activeJobForStage</span><span class="token punctuation">(</span>stage<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>jobId<span class="token punctuation">.</span>isDefined<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token function">logDebug</span><span class="token punctuation">(</span><span class="token string">"submitStage("</span> <span class="token operator">+</span> stage <span class="token operator">+</span> <span class="token string">")"</span><span class="token punctuation">)</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">waitingStages</span><span class="token punctuation">(</span>stage<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span><span class="token function">runningStages</span><span class="token punctuation">(</span>stage<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span><span class="token function">failedStages</span><span class="token punctuation">(</span>stage<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        val missing <span class="token operator">=</span> <span class="token function">getMissingParentStages</span><span class="token punctuation">(</span>stage<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">sortBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>id<span class="token punctuation">)</span>        <span class="token function">logDebug</span><span class="token punctuation">(</span><span class="token string">"missing: "</span> <span class="token operator">+</span> missing<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>missing<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token function">logInfo</span><span class="token punctuation">(</span><span class="token string">"Submitting "</span> <span class="token operator">+</span> stage <span class="token operator">+</span> <span class="token string">" ("</span> <span class="token operator">+</span> stage<span class="token punctuation">.</span>rdd <span class="token operator">+</span> <span class="token string">"), which has no missing parents"</span><span class="token punctuation">)</span>          <span class="token function">submitMissingTasks</span><span class="token punctuation">(</span>stage<span class="token punctuation">,</span> jobId<span class="token punctuation">.</span>get<span class="token punctuation">)</span>        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>          <span class="token keyword">for</span> <span class="token punctuation">(</span>parent <span class="token operator">&lt;</span><span class="token operator">-</span> missing<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token function">submitStage</span><span class="token punctuation">(</span>parent<span class="token punctuation">)</span>          <span class="token punctuation">}</span>          waitingStages <span class="token operator">+=</span> stage        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>      <span class="token function">abortStage</span><span class="token punctuation">(</span>stage<span class="token punctuation">,</span> <span class="token string">"No active job for stage "</span> <span class="token operator">+</span> stage<span class="token punctuation">.</span>id<span class="token punctuation">,</span> None<span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span></code></pre><p>方法首先接收到的是finalStage，关键是<code>getMissingParentStages(stage)</code>，方法返回<code>List[Stage]</code>。这里能够得到尚未执行的ParentStage，并按照id排序。<br>所以这边的逻辑实际是：提交finalStage → 尚有父Stage没执行 → 提交父Stage，递归，直到所有父Stage都执行。<br>最终，对于每个Stage，底层都会来到<code>submitMissingTasks(stage, jobId.get)</code></p><h3 id="3-4）submitMissingTasks"><a href="#3-4）submitMissingTasks" class="headerlink" title="3.4）submitMissingTasks"></a>3.4）submitMissingTasks</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span> def <span class="token function">submitMissingTasks</span><span class="token punctuation">(</span>stage<span class="token operator">:</span> Stage<span class="token punctuation">,</span> jobId<span class="token operator">:</span> Int<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//..省略一些前置处理..</span>    <span class="token comment" spellcheck="true">//..主要就是将Stage包装成Tasks用来提交</span>    val tasks<span class="token operator">:</span> Seq<span class="token punctuation">[</span>Task<span class="token punctuation">[</span>_<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token keyword">try</span> <span class="token punctuation">{</span>        stage match <span class="token punctuation">{</span>        <span class="token keyword">case</span> stage<span class="token operator">:</span> ShuffleMapStage <span class="token operator">=</span><span class="token operator">></span>          stage<span class="token punctuation">.</span>pendingPartitions<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span>          partitionsToCompute<span class="token punctuation">.</span>map <span class="token punctuation">{</span> id <span class="token operator">=</span><span class="token operator">></span>            <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>            <span class="token keyword">new</span> <span class="token class-name">ShuffleMapTask</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>          <span class="token punctuation">}</span>        <span class="token keyword">case</span> stage<span class="token operator">:</span> ResultStage <span class="token operator">=</span><span class="token operator">></span>          partitionsToCompute<span class="token punctuation">.</span>map <span class="token punctuation">{</span> id <span class="token operator">=</span><span class="token operator">></span>            <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>            <span class="token keyword">new</span> <span class="token class-name">ResultTask</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>          <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//..提交tasks..//</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>tasks<span class="token punctuation">.</span>size <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        taskScheduler<span class="token punctuation">.</span><span class="token function">submitTasks</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">TaskSet</span><span class="token punctuation">(</span>            tasks<span class="token punctuation">.</span>toArray<span class="token punctuation">,</span> stage<span class="token punctuation">.</span>id<span class="token punctuation">,</span> stage<span class="token punctuation">.</span>latestInfo<span class="token punctuation">.</span>attemptNumber<span class="token punctuation">,</span> jobId<span class="token punctuation">,</span> properties<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>调用<code>taskScheduler</code>的<code>submitTasks</code>方法进行task的提交，这段方法的用途注释已经说明，其实目的就是将Stage包装成Task集合。里面有大量的判断这边就忽略了。<br>其实刚刚也提到过，Task在内部被Spark分为中间的：ShuffleMap，以及最后的：Result。这个也对应的不同的Job。<br>代码跟着就来到了TasklScheduler</p><h2 id="4）TaskScheduler"><a href="#4）TaskScheduler" class="headerlink" title="4）TaskScheduler"></a>4）TaskScheduler</h2><p>实际上TaskScheduler是一个trait，通过之前的分析(初始化SparkContext)，我们可以知道实际上我们建立的是一个<code>TaskSchedulerImpl</code>对象，实际上Spark本身就实现了这么一个子类。这里直接定位到对应方法即可。</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">TaskSchedulerImpl</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    override def <span class="token function">submitTasks</span><span class="token punctuation">(</span>taskSet<span class="token operator">:</span> TaskSet<span class="token punctuation">)</span> <span class="token punctuation">{</span>        val tasks <span class="token operator">=</span> taskSet<span class="token punctuation">.</span>tasks        <span class="token keyword">this</span><span class="token punctuation">.</span><span class="token keyword">synchronized</span> <span class="token punctuation">{</span>            val manager <span class="token operator">=</span> <span class="token function">createTaskSetManager</span><span class="token punctuation">(</span>taskSet<span class="token punctuation">,</span> maxTaskFailures<span class="token punctuation">)</span>            val stage <span class="token operator">=</span> taskSet<span class="token punctuation">.</span>stageId            val stageTaskSets <span class="token operator">=</span>                taskSetsByStageIdAndAttempt<span class="token punctuation">.</span><span class="token function">getOrElseUpdate</span><span class="token punctuation">(</span>stage<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">HashMap</span><span class="token punctuation">[</span>Int<span class="token punctuation">,</span> TaskSetManager<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true">// 建立taskset与manager 的对应关系</span>            <span class="token function">stageTaskSets</span><span class="token punctuation">(</span>taskSet<span class="token punctuation">.</span>stageAttemptId<span class="token punctuation">)</span> <span class="token operator">=</span> manager            <span class="token comment" spellcheck="true">// TaskSetManager会被放入调度池中。</span>            schedulableBuilder<span class="token punctuation">.</span><span class="token function">addTaskSetManager</span><span class="token punctuation">(</span>manager<span class="token punctuation">,</span> manager<span class="token punctuation">.</span>taskSet<span class="token punctuation">.</span>properties<span class="token punctuation">)</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 为tasks分配资源，调度任务</span>        backend<span class="token punctuation">.</span><span class="token function">reviveOffers</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这里会初始化一个Manager对Tasks进行管理，在最开始也说过，对于失败或者落队的任务Manager会进行重试，并与上一级交互执行情况。不过对于本文章讨论的流程来说，这部分实际上最重要的，就是将Task提交到具体的Executor。也就是<code>backend.reviveOffers()</code>。<br>对于不同的任务，SchedulerBackend有两个实现类：</p><ul><li>CoarseGrainedSchedulerBackend</li><li>LocalSchedulerBackend。</li></ul><p>这边就以CoarseGrainedSchedulerBackend为例进行继续讲解。他们的区别从命名其实就能猜到。</p><h3 id="4-1）CoarseGrainedSchedulerBackend"><a href="#4-1）CoarseGrainedSchedulerBackend" class="headerlink" title="4.1）CoarseGrainedSchedulerBackend"></a>4.1）CoarseGrainedSchedulerBackend</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">CoarseGrainedSchedulerBackend</span><span class="token punctuation">(</span>scheduler<span class="token operator">:</span> TaskSchedulerImpl<span class="token punctuation">,</span> val rpcEnv<span class="token operator">:</span> RpcEnv<span class="token punctuation">)</span>  <span class="token keyword">extends</span> <span class="token class-name">ExecutorAllocationClient</span> with SchedulerBackend with Logging <span class="token punctuation">{</span>    override def <span class="token function">reviveOffers</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        driverEndpoint<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span>ReviveOffers<span class="token punctuation">)</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// driverEndpoint的初始化，都在CoarseGrainedSchedulerBackend类里。</span>    override def <span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        val properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayBuffer</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span>        driverEndpoint <span class="token operator">=</span> <span class="token function">createDriverEndpointRef</span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span>    <span class="token punctuation">}</span>        <span class="token keyword">protected</span> def <span class="token function">createDriverEndpointRef</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token operator">:</span> RpcEndpointRef <span class="token operator">=</span> <span class="token punctuation">{</span>        rpcEnv<span class="token punctuation">.</span><span class="token function">setupEndpoint</span><span class="token punctuation">(</span>ENDPOINT_NAME<span class="token punctuation">,</span> <span class="token function">createDriverEndpoint</span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    <span class="token keyword">protected</span> def <span class="token function">createDriverEndpoint</span><span class="token punctuation">(</span>properties<span class="token operator">:</span> Seq<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> DriverEndpoint <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token keyword">new</span> <span class="token class-name">DriverEndpoint</span><span class="token punctuation">(</span>rpcEnv<span class="token punctuation">,</span> properties<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>由于<code>reviveOffers</code>方法里会使用到<code>driverEndpoint</code>，这边一并将他的初始化源码附上。通过源码我们也能注意到，这里主要是<code>DriverEndpoint</code>完成了实际工作，毕竟，我们要发送ReviveOffers事件，这个事件从哪来的？进入DriverEndPoint一探究竟。</p><blockquote><p>DriverEndpoint是CoarseGrainedSchedulerBackend的一个内部类。</p></blockquote><h3 id="4-2）DriverEndpoint"><a href="#4-2）DriverEndpoint" class="headerlink" title="4.2）DriverEndpoint"></a>4.2）DriverEndpoint</h3><p>实际上，driverEndpoint.send(ReviveOffers)会触发DriverEndPoint的makeOffers方法。</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">DriverEndpoint</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">ThreadSafeRpcEndpoint</span> with Logging <span class="token punctuation">{</span>    override def receive<span class="token operator">:</span> PartialFunction<span class="token punctuation">[</span>Any<span class="token punctuation">,</span> Unit<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token keyword">case</span> ReviveOffers <span class="token operator">=</span><span class="token operator">></span>            <span class="token function">makeOffers</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 调用makeOffers</span>    <span class="token keyword">private</span> def <span class="token function">makeOffers</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 主要是找出要在哪些Worker上启动哪些task。</span>        val taskDescs <span class="token operator">=</span> withLock <span class="token punctuation">{</span>            scheduler<span class="token punctuation">.</span><span class="token function">resourceOffers</span><span class="token punctuation">(</span>workOffers<span class="token punctuation">)</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>taskDescs<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token function">launchTasks</span><span class="token punctuation">(</span>taskDescs<span class="token punctuation">)</span>          <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>resourceOffers返回的是Seq[Seq[TaskDescription]] 类型，主要用途是找出要在哪些Worker上启动哪些task。最后调用launchTasks(taskDescs)。</p><h3 id="4-3）launchTasks"><a href="#4-3）launchTasks" class="headerlink" title="4.3）launchTasks"></a>4.3）launchTasks</h3><p>主要完成序列化，发送任务的工作：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span> def <span class="token function">launchTasks</span><span class="token punctuation">(</span>tasks<span class="token operator">:</span> Seq<span class="token punctuation">[</span>Seq<span class="token punctuation">[</span>TaskDescription<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>task <span class="token operator">&lt;</span><span class="token operator">-</span> tasks<span class="token punctuation">.</span>flatten<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 序列化Task</span>        val serializedTask <span class="token operator">=</span> TaskDescription<span class="token punctuation">.</span><span class="token function">encode</span><span class="token punctuation">(</span>task<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">// 省略一些判断</span>        <span class="token comment" spellcheck="true">// 发送序列化后的Task</span>        executorData<span class="token punctuation">.</span>executorEndpoint<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token function">LaunchTask</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">SerializableBuffer</span><span class="token punctuation">(</span>serializedTask<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span></code></pre><p>至此，任务已经发往了Executor</p><h2 id="5）Executor"><a href="#5）Executor" class="headerlink" title="5）Executor"></a>5）Executor</h2><p>先找到：CoarseGrainedExecutorBackend.scala</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">CoarseGrainedExecutorBackend</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">extends</span> <span class="token class-name">ThreadSafeRpcEndpoint</span> with ExecutorBackend with Logging <span class="token punctuation">{</span>    override def receive<span class="token operator">:</span> PartialFunction<span class="token punctuation">[</span>Any<span class="token punctuation">,</span> Unit<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token keyword">case</span> <span class="token function">LaunchTask</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span>            val taskDesc <span class="token operator">=</span> TaskDescription<span class="token punctuation">.</span><span class="token function">decode</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>value<span class="token punctuation">)</span>            <span class="token function">logInfo</span><span class="token punctuation">(</span><span class="token string">"Got assigned task "</span> <span class="token operator">+</span> taskDesc<span class="token punctuation">.</span>taskId<span class="token punctuation">)</span>            executor<span class="token punctuation">.</span><span class="token function">launchTask</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">,</span> taskDesc<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这部分代码完成：接收，生成Executor，在Executor上launchTask的任务。代码进入到Executor.scala</p><h3 id="5-1）Executor-scala"><a href="#5-1）Executor-scala" class="headerlink" title="5.1）Executor.scala"></a>5.1）Executor.scala</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">Executor</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">private</span> val runningTasks <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ConcurrentHashMap</span><span class="token punctuation">[</span>Long<span class="token punctuation">,</span> TaskRunner<span class="token punctuation">]</span>      def <span class="token function">launchTask</span><span class="token punctuation">(</span>context<span class="token operator">:</span> ExecutorBackend<span class="token punctuation">,</span> taskDescription<span class="token operator">:</span> TaskDescription<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>        val tr <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">TaskRunner</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> taskDescription<span class="token punctuation">)</span>        runningTasks<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>taskDescription<span class="token punctuation">.</span>taskId<span class="token punctuation">,</span> tr<span class="token punctuation">)</span>        threadPool<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span>tr<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这里就是将我们的Task新起一个TaskRunner，并加入了线程池，所以具体的处理需要进入TaskRunner里。</p><h3 id="5-2）TaskRunner"><a href="#5-2）TaskRunner" class="headerlink" title="5.2）TaskRunner"></a>5.2）TaskRunner</h3><p>这是一个继承了Runnable的线程，所以我们直接找到run方法，其实主要干了三件事：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">TaskRunner</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">Runnable</span> <span class="token punctuation">{</span>    override def <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 反序列化</span>        Executor<span class="token punctuation">.</span>taskDeserializationProps<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>taskDescription<span class="token punctuation">.</span>properties<span class="token punctuation">)</span>        <span class="token function">updateDependencies</span><span class="token punctuation">(</span>taskDescription<span class="token punctuation">.</span>addedFiles<span class="token punctuation">,</span> taskDescription<span class="token punctuation">.</span>addedJars<span class="token punctuation">)</span>        task <span class="token operator">=</span> ser<span class="token punctuation">.</span>deserialize<span class="token punctuation">[</span>Task<span class="token punctuation">[</span>Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">(</span>          taskDescription<span class="token punctuation">.</span>serializedTask<span class="token punctuation">,</span> Thread<span class="token punctuation">.</span>currentThread<span class="token punctuation">.</span>getContextClassLoader<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 运行我们的任务，得到结果</span>        val value <span class="token operator">=</span> Utils<span class="token punctuation">.</span>tryWithSafeFinally <span class="token punctuation">{</span>            task<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>                taskAttemptId <span class="token operator">=</span> taskId<span class="token punctuation">,</span>                attemptNumber <span class="token operator">=</span> taskDescription<span class="token punctuation">.</span>attemptNumber<span class="token punctuation">,</span>                metricsSystem <span class="token operator">=</span> env<span class="token punctuation">.</span>metricsSystem<span class="token punctuation">)</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 包装结果，序列化，并发回。</span>    val valueBytes <span class="token operator">=</span> resultSer<span class="token punctuation">.</span><span class="token function">serialize</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span>    val directResult <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DirectTaskResult</span><span class="token punctuation">(</span>valueBytes<span class="token punctuation">,</span> accumUpdates<span class="token punctuation">)</span>    val serializedDirectResult <span class="token operator">=</span> ser<span class="token punctuation">.</span><span class="token function">serialize</span><span class="token punctuation">(</span>directResult<span class="token punctuation">)</span>    val serializedResult<span class="token operator">:</span> ByteBuffer <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>serializedDirectResult<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    execBackend<span class="token punctuation">.</span><span class="token function">statusUpdate</span><span class="token punctuation">(</span>taskId<span class="token punctuation">,</span> TaskState<span class="token punctuation">.</span>FINISHED<span class="token punctuation">,</span> serializedResult<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>基本上到这里，整个执行流程就说完了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>SparkContext<br> 1）根据提交的master类型，deploy类型等，生成TaskScheduler、DAGScheduler、 SchedulerBackend<br> 2）解析程序的业务逻辑：数据源 → trans → action<br> 3）触发Action，就到了DAGScheduler </li><li>DAGScheduler<br>1）runJob方法，最终目的就是拆分Stage<br>2）拆分方式是：先提交finalStage，如果尚有父Stage未提交，则会触发拆分。根据shuﬀle来 拆分。<br>3）总的来说，会一直递归，直到最父的Stage提交。最终才会提交finalStage 每次提交Stage，最终就会来到taskScheduler.submitTasks(new TaskSet) </li><li>TaskScheduler<br> 1）将接收到的Stage封装成TaskSet。使用submitTasks(taskSet: TaskSet)提交。<br> 2）将每个Task拿出来，通过TaskDescription.encode(task)序列化，用于网络传输。<br> 3）通过executorData.executorEndpoint.send将任务发往Executor </li><li>Executor<br> 1）receive方法接收发过来的Task。<br> 2）最终的处理逻辑在TaskRunner里。 <pre><code> 2.1）反序列化。task = ser.deserialize 执行func， 2.2）返回结果。val value = Utils.tryWithSafeFinally { task.run() } 2.3）将结果序列化。val serializedResult = serializedDirectResult 将结果发回。execBackend.statusUpdate</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark任务提交流程</title>
      <link href="/2018/09/30/spark-ren-wu-ti-jiao-liu-cheng/"/>
      <url>/2018/09/30/spark-ren-wu-ti-jiao-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>通过源码，整理了下Spark-submit任务提交-&gt;执行流程。源码中其实有很多保证可靠性的判断代码，这边会将不那么核心的源码略去，所以贴的代码实际与源码会有些许区别。不过源码中的英文注释会保留重要的。<br>任务执行流程会放在另一篇博客。（马上就国庆节了~，连着写两篇）</p><h1 id="Spark-submit提交"><a href="#Spark-submit提交" class="headerlink" title="Spark-submit提交"></a>Spark-submit提交</h1><h2 id="提交入口"><a href="#提交入口" class="headerlink" title="提交入口"></a>提交入口</h2><p>我们通过spark-submit提交我们的任务，所以打开spark-submit这个脚本，跳到最后，可以看到脚本具体调用的类<code>org.apache.spark.deploy.SparkSubmit</code>，<code>“$@”</code>表示所有传入参数</p><pre class=" language-shell"><code class="language-shell"># disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</code></pre><h2 id="提交作业"><a href="#提交作业" class="headerlink" title="提交作业"></a>提交作业</h2><p>1）前往<code>spark.deploy.SparkSubmit</code>,找到<code>main</code>函数：</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * This entry point is used by the launcher library to start in-process Spark applications. */</span><span class="token keyword">private</span><span class="token punctuation">[</span>spark<span class="token punctuation">]</span> object InProcessSparkSubmit <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val submit <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkSubmit</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    submit<span class="token punctuation">.</span><span class="token function">doSubmit</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>通过注释和源码，我们可以看到这是SparkApp的启动入口，我们的参数会传入submit.doSubmit(args)。</p><h3 id="2）doSubmit"><a href="#2）doSubmit" class="headerlink" title="2）doSubmit"></a>2）doSubmit</h3><pre class=" language-java"><code class="language-java">  def <span class="token function">doSubmit</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 初始化日志的一些操作</span>    val uninitLog <span class="token operator">=</span> <span class="token function">initializeLogIfNecessary</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> silent <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 解析我们传入的参数。</span>    val appArgs <span class="token operator">=</span> <span class="token function">parseArguments</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 判断我们的操作，我们会match上SUBMIT。</span>    appArgs<span class="token punctuation">.</span>action match <span class="token punctuation">{</span>      <span class="token keyword">case</span> SparkSubmitAction<span class="token punctuation">.</span>SUBMIT <span class="token operator">=</span><span class="token operator">></span> <span class="token function">submit</span><span class="token punctuation">(</span>appArgs<span class="token punctuation">,</span> uninitLog<span class="token punctuation">)</span>      <span class="token keyword">case</span> SparkSubmitAction<span class="token punctuation">.</span>KILL <span class="token operator">=</span><span class="token operator">></span> <span class="token function">kill</span><span class="token punctuation">(</span>appArgs<span class="token punctuation">)</span>      <span class="token keyword">case</span> SparkSubmitAction<span class="token punctuation">.</span>REQUEST_STATUS <span class="token operator">=</span><span class="token operator">></span> <span class="token function">requestStatus</span><span class="token punctuation">(</span>appArgs<span class="token punctuation">)</span>      <span class="token keyword">case</span> SparkSubmitAction<span class="token punctuation">.</span>PRINT_VERSION <span class="token operator">=</span><span class="token operator">></span> <span class="token function">printVersion</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span></code></pre><p><code>doSubmit</code>方法主要的工作其实可以概括为：初始化日志服务、解析我们传入的参数。<br>接下来，由于我们是submit操作，理所应到会调用<code>submit(appArgs, uninitLog)</code>方法</p><h3 id="3）submit-appArgs-uninitLog"><a href="#3）submit-appArgs-uninitLog" class="headerlink" title="3）submit(appArgs, uninitLog)"></a>3）submit(appArgs, uninitLog)</h3><pre class=" language-java"><code class="language-java">  <span class="token comment" spellcheck="true">/**   * Submit the application using the provided parameters, ensuring to first wrap   * in a doAs when --proxy-user is specified.   */</span>  <span class="token annotation punctuation">@tailrec</span>  <span class="token keyword">private</span> def <span class="token function">submit</span><span class="token punctuation">(</span>args<span class="token operator">:</span> SparkSubmitArguments<span class="token punctuation">,</span> uninitLog<span class="token operator">:</span> Boolean<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    def <span class="token function">doRunMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// 这边以常用的Yarn/local为例，所以直接进入else。if里的内容暂时省略</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>isStandaloneCluster <span class="token operator">&amp;&amp;</span> args<span class="token punctuation">.</span>useRest<span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token comment" spellcheck="true">// 其实主要也是 doRunMain() 方法。不过会有一些其他判断。</span>      <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 对于其他Mode，直接doRunMain</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>      <span class="token function">doRunMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span></code></pre><p>因为这次以local或yarn模式进行演示，这个地方我们直接就来到了 <code>doRunMain</code>方法。</p><h3 id="4）doRunMain"><a href="#4）doRunMain" class="headerlink" title="4）doRunMain()"></a>4）doRunMain()</h3><p>方法其实也就在这个方法里面。我们把方法具体展开：</p><pre class=" language-java"><code class="language-java">def <span class="token function">doRunMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>proxyUser <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        val proxyUser <span class="token operator">=</span> UserGroupInformation<span class="token punctuation">.</span><span class="token function">createProxyUser</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>proxyUser<span class="token punctuation">,</span>          UserGroupInformation<span class="token punctuation">.</span><span class="token function">getCurrentUser</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>          proxyUser<span class="token punctuation">.</span><span class="token function">doAs</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PrivilegedExceptionAction</span><span class="token punctuation">[</span>Unit<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            override def <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>              <span class="token function">runMain</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> uninitLog<span class="token punctuation">)</span>            <span class="token punctuation">}</span>          <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>       <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token function">runMain</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> uninitLog<span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span></code></pre><p>主要就是判断下<code>args.proxyUser</code>，这个主要是模拟提交应用程序的用户，具体用途参考Spark的参数就好了。</p><h3 id="5）runMain"><a href="#5）runMain" class="headerlink" title="5）runMain()"></a>5）runMain()</h3><p>源码中会有很多判断和log信息，这边删去方便看到核心</p><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span> def <span class="token function">runMain</span><span class="token punctuation">(</span>args<span class="token operator">:</span> SparkSubmitArguments<span class="token punctuation">,</span> uninitLog<span class="token operator">:</span> Boolean<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token function">val</span> <span class="token punctuation">(</span>childArgs<span class="token punctuation">,</span> childClasspath<span class="token punctuation">,</span> sparkConf<span class="token punctuation">,</span> childMainClass<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">prepareSubmitEnvironment</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span>    val loader <span class="token operator">=</span> Thread<span class="token punctuation">.</span>currentThread<span class="token punctuation">.</span>getContextClassLoader<span class="token punctuation">)</span>    Thread<span class="token punctuation">.</span>currentThread<span class="token punctuation">.</span><span class="token function">setContextClassLoader</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span>    <span class="token function">addJarToClasspath</span><span class="token punctuation">(</span>jar<span class="token punctuation">,</span> loader<span class="token punctuation">)</span>    var mainClass<span class="token operator">:</span> Class<span class="token punctuation">[</span>_<span class="token punctuation">]</span> <span class="token operator">=</span> null    <span class="token keyword">try</span> <span class="token punctuation">{</span>      mainClass <span class="token operator">=</span> Utils<span class="token punctuation">.</span><span class="token function">classForName</span><span class="token punctuation">(</span>childMainClass<span class="token punctuation">)</span>    <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    val app<span class="token operator">:</span> SparkApplication <span class="token operator">=</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>SparkApplication<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">isAssignableFrom</span><span class="token punctuation">(</span>mainClass<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      mainClass<span class="token punctuation">.</span><span class="token function">newInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>asInstanceOf<span class="token punctuation">[</span>SparkApplication<span class="token punctuation">]</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>      <span class="token keyword">new</span> <span class="token class-name">JavaMainApplication</span><span class="token punctuation">(</span>mainClass<span class="token punctuation">)</span>    <span class="token punctuation">}</span>    <span class="token keyword">try</span> <span class="token punctuation">{</span>      app<span class="token punctuation">.</span><span class="token function">start</span><span class="token punctuation">(</span>childArgs<span class="token punctuation">.</span>toArray<span class="token punctuation">,</span> sparkConf<span class="token punctuation">)</span>    <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>  <span class="token punctuation">}</span></code></pre><p>这段代码其实将jar包、依赖加入<code>Classpath</code>，生成一个<code>SparkApplication</code>。调用<code>app.start</code>，将参数传入，启动app。由于<code>SparkApplication</code>是一个<code>trait</code>，它的具体实现类有三个：</p><ul><li>ClientApp </li><li>RestSubmissionClientApp </li><li>JavaMainApplication</li></ul><p>那么具体是调用哪个呢？我们先进入<code>prepareSubmitEnvironment(args)</code>，这个方法的内容非常多，我们只找我们需要的，通过观察源码，构建Spark Application的关键参数其实是<code>childMainClass</code>，我们现在找这个就行了，这是一个String类型的变量。</p><h3 id="6）prepareSubmitEnvironment"><a href="#6）prepareSubmitEnvironment" class="headerlink" title="6）prepareSubmitEnvironment"></a>6）prepareSubmitEnvironment</h3><pre class=" language-java"><code class="language-java">var childMainClass <span class="token operator">=</span> <span class="token string">""</span><span class="token keyword">if</span> <span class="token punctuation">(</span>deployMode <span class="token operator">==</span> CLIENT<span class="token punctuation">)</span> <span class="token punctuation">{</span>      childMainClass <span class="token operator">=</span> args<span class="token punctuation">.</span>mainClass    <span class="token punctuation">}</span><span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>isStandaloneCluster<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>useRest<span class="token punctuation">)</span> <span class="token punctuation">{</span>        childMainClass <span class="token operator">=</span> REST_CLUSTER_SUBMIT_CLASS      <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        childMainClass <span class="token operator">=</span> STANDALONE_CLUSTER_SUBMIT_CLASS    <span class="token punctuation">}</span><span class="token keyword">if</span> <span class="token punctuation">(</span>isYarnCluster<span class="token punctuation">)</span> <span class="token punctuation">{</span>      childMainClass <span class="token operator">=</span> YARN_CLUSTER_SUBMIT_CLASS    <span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 其他模式暂时省略，有兴趣可以翻阅具体源码</span><span class="token comment" spellcheck="true">// 上述部分变量的具体值：</span><span class="token keyword">private</span><span class="token punctuation">[</span>deploy<span class="token punctuation">]</span> val YARN_CLUSTER_SUBMIT_CLASS <span class="token operator">=</span>    <span class="token string">"org.apache.spark.deploy.yarn.YarnClusterApplication"</span><span class="token keyword">private</span><span class="token punctuation">[</span>deploy<span class="token punctuation">]</span> val REST_CLUSTER_SUBMIT_CLASS <span class="token operator">=</span> classOf<span class="token punctuation">[</span>RestSubmissionClientApp<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">private</span><span class="token punctuation">[</span>deploy<span class="token punctuation">]</span> val STANDALONE_CLUSTER_SUBMIT_CLASS <span class="token operator">=</span> classOf<span class="token punctuation">[</span>ClientApp<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>这下就很清楚了，我们的SparkApplication是在<code>runMain</code>中的这里创建的。</p><pre class=" language-java"><code class="language-java">    val app<span class="token operator">:</span> SparkApplication <span class="token operator">=</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>SparkApplication<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">isAssignableFrom</span><span class="token punctuation">(</span>mainClass<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      mainClass<span class="token punctuation">.</span><span class="token function">newInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>asInstanceOf<span class="token punctuation">[</span>SparkApplication<span class="token punctuation">]</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>      <span class="token keyword">new</span> <span class="token class-name">JavaMainApplication</span><span class="token punctuation">(</span>mainClass<span class="token punctuation">)</span>    <span class="token punctuation">}</span></code></pre><p><code>StandaloneCluster的useRest</code>会进入<code>RestSubmissionClientApp</code>，其他会进入<code>ClientApp</code>。<br>而yarn并非SparkApplication的子类，则false进入创建<code>JavaMainApplication</code></p><h3 id="7）JavaMainApplication"><a href="#7）JavaMainApplication" class="headerlink" title="7）JavaMainApplication"></a>7）JavaMainApplication</h3><pre class=" language-java"><code class="language-java"><span class="token keyword">private</span><span class="token punctuation">[</span>deploy<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">JavaMainApplication</span><span class="token punctuation">(</span>klass<span class="token operator">:</span> Class<span class="token punctuation">[</span>_<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">SparkApplication</span> <span class="token punctuation">{</span>  override def <span class="token function">start</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">,</span> conf<span class="token operator">:</span> SparkConf<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val mainMethod <span class="token operator">=</span> klass<span class="token punctuation">.</span><span class="token function">getMethod</span><span class="token punctuation">(</span><span class="token string">"main"</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Array</span><span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getClass<span class="token punctuation">)</span>    val sysProps <span class="token operator">=</span> conf<span class="token punctuation">.</span>getAll<span class="token punctuation">.</span>toMap    sysProps<span class="token punctuation">.</span>foreach <span class="token punctuation">{</span> <span class="token keyword">case</span> <span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span>      sys<span class="token punctuation">.</span><span class="token function">props</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token operator">=</span> v    <span class="token punctuation">}</span>    mainMethod<span class="token punctuation">.</span><span class="token function">invoke</span><span class="token punctuation">(</span>null<span class="token punctuation">,</span> args<span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>最终就是调用<code>mainMethod.invoke(null, args)</code><br>后面就是Java部分的内容了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>将主要的方法和参数用幕布整理了下。</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217224352.png" alt="Spark任务提交"></p><p>其实提交流程简单来说可以是：</p><ol><li>解析参数。     </li><li>将该添加的内容添加到ClassPath。     </li><li>判断deploy模式。     </li><li>提交任务。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过BaseRelation自定义SparkSQL数据源</title>
      <link href="/2018/08/05/tong-guo-baserelation-zi-ding-yi-sparksql-shu-ju-yuan/"/>
      <url>/2018/08/05/tong-guo-baserelation-zi-ding-yi-sparksql-shu-ju-yuan/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用<code>org.apache.spark.sql.sources.interfaces</code>中的接口可以实现自定义SparkSql数据源。不过对此在官网和相关书籍中没有找到相关信息，使用网上资料实际使用时其实遇到很多坑，特此记录下。<br>本身是用Spark中的源码JdbcRelationProvider作为参考。这部分解析就不写了。<br>本文章主要记录整体流程和使用过程中遇到的问题。</p><blockquote><p>Spark版本：spark2.1.1</p></blockquote><h1 id="DefaultSource类"><a href="#DefaultSource类" class="headerlink" title="DefaultSource类"></a>DefaultSource类</h1><p>实现自定义数据源，第一步是新建一个名为<code>DefaultSource</code>的类，并需要实现一些Trait，其中<code>RelationProvider</code>是必须实现的。</p><h2 id="实现RelationProvider"><a href="#实现RelationProvider" class="headerlink" title="实现RelationProvider"></a>实现RelationProvider</h2><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">DefaultSource</span> <span class="token keyword">extends</span> <span class="token class-name">RelationProvider</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span></code></pre><blockquote><p>1）DefaultSource必须一字不差。除非我们在META-INFO中注册。<br>2）必须实现RelationProvider接口。<br>3）其他常用接口：SchemaRelationProvider、DataSourceRegister。</p></blockquote><h2 id="实现SchemaRelationProvider（Optional）"><a href="#实现SchemaRelationProvider（Optional）" class="headerlink" title="实现SchemaRelationProvider（Optional）"></a>实现SchemaRelationProvider（Optional）</h2><p>实际使用的时候，为了方便，我们还会实现SchemaRelationProvider。<br>所以常见构造其实是这样：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">DefaultSource</span> <span class="token keyword">extends</span> <span class="token class-name">RelationProvider</span> with SchemaRelationProvider<span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// RelationProvider </span>    override def <span class="token function">createRelation</span><span class="token punctuation">(</span>sqlContext<span class="token operator">:</span>SQLContext<span class="token punctuation">,</span>parameters<span class="token operator">:</span>Map<span class="token punctuation">[</span>String<span class="token punctuation">,</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span>BaseRelation<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// SchemaRelationProvider</span>    override def <span class="token function">createRelation</span><span class="token punctuation">(</span>sqlContext<span class="token operator">:</span>SQLContext<span class="token punctuation">,</span>parameters<span class="token operator">:</span>Map<span class="token punctuation">[</span>String<span class="token punctuation">,</span>String<span class="token punctuation">]</span><span class="token punctuation">,</span>schema<span class="token operator">:</span>StructType<span class="token punctuation">)</span><span class="token operator">:</span>BaseRelation<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>两个Provider会提供同名的方法，不过通过SchemaRelationProvider我们能够轻松的拿到用户想要传入的Schema信息。parameters则是用户在<code>.option</code>传入的一些映射数据。</p><h2 id="实现DataSourceRegister（Optional）"><a href="#实现DataSourceRegister（Optional）" class="headerlink" title="实现DataSourceRegister（Optional）"></a>实现DataSourceRegister（Optional）</h2><p>如果没有实现DataSourceRegister，我们使用时就必须要写完整包名，并且<strong>类名只能为DefaultSource</strong>。该Trait只有一个方法：</p><pre class=" language-java"><code class="language-java">override def <span class="token function">shortName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span>String <span class="token operator">=</span> <span class="token string">"yourName"</span></code></pre><p>系统自带的数据源，比如说jdbc，csv等，都是通过这个方法定义的。不过，实现这个方法还不能被系统识别，我们需要在META-INFO中进行注册，具体位置：<code>spark/sql/core/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code>：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223839.png" alt="包位置"></p><h2 id="附：source的名字在哪检测？"><a href="#附：source的名字在哪检测？" class="headerlink" title="附：source的名字在哪检测？"></a>附：source的名字在哪检测？</h2><p>debug源码，可以很清楚的看到整个逻辑：</p><pre class=" language-java"><code class="language-java">  <span class="token comment" spellcheck="true">/** Given a provider name, look up the data source class definition. */</span>  def <span class="token function">lookupDataSource</span><span class="token punctuation">(</span>provider<span class="token operator">:</span> String<span class="token punctuation">,</span> conf<span class="token operator">:</span> SQLConf<span class="token punctuation">)</span><span class="token operator">:</span> Class<span class="token punctuation">[</span>_<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    val provider1 <span class="token operator">=</span> backwardCompatibilityMap<span class="token punctuation">.</span><span class="token function">getOrElse</span><span class="token punctuation">(</span>provider<span class="token punctuation">,</span> provider<span class="token punctuation">)</span> match <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// DefaultSource的由来</span>    val provider2 <span class="token operator">=</span> s<span class="token string">"$provider1.DefaultSource"</span>    val loader <span class="token operator">=</span> Utils<span class="token punctuation">.</span>getContextOrSparkClassLoader    val serviceLoader <span class="token operator">=</span> ServiceLoader<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>DataSourceRegister<span class="token punctuation">]</span><span class="token punctuation">,</span> loader<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 这边会进行查找，先回找别称，没找到就会去找xxx.DefaultSource</span>    <span class="token keyword">try</span> <span class="token punctuation">{</span>      serviceLoader<span class="token punctuation">.</span>asScala<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">shortName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equalsIgnoreCase</span><span class="token punctuation">(</span>provider1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toList match <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// the provider format did not match any given registered aliases</span>        <span class="token keyword">case</span> Nil <span class="token operator">=</span><span class="token operator">></span>          <span class="token keyword">try</span> <span class="token punctuation">{</span>            <span class="token function">Try</span><span class="token punctuation">(</span>loader<span class="token punctuation">.</span><span class="token function">loadClass</span><span class="token punctuation">(</span>provider1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">orElse</span><span class="token punctuation">(</span><span class="token function">Try</span><span class="token punctuation">(</span>loader<span class="token punctuation">.</span><span class="token function">loadClass</span><span class="token punctuation">(</span>provider2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> match <span class="token punctuation">{</span></code></pre><p>provider就是我们<code>.format(&quot;xxx&quot;)</code>传入的xxx<br>系统首先会先去找别称，没找到再去补充找全称。</p><h1 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h1><p>观察DefaultSource，可以发现其需要返回BaseRelation，这是一个抽象类，所以我们第二步就是实现它。这个类倒是没有什么命名要求。</p><h2 id="继承BaseRelation"><a href="#继承BaseRelation" class="headerlink" title="继承BaseRelation"></a>继承BaseRelation</h2><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">ETLRelation</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">BaseRelation</span><span class="token punctuation">{</span>    override def schema<span class="token operator">:</span>StructType<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span>    override def sqlContext<span class="token operator">:</span>SQLContext<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>继承该类，需要实现两个方法，即拿到schema和sqlContext。这个时候观察DefaultSource，schema和sqlContext其实可以传进来。所以这部分其实常常为这个样子：</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">ETLRelation</span><span class="token punctuation">(</span>userSchema<span class="token operator">:</span>StructType<span class="token punctuation">)</span><span class="token punctuation">(</span>override val sqlContext<span class="token operator">:</span>SQLContext<span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">BaseRelation</span><span class="token punctuation">{</span>    override def schema<span class="token operator">:</span>StructType<span class="token operator">=</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>userSchema<span class="token operator">==</span>null<span class="token punctuation">)</span><span class="token punctuation">{</span>            userSchema        <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">//TODO.. StructType(StructField()::Nil)</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这边会给schema设置一个默认值，当然也可以不设，仿照sqlContext的写法就好了。为了方便使用，我一般会将override的参数与一般参数分开放。构造体中我们可以传入其他需要的参数。<br>Relation的框已经搭起来了，下一步就是实现读取功能了。</p><h2 id="TableScan"><a href="#TableScan" class="headerlink" title="TableScan"></a>TableScan</h2><p>Spark提供了几种Trait供使用：<br>| trait              | 用途                                                         |<br>| :—————– | :———————————————————– |<br>| TableScan          | 将所有列进行读入                                             |<br>| PrunedScan         | 不需要的列不会从外部数据源加载。                             |<br>| PrunedFilteredScan | 在PrunedScan的基础上，并且加入Filter，在加载数据也的时候就进行过滤 |<br>| CatalystScan       | Catalyst的支持传入expressions来进行Scan。支持列裁剪和Filter。 |<br>三个Trait均有一个同名方法：buildScan</p><pre class=" language-java"><code class="language-java">trait <span class="token class-name">TableScan</span> <span class="token punctuation">{</span>  def <span class="token function">buildScan</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span><span class="token punctuation">}</span>trait <span class="token class-name">PrunedScan</span> <span class="token punctuation">{</span>  def <span class="token function">buildScan</span><span class="token punctuation">(</span>requiredColumns<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span><span class="token punctuation">}</span>trait <span class="token class-name">PrunedFilteredScan</span> <span class="token punctuation">{</span>  def <span class="token function">buildScan</span><span class="token punctuation">(</span>requiredColumns<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">,</span> filters<span class="token operator">:</span> Array<span class="token punctuation">[</span>Filter<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span><span class="token punctuation">}</span>trait <span class="token class-name">CatalystScan</span> <span class="token punctuation">{</span>  def <span class="token function">buildScan</span><span class="token punctuation">(</span>requiredColumns<span class="token operator">:</span> Seq<span class="token punctuation">[</span>Attribute<span class="token punctuation">]</span><span class="token punctuation">,</span> filters<span class="token operator">:</span> Seq<span class="token punctuation">[</span>Expression<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span><span class="token punctuation">}</span></code></pre><p>requiredColumns与filters两个参数是在我们操作DataFrame时传入的，如使用filter算子时。以PrunedFilteredScan 为例，实现这个Trait的好处时Spark能够将我们的filter和select实现下压操作。这样我们就无需将所有数据都读进来供后面的运算了。</p><blockquote><p>不实现PrunedFilteredScan我们仍然能够使用filter、select等算子，只是不支持下压操作。</p></blockquote><p>本文就以实现TableScan 为例继续：（假设我们需要读取一个文件）</p><pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">myRelation</span><span class="token punctuation">(</span>schema<span class="token operator">:</span>StructType<span class="token punctuation">,</span>parameters<span class="token operator">:</span>Map<span class="token punctuation">[</span>String<span class="token punctuation">,</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>override val sqlContext<span class="token operator">:</span>SQLContext<span class="token punctuation">)</span>    <span class="token keyword">extends</span> <span class="token class-name">BaseRelation</span> with TableScan with Loggin<span class="token punctuation">{</span>    override def <span class="token function">builScan</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span>RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span>        val triedGetPath <span class="token operator">=</span> <span class="token function">Try</span><span class="token punctuation">(</span>parameters<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"path"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        val path <span class="token operator">=</span> triedGetPath match <span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token function">Failure</span><span class="token punctuation">(</span>exception<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token function">logError</span><span class="token punctuation">(</span><span class="token string">"Path must set"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>sys<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//TODO..</span>            <span class="token keyword">case</span> <span class="token function">Success</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> value<span class="token punctuation">.</span>get        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// rdd是Tuple类型，(文件地址,文件内容)。</span>        val rdd <span class="token operator">=</span> sqlContext<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span><span class="token function">wholeTextFiles</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">// 我们只需要文件内容，先分行，再拆分出单词。</span>        val res <span class="token operator">=</span> rdd<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toSeq<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">// 构成Row类型，并返回。注意要和Schema对应上。</span>        res<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>Row<span class="token punctuation">.</span><span class="token function">fromSeq</span><span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>为节省页面，上面代码只有builScan这个函数。<br>其实我们就是将rdd的处理进行了一层封装。返回SparkSql能够识别的数据。 所以其他Rdd的操作在这里都是允许的，我们需要按照需求选取即可</p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><pre class=" language-java"><code class="language-java">val session <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>val df <span class="token operator">=</span> session<span class="token punctuation">.</span>read<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"com.bigdata.csdn"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">option</span><span class="token punctuation">(</span><span class="token string">"path"</span><span class="token punctuation">,</span><span class="token string">"xxx"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>loaddf<span class="token punctuation">.</span>showsession<span class="token punctuation">.</span><span class="token function">stop</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>format里需要写DefaultSource所在的包名。系统会自动加上DefaultSource。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala中的Option、Try、Either类型简介</title>
      <link href="/2018/07/01/scala-zhong-de-option-try-either-lei-xing-jian-jie/"/>
      <url>/2018/07/01/scala-zhong-de-option-try-either-lei-xing-jian-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="Scala中的异常处理"><a href="#Scala中的异常处理" class="headerlink" title="Scala中的异常处理"></a>Scala中的异常处理</h2><p>Scala提供了许多选择来处理异常的数据。总的来说，三种类型解决三个方面的问题：</p><ul><li><p>Option：Null或空指针问题。</p></li><li><p>Try：运行时函数抛出异常的问题。</p></li><li><p>Either：返回值不确定的问题</p><blockquote><p>Scala版本：2.11.x</p></blockquote><h2 id="Option-Some-None："><a href="#Option-Some-None：" class="headerlink" title="Option/Some/None："></a>Option/Some/None：</h2><p>Option类型其实包含有三类：Option、Some、None。其中Some和None都是继承自Option。</p></li><li><p>Some：返回有效数据。</p></li><li><p>None：返回无效数据（或空值）。</p></li></ul><h3 id="使用Option"><a href="#使用Option" class="headerlink" title="使用Option"></a>使用Option</h3><p>定义一个拥有Option类型的函数。</p><pre class=" language-java"><code class="language-java">def <span class="token function">validateName</span><span class="token punctuation">(</span>name<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Option<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>name<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> None    <span class="token keyword">else</span> <span class="token function">Some</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>当input为空时，会返回None类型。否则返回Some类型。对于Some，通过get就能够拿到value。<br>Option主要用来处理Null值。通过getOrElse方法可以避免一些异常。</p><h2 id="Try-Success-Failure"><a href="#Try-Success-Failure" class="headerlink" title="Try/Success/Failure"></a>Try/Success/Failure</h2><p>在java中，对于可能会出现Exception的情况。我们可以选择try-catch来包裹。在Scala中，Try类型会让代码看上去更加优雅。<br>我们定义一个方法，用Try来对toInt进行包装，避免转换出现异常。</p><pre class=" language-java"><code class="language-java">def <span class="token function">parseInt</span><span class="token punctuation">(</span>value<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Try<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">Try</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>toInt<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//模式匹配</span>val list <span class="token operator">=</span> <span class="token function">List</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token string">"b"</span><span class="token punctuation">,</span><span class="token string">"1"</span><span class="token punctuation">,</span><span class="token string">"2"</span><span class="token punctuation">)</span>    list<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>parseInt<span class="token punctuation">)</span><span class="token punctuation">.</span>foreach <span class="token punctuation">{</span>      <span class="token keyword">case</span> <span class="token function">Failure</span><span class="token punctuation">(</span>exception<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token function">println</span><span class="token punctuation">(</span>exception<span class="token punctuation">)</span>      <span class="token keyword">case</span> <span class="token function">Success</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token function">println</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token comment" spellcheck="true">// isSuccess、isFailure</span>list<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>parseInt<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">foreach</span><span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">{</span>      <span class="token keyword">if</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>isSuccess<span class="token punctuation">)</span> <span class="token function">println</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>get<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>我们可以通过模式匹配来判断Try的结果，并对应处理。也可以通过isSuccess、isFailure来进行判断。<br>使用逻辑上适合try-catch是差不多的。<br>注意：</p><blockquote><p>1）Try可以调用toOption方法转换成Option类型。isFailure会转换成None。<br>2）recover，recoverWith，transform可以让你优雅地处理Success和Failure的结果。</p></blockquote><h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p>利用recover，recoverWith方法，能够帮助我们取处理异常，并从故障中恢复。<br>以recover为例，其接收一个偏函数，返回一个新的Try[U]</p><pre class=" language-java"><code class="language-java">def recover<span class="token punctuation">[</span>U <span class="token operator">></span><span class="token operator">:</span> T<span class="token punctuation">]</span><span class="token punctuation">(</span>f<span class="token operator">:</span> PartialFunction<span class="token punctuation">[</span>Throwable<span class="token punctuation">,</span> U<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Try<span class="token punctuation">[</span>U<span class="token punctuation">]</span></code></pre><p>我们可以将上面的代码改造一下：</p><pre class=" language-java"><code class="language-java">list<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>parseInt<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">foreach</span><span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">{</span>      val triedInt <span class="token operator">=</span> x<span class="token punctuation">.</span>recover <span class="token punctuation">{</span><span class="token keyword">case</span> e<span class="token operator">:</span>NumberFormatException<span class="token operator">=</span><span class="token operator">></span> <span class="token number">0</span> <span class="token punctuation">}</span>      <span class="token function">println</span><span class="token punctuation">(</span>triedInt<span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>这样，我们可以对异常进行匹配，并进行对应的处理，并返回一个新的Try类型</p><h2 id="Either-Left-Right"><a href="#Either-Left-Right" class="headerlink" title="Either/Left/Right"></a>Either/Left/Right</h2><p>有时候，我们需要在函数中返回两种不同类型的值，Either就派上了用场。Left和Right代表了两种不同的类型。<br>如果 Either[A, B] 对象包含的是 A 的实例，那它就是 Left 实例，B则是 Right 实例。</p><pre class=" language-java"><code class="language-java">def <span class="token function">validateName</span><span class="token punctuation">(</span>name<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Either<span class="token punctuation">[</span>Int<span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token keyword">if</span> <span class="token punctuation">(</span>name<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token function">Left</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">else</span> <span class="token function">Right</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><p>后续我们可以通过模式匹配来获取结果。</p><h3 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h3><p>注意：Either是无偏向的，我们在使用Try时，能够直接使用map算子，默认情况下map会只对Success进行处理，并忽略Failure。但Either不一样。<br>对于Either对象，我们能够调用left、right方法拿到LeftProjection 或 RightProjection实例。并在之后进行map操作：</p><pre><code>validateName(&quot;&quot;).left.map(_ + 1).left.get</code></pre><p>这样，我们可以将left的数据进行+1处理，right的数据则不进行处理。需要注意，map返回的时一个Either类型，而不是leftProjection。</p><blockquote><p>例中get的使用其实是有问题的，当left没有数据时，get会报错。如果确实需要get值，建议使用getOrElse。</p></blockquote><h2 id="如何选择："><a href="#如何选择：" class="headerlink" title="如何选择："></a>如何选择：</h2><ol><li>Option[T]：当值不存在或者某些验证可能失败，并且你不关心为什么失败是。</li><li>Try[T]：当无法在函数中处理异常时可以选择使用Try。</li><li>Either[L,R]：当前两种都无法满足你的需求时。</li></ol><hr><p>参考：<a href="https://xebia.com/blog/try-option-or-either/" target="_blank" rel="noopener">Try, Option or Either?</a></p>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shuffle in Spark</title>
      <link href="/2018/06/16/shuffle-in-spark/"/>
      <url>/2018/06/16/shuffle-in-spark/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h1><p>Shuffle简单来说就是将同一特征的数据经过网络/磁盘IO，分别分发聚集到各自的Executor上，因为这个过程涉及着数据的落地，传输。自然是费性能的。<br>在Spark中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。新版本的spark，已经移除了HashShuffleManager。但仍然需要了解。Shuffle整体可调整的内容有限。<br>虽然很多文章总说要尽量避免shuffle，但shuffle很多时候是不可避免的。目前比较好的解决办法是SMB-Join，但这个是从存储角度出发的解决方案，且有很多的局限性。其实SMB-Join的就是在理解Shuffle上实现的。MapJoin其实本质上也是在理解Shuffle上实现的，所以理解Shuffle很重要。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>Shuffle总体可以分为Write和Read两个阶段：</p><ol><li>Write：即MapTask将数据写出。<br> Write数据会优先写入内存中，达到阈值后再溢写到文件内。</li><li>Read：即ReduceTask将数据读入。<br> Reduce会去读MapTask写出的文件，只读自己需要的Key的数据。<br> ReduceTask也会有自己的read buffer缓冲，但理解上没啥难度，通过参数调整大小即可。</li></ol><blockquote><p>本文基于Spark 2.2.x</p></blockquote><h1 id="HashShuffleManager（2-0后已弃用）"><a href="#HashShuffleManager（2-0后已弃用）" class="headerlink" title="HashShuffleManager（2.0后已弃用）"></a>HashShuffleManager（2.0后已弃用）</h1><p>相比于MapReduce中的shuffle，Spark的第一版本Shuffle是不存在排序过程的，Spark认为减少排序这一过程能够提高性能。但HashShuffle最后之所以被弃用，主要还是在shuffle过程中会产生大量磁盘中间文件，导致大量的磁盘IO操作，影响性能。<br>尽管已经弃用，但理解HashShuffle能够帮助去理解新的shuffleManager。<br>HashShuffle整个生命流程有两个阶段，本文简称未经优化V1，和优化后V2版本。</p><ol><li>未经优化：</li></ol><p>Shuffle的MapTask会根据ReduceTask的数量，生成等量的文件，这也是为什么HashShuffle的文件数异常的多。<br>即可能会产生M * R个文件（MapTask数量 * ReduceTask数量）<br>具体逻辑见图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825369515.png" alt="HashShuffle"></p><ol start="2"><li>优化后：</li></ol><p>针对上一个版本的问题，HashShuffle进行了一层优化：<br>    同一个Core的连续任务会将文件写入到一个FileGroup中：仍然是一个reduceTask对应一个文件，不过在一个Core中连续执行的MapTask会共用这些文件，而非单独生成。<br>即会产生 Core * R 个文件。<br>具体逻辑看图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825492345.png" alt="优化后的HashShuffle"></p><h1 id="SortShuffleManager（1-2后默认）"><a href="#SortShuffleManager（1-2后默认）" class="headerlink" title="SortShuffleManager（1.2后默认）"></a>SortShuffleManager（1.2后默认）</h1><p>SortShuffle是参考MapReduce的Shuffle原理设计的，正如名字，会有Sort这一过程。SortShuffle会有3类，不过后两类其实都是在特殊条件下触发的，通过舍弃/修改一些基础步骤，从而得到更好的性能，所以重点还是理解第一类。</p><h2 id="普通"><a href="#普通" class="headerlink" title="普通"></a>普通</h2><p>SortShuffle通过index解决了中间文件过多的问题，也正因为这样所以才需要排序。具体实现逻辑如图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581825966812.png" alt="SortShuffle"></p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581826998269.png" alt="SortShuffle"></p><p>具体步骤：</p><ol><li>写入内存前：</li></ol><p>Spark根据不同Shuffle算子，可能会选用不同的数据结构：</p><table><thead><tr><th>算子</th><th>数据结构</th></tr></thead><tbody><tr><td>Reduce等预聚合</td><td>Map数据结构，一边通过Map进行聚合，一边写入内存</td></tr><tr><td>Shuffle等普通</td><td>Array数据结构，直接写入内存。</td></tr><tr><td>（GroupByKey没有Map预聚合，应该算第二类）</td><td></td></tr></tbody></table><ol start="2"><li>写入磁盘文件前：</li></ol><p>内存数据达到阈值后，Spark会先进行排序，然后通过BufferedOutputStream，通过缓冲Buffer，分批写入到文件中。<br>即这个阶段会发送多次溢写操作，产生多个临时文件。由于有数据的落地，也伴随着这序列化/反序列化。</p><ol start="3"><li>合并文件：</li></ol><p>上一步产生的磁盘文件会进行合并。并生成一个索引文件表示key的offset。</p><ol start="4"><li>索引文件：</li></ol><p>因为所有的Key都放在了一个文件中，所以会为合并的文件生成一个索引文件。<br>标识了下游各个task的数据在文件中的start offset与end offset。（主要包含的就是一个Tuple3（partition， offset, length），其中partition就指定了这个segment数据片段属于哪一个下游的reduceTask，offset和length决定这个segment数据数据内容是哪些。）</p><p>最终会产生2 * M个文件。</p><h2 id="Bypass"><a href="#Bypass" class="headerlink" title="Bypass"></a>Bypass</h2><p>Bypass会在满足触发条件后自动触发，具有更快的Shuffle速度。</p><ol><li>shuffle map task数量小于<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值。</li><li>非Map端聚合类的shuffle算子（比如join）</li></ol><blockquote><ol><li>no map-side combine is specified</li><li>the number of partitions is less than or equal to spark.shuffle.sort.bypassMergeThreshold</li></ol></blockquote><p> 其实Bypass机制可以理解为加速非map聚合shuffle算子的shuffle速度，所以我们主要关注的还是<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值。</p><p>Bypass的逻辑如图所示：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581826653584.png" alt="Bypass"></p><p>很明显，相比于普通的：</p><ol><li>少了一个排序过程。这也是为什么比较快的地方。</li></ol><blockquote><p>Sorting is slower than hashing. It might worth tuning the bypassMergeThreshold parameter for your own cluster to find a sweet spot, but in general for most of the clusters it is even too high with its default</p></blockquote><ol start="2"><li>那么Bypass的Index文件是如何产生的呢？其实数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。</li></ol><h2 id="Unsafe-Shuffle-or-Tungsten-Sort"><a href="#Unsafe-Shuffle-or-Tungsten-Sort" class="headerlink" title="Unsafe Shuffle or Tungsten Sort"></a>Unsafe Shuffle or Tungsten Sort</h2><p>Spark 1.4.0+，Spark启动了TungstenSort，即钨丝计划。具体参见：<a href="https://issues.apache.org/jira/browse/SPARK-7081" target="_blank" rel="noopener">SPARK-7081</a>、<a href="https://issues.apache.org/jira/browse/SPARK-7075" target="_blank" rel="noopener">SPARK-7075</a>。</p><p>Spark 1.6.0+ Tungsten-sort并入Sort Based Shuffle,由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle。</p><p>简单来讲，Tungsten Sort带来了如下优化点：</p><blockquote><ol><li>Operate directly on serialized binary data without the need to deserialize it. It uses unsafe (sun.misc.Unsafe) memory copy functions to directly copy the data itself, which works fine for serialized data as in fact it is just a byte array.</li><li>Uses special cache-efficient sorter ShuffleExternalSorter that sorts arrays of compressed record pointers and partition ids. By using only 8 bytes of space per record in the sorting array, it works more efficienly with CPU cache.</li><li>As the records are not deserialized, spilling of the serialized data is performed directly (no deserialize-compare-serialize-spill logic)</li></ol></blockquote><p>翻译概括以下：</p><blockquote><ol><li>将数据记录用二进制的方式存储，直接在序列化的二进制数据上sort而不是反序列化为java 对象，这样一方面可以减少memory的使用和GC的开销，另一方面避免shuffle过程中频繁的序列化以及反序列化。</li><li>在排序过程中，它提供cache-efficient sorter —— <a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java" target="_blank" rel="noopener">ShuffleExternalSorter</a>，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能。</li></ol></blockquote><p>开启条件：</p><blockquote><ol><li>The shuffle dependency specifies no aggregation or output ordering.</li><li>The shuffle serializer supports relocation of serialized values (this is currently supported<br>by KryoSerializer and Spark SQL’s custom serializers).</li><li>The shuffle produces fewer than 16777216 output partitions.</li><li>No individual record is larger than 128 MB when serialized.</li></ol></blockquote><ol><li>shuffle阶段不能有aggregate操作。</li><li>shuffle的produce输出需要少于16777216 分区。</li><li>序列化时。单条记录小于128 MB。</li><li>能够移动数据的序列化器，如使用Kryo序列化器。</li></ol><h1 id="三种sortShuffle的判定顺序"><a href="#三种sortShuffle的判定顺序" class="headerlink" title="三种sortShuffle的判定顺序"></a>三种sortShuffle的判定顺序</h1><p>观察源码：</p><pre><code>if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) {...      new BypassMergeSortShuffleHandle[K, V](        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])    } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {...      new SerializedShuffleHandle[K, V](        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])    } else {...      new BaseShuffleHandle(shuffleId, numMaps, dependency)    }</code></pre><p>可以看到，目前Spark主要的shuffle有三种方式组成，并且判定的<strong>先后顺序</strong>：BypassMergeSortShuffle，SerializedShuffle(Tungsten)，BaseShuffle(SortShuffle)。在使用非map端聚合算子且无需排序时，尽量能够满足触发前两者的条件，这能够带来一些性能提升。</p><h1 id="常见参数："><a href="#常见参数：" class="headerlink" title="常见参数："></a>常见参数：</h1><ol><li><p>spark.shuffle.file.buffer<br>默认值：32k<br>即MapTask输出内存的大小。增加会占用Execution内存，但能够减少溢写次数。</p></li><li><p>spark.shuffle.sort.bypassMergeThreshold<br>默认值：200<br>Bypass触发的条件，当使用非map聚合算子，且reduce的分区数小于该值时触发。</p></li><li><p>spark.reducer.maxSizeInFlight<br>默认值：48m<br>reduce端的缓冲大小，调大该参数，增加会占用Execution内存，但能够减少拉取次数。</p></li></ol><hr><p>参考文章：</p><ol><li><a href="https://0x0fff.com/spark-architecture-shuffle/" target="_blank" rel="noopener">Spark Architecture: Shuffle</a></li><li><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="noopener">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li><li><a href="https://issues.apache.org/jira/browse/SPARK-7075" target="_blank" rel="noopener">SPARK-7075</a></li><li><a href="https://issues.apache.org/jira/browse/SPARK-7081" target="_blank" rel="noopener">SPARK-7081</a></li><li><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala" target="_blank" rel="noopener">SortShuffleManager</a></li><li><a href="https://www.iteblog.com/archives/1672.html" target="_blank" rel="noopener">Spark性能优化：shuffle调优</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop使用LZO压缩</title>
      <link href="/2018/05/17/hadoop-shi-yong-lzo-ya-suo/"/>
      <url>/2018/05/17/hadoop-shi-yong-lzo-ya-suo/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么要使用LZO压缩。"><a href="#为什么要使用LZO压缩。" class="headerlink" title="为什么要使用LZO压缩。"></a>为什么要使用LZO压缩。</h3><p>Hadoop目前支持的压缩中，用得多的主要有：Snappy，Bzip2，LZO。</p><p>其中：</p><table><thead><tr><th>压缩格式</th><th>特点</th></tr></thead><tbody><tr><td>Bzip2</td><td>压缩率高，可切分。</td></tr><tr><td>LZO</td><td>压缩率中，速度快，可建索引来完成分片。</td></tr><tr><td>Snappy</td><td>压缩率中，速度快。不可切分。</td></tr></tbody></table><h3 id="为啥使用了lzo仍然不能分片"><a href="#为啥使用了lzo仍然不能分片" class="headerlink" title="为啥使用了lzo仍然不能分片"></a>为啥使用了lzo仍然不能分片</h3><p>在hdfs.xml中，有这样的配置</p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.blocksize&lt;/name&gt;    &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;</code></pre><p>这个配置设置了块大小为128M，在MapReduce/Spark的过程中，inputformat执行完毕之后，默认就会根据该配置，对文件进行切块(split)，进而根据块的数量来决定map task的数量。</p><p>除了textFile之外，压缩格式中的lzo，bzip2也可以进行文件的切块操作。</p><p>但是从一般情况，lzo本身是无法进行切块的——如果直接将大于128M的data.lzo文件作为map的输入时，默认blocksize为128M的情况下，number of splits的值仍然为1，即data.lzo仍然被当为一块直接输入map task。</p><p>所以为了实现lzo的切块，需要为lzo的压缩文件生成一个索引文件data.lzo.index。</p><h3 id="如何生成lzo文件"><a href="#如何生成lzo文件" class="headerlink" title="如何生成lzo文件"></a>如何生成lzo文件</h3><p><code>lzop -v data</code>，就会生成data.lzo文件</p><h3 id="给data-lzo配置索引文件"><a href="#给data-lzo配置索引文件" class="headerlink" title="给data.lzo配置索引文件"></a>给data.lzo配置索引文件</h3><p>需要准备hadoop-lzo-0.4.21-SNAPSHOT.jar，如果没有的话就需要编译生成一下。</p><p>1.安装编译所需文件</p><pre><code>yum -y install lzo-devel zlib-devel gcc autoconf automake libtool</code></pre><p>2.下载，解压</p><pre><code>wget https://github.com/twitter/hadoop-lzo/archive/master.zip</code></pre><p>3.修改pom.xml，将其中的hadoop.current.version改为自己的hadoop版本<br>4.编译</p><p>在hadoop-lzo-master/下执行<code>mvn clean package -Dmaven.test.skip=true</code>进行编译，编译好的jar包在hadoop-lzo-master/target/</p><p>5.修改hadoop的配置文件：core-site.xml</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.io.compress.GzipCodec,        org.apache.hadoop.io.compress.DefaultCodec,        org.apache.hadoop.io.compress.BZip2Codec,        org.apache.hadoop.io.compress.SnappyCodec,        com.hadoop.compression.lzo.LzoCodec,        com.hadoop.compression.lzo.LzopCodec    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>mapred-site.xml<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.map.output.compress<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.map.output.compress.codec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></code></pre><p>6.重启hadoop集群，将data.lzo丢到hdfs里。</p><p>7.创建index文件</p><pre><code># 使用mapreduce创建索引hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/data.lzo# 使用本地程序创建索引hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /input/data.lzo</code></pre><p>8.执行自己的Spark程序的时候，输入路径为/input而非/input/data.lzo，这样就能实现lzo的分片操作了。</p><h3 id="什么时候选择LZO？"><a href="#什么时候选择LZO？" class="headerlink" title="什么时候选择LZO？"></a>什么时候选择LZO？</h3><p>这个主要看离线任务的粒度。因为创建索引需要耗时的。数据越大，耗时也越长。</p><p>如果是以天为单位，一般是具有创建索引时间，而创建索引后，Spark就能直接并行读取这批数据。而我们不用去关注文件的大小是否合适</p><p>如果以小时甚至分钟为单位，一般不具备创建索引的时间，这时候就需要控制单个文件的大小，具体选择需要进行测试选择一个能接受的大小，或者也可以让单个文件&lt; Block（我们测试后认为0.7*Block是一个比较合适的值），这样能提供和切片后一样的性能，但元数据也会有明显的负担。这种粒度选择Snappy与LZO区别并不是很大。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理</title>
      <link href="/2018/04/15/spark-nei-cun-guan-li/"/>
      <url>/2018/04/15/spark-nei-cun-guan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark的内存管理"><a href="#Spark的内存管理" class="headerlink" title="Spark的内存管理"></a>Spark的内存管理</h2><p>Spark的内存管理一共分为了：UnifiedMemoryManager和StaticMemoryManager。1.6以后Spark默认会使用前者。本文主要讨论在yarn上运行的情况。大体上是一样的。</p><blockquote><p>Version ：2.2.x</p></blockquote><h2 id="StaticMemoryManager："><a href="#StaticMemoryManager：" class="headerlink" title="StaticMemoryManager："></a>StaticMemoryManager：</h2><p>由于已经不被推荐使用，也不过多介绍：<br>在静态内存管理里，其实也是分了堆外内存和堆内内存，在Yarn的模式下，布局如下图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581761503848.png" alt="Executor布局"></p><ol><li><p>堆外内存：</p><p> 堆外内存主要由参数<code>spark.yarn.executor.memoryOverhead</code>指定，此部分为用户代码及Spark 不可操作的内存，具体了解可以看下官网介绍：</p><blockquote><p>The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</p></blockquote><p> 部分博客把这个参数和统一内存管理里的<code>spark.memory.offheap.size</code>混为一谈，这完全是两个版本里的参数。</p></li><li><p>堆内内存：<br> 这部分是主要需要关注的内存，静态内存主要分为执行内存和存储内存，默认情况下：<br>  2.1 执行内存 = SystemMemory * execution.memory.Fraction(0.2) * executor.safetyFraction(0.8)<br>  2.2 存储内存 = SystemMemory * storage.memory.Fraction(0.2) * storage.safetyFraction(0.8)<br>  2.3 其中SystemMemory可以近似小于executor-memory，对于不同的JVM会略有区别。</p></li></ol><h2 id="UnifiedMemoryManager："><a href="#UnifiedMemoryManager：" class="headerlink" title="UnifiedMemoryManager："></a>UnifiedMemoryManager：</h2><p>1.6后默认使用UnifiedMemoryManager，简单来说就是执行内存和存储内存能够相互借用。对于统一的内存管理，也分为堆内内存和堆外内存。</p><h3 id="堆内内存："><a href="#堆内内存：" class="headerlink" title="堆内内存："></a>堆内内存：</h3><p>堆内内存主要还是分为执行内存和存储内存，另外还有两块空间：UserMemory，Reserved Memory。具体图示：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762131167.png" alt="统一内存管理"></p><table><thead><tr><th>空间</th><th>用途</th></tr></thead><tbody><tr><td>execution</td><td>执行内存，计算、shuffle时使用</td></tr><tr><td>storage</td><td>存储内存，用来Cache数据</td></tr><tr><td>user memory</td><td>存储转换时的依赖信息。</td></tr><tr><td>reserved memory</td><td>用来存储Spark内部对象。</td></tr></tbody></table><p>通过观察UnifiedMemoryManager的源码，我们就能够得到这几块内存区域的计算公式：</p><table><thead><tr><th>区域</th><th>计算来源</th></tr></thead><tbody><tr><td>系统内存（SystemMemory）</td><td>近似小于executor-memory</td></tr><tr><td>预留内存（reservedMemory）</td><td>testing.reservedMemory（默认300M）</td></tr><tr><td>可用最大内存（MaxMemory）</td><td>（SystemMemory-reservedMemory）*memory.fraction(默认0.6)</td></tr><tr><td>用户内存</td><td>（SystemMemory-reservedMemory）*（1-memory.fraction)</td></tr><tr><td>执行内存</td><td>MaxMemory * 0.5</td></tr><tr><td>存储内存</td><td>MaxMemory * 0.5</td></tr></tbody></table><h3 id="堆外内存："><a href="#堆外内存：" class="headerlink" title="堆外内存："></a>堆外内存：</h3><p>堆外内存只分：执行内存和存储内存。功能上与堆内内存互补。两者的比例也是通过参数 <code>spark.memory.storageFraction</code> 控制。</p><p>相关参数：</p><table><thead><tr><th>功能</th><th>参数</th></tr></thead><tbody><tr><td>开启</td><td>spark.memory.offHeap.enabled</td></tr><tr><td>总大小</td><td>spark.memory.offHeap.size（只由这个参数控制，submit是那个参数只控制堆内内存）</td></tr><tr><td>比例</td><td>spark.memory.storageFraction</td></tr></tbody></table><h3 id="借用规则"><a href="#借用规则" class="headerlink" title="借用规则"></a>借用规则</h3><p>统一内存管理的有点就在于能够相互借用，具体规则其实很简单：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762641262.png" alt="借用规则"></p><ol><li>Executor和Storage都能够相互借用对方的内存，但以executor优先。</li><li>Storage需要借用时，只能借用executor释放后的内存。</li><li>Execution需要借用时，则可以主动要求storage释放占用的己方内存。</li><li>这种借用关系存在于堆外和堆内，但堆外不能借堆内，反之亦然。</li></ol><h4 id="从Spark的源码看两者的借用逻辑："><a href="#从Spark的源码看两者的借用逻辑：" class="headerlink" title="从Spark的源码看两者的借用逻辑："></a>从Spark的源码看两者的借用逻辑：</h4><ol><li>acquireStorageMemory</li></ol><p>核心代码：</p><pre><code> if (numBytes &gt; storagePool.memoryFree) {   // There is not enough free memory in the storage pool, so try to borrow free memory from   // the execution pool.   val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree,     numBytes - storagePool.memoryFree)   executionPool.decrementPoolSize(memoryBorrowedFromExecution)   storagePool.incrementPoolSize(memoryBorrowedFromExecution) }</code></pre><p>这里就能够看到，但己方内存不够用时，Storage最终的借用内存。<code>Math.min(executionPool.memoryFree,     numBytes - storagePool.memoryFree)</code>，前者为Execution池中的空闲内存。</p><ol start="2"><li>acquireExecutionMemory</li></ol><p>核心代码：</p><pre><code>def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = {  if (extraMemoryNeeded &gt; 0) {    ...    if (memoryReclaimableFromStorage &gt; 0) {      val spaceToReclaim = storagePool.freeSpaceToShrinkPool(        math.min(extraMemoryNeeded, memoryReclaimableFromStorage))      storagePool.decrementPoolSize(spaceToReclaim)      executionPool.incrementPoolSize(spaceToReclaim)    }  }}...def computeMaxExecutionPoolSize(): Long = {  maxMemory - math.min(storagePool.memoryUsed, storageRegionSize)}...while (true) {      ...      maybeGrowPool(numBytes - memoryFree)      val maxPoolSize = computeMaxPoolSize()      ...      val toGrant = math.min(maxToGrant, memoryFree)      if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) {        ...      } else {        memoryForTask(taskAttemptId) += toGrant        return toGrant      }    }</code></pre><p>这段具体的计算公式我们可以不过多关注，其实官方注释一句话就把这段逻辑解释清楚了，通过：</p><blockquote><p>The size the execution pool would have after evicting storage memory.</p></blockquote><p>上述代码在计算好待释放内存后，通过<code>storagePool.freeSpaceToShrinkPool</code> -&gt; <code>memoryStore.evictBlocksToFreeSpace</code>来完成内存释放。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本地spark+idea访问云主机上的hive</title>
      <link href="/2018/03/31/ben-di-spark-idea-fang-wen-yun-zhu-ji-shang-de-hive/"/>
      <url>/2018/03/31/ben-di-spark-idea-fang-wen-yun-zhu-ji-shang-de-hive/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录下本人使用本地spark+idea访问云主机遇到过的一些小问题。<br>仅供参考。欢迎讨论。</p><blockquote><p>spark版本：2.1.1 </p></blockquote><h2 id="使用sparkSQL-idea访问云主机HIVE"><a href="#使用sparkSQL-idea访问云主机HIVE" class="headerlink" title="使用sparkSQL+idea访问云主机HIVE"></a>使用sparkSQL+idea访问云主机HIVE</h2><h3 id="准备hive-site-xml"><a href="#准备hive-site-xml" class="headerlink" title="准备hive-site.xml"></a>准备hive-site.xml</h3><p>官网是这么描述的：</p><blockquote><p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.</p></blockquote><p>实际上，一般只需要准备hive-site.xml这个文件即可，因为spark访问Hive主要是通过访问hive的原数据库来完成的：<br><img src="https://img-blog.csdnimg.cn/20191014173213417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Ntb2tlcml1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以最重要的就是让spark程序能够访问到原数据库。本例是使用mysql。<br>贴一下hive-site.xml供参考。</p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span><span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!--hadoop换成云主机ip就好了，不过mysql需要注意开启远程访问权限 --></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://hadoop:3306/hive?createDatabaseIfNotExist=true<span class="token entity" title="&amp;">&amp;amp;</span>useSSL=false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!--客户端显示当前查询表的头信息 --></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.header<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!--客户端显示当前数据库名称信息 --></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.current.db<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></code></pre><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><p>主要是三个依赖：sparksql、mysql、spark-hive。<br>请根据需要进行更改。</p><pre class=" language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-sql_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${spark.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>mysql<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>mysql-connector-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>5.1.47<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-hive_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.1.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><h3 id="获取sparkSession"><a href="#获取sparkSession" class="headerlink" title="获取sparkSession"></a>获取sparkSession</h3><p>新版的spark，通过sparkSession来连接hive，不过有几个参数需要注意，先贴代码：</p><pre class=" language-java"><code class="language-java">val spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">master</span><span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">appName</span><span class="token punctuation">(</span><span class="token string">"hiveApp"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">enableHiveSupport</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">config</span><span class="token punctuation">(</span><span class="token string">"dfs.client.use.datanode.hostname"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">getOrCreate</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>需要<code>enableHiveSupport</code>来对hive进行支持。<br>访问云主机，需要<code>.config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</code>，因为hadoop返回对ip是内网地址，通过这个设置让hadoop返回主机名，这样我们配置下hosts就能够访问了。</p><h3 id="读取hive表"><a href="#读取hive表" class="headerlink" title="读取hive表"></a>读取hive表</h3><p>拿到正确的sparksession，就能对hive进行各种操作了。<br>这里贴一下完整代码方便查阅。</p><pre class=" language-java"><code class="language-java">object hiveApp <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">master</span><span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">appName</span><span class="token punctuation">(</span><span class="token string">"hiveApp"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">enableHiveSupport</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">config</span><span class="token punctuation">(</span><span class="token string">"dfs.client.use.datanode.hostname"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">getOrCreate</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span><span class="token function">setLogLevel</span><span class="token punctuation">(</span><span class="token string">"WARN"</span><span class="token punctuation">)</span>    val df <span class="token operator">=</span> spark<span class="token punctuation">.</span><span class="token function">table</span><span class="token punctuation">(</span><span class="token string">"spark.person"</span><span class="token punctuation">)</span>    df<span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux中awk与sed指令的使用笔记</title>
      <link href="/2018/02/17/linux-zhong-awk-yu-sed-zhi-ling-de-shi-yong-bi-ji/"/>
      <url>/2018/02/17/linux-zhong-awk-yu-sed-zhi-ling-de-shi-yong-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>最近工作要使用到这对命令进行脚本编写，使用感觉特别容易忘，故记录整理下。</p><h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>简单来说，awk是把文本逐行读入，默认以空格为列分隔符进行切片。对切好对数据再进行处理。<br>本文会再学习实验的同时，简单介绍一些常用的awk命令。</p><h3 id="使用方法："><a href="#使用方法：" class="headerlink" title="使用方法："></a>使用方法：</h3><pre class=" language-shell"><code class="language-shell">awk '{pattern + action}' {filename}</code></pre><p>pattern表示awk在数据中查找对内容。<br>action表示找到后执行什么命令。<br>{}用来对一系列指令进行分组。</p><h3 id="调用途径："><a href="#调用途径：" class="headerlink" title="调用途径："></a>调用途径：</h3><pre class=" language-shell"><code class="language-shell">#1)命令行awk [-F field-separator] 'commands' input-files#2)shell脚本#awk命令解释器作为脚本首行，相当于将#!/bin/bash => #!/bin/awk</code></pre><h3 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h3><p>首先，准备一个文本格式对文件（a1）。以空格分开。<br>再准备一个文件b1，以【,】作为分隔符<br>比如说a1对内容：第三行会多处一列。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217214320.png" alt="awk"></p><h4 id="1）区分行与列，打印数据"><a href="#1）区分行与列，打印数据" class="headerlink" title="1）区分行与列，打印数据"></a>1）区分行与列，打印数据</h4><p>1）打印a1所有数据：</p><pre class=" language-shell"><code class="language-shell">awk '{print}' a1#输出：first line 3 4second second 3 4hadoop 3 4 50</code></pre><p>2）打印a1第一列，第二列数据：</p><pre class=" language-shell"><code class="language-shell">awk '{print $1,$2}' a1#输出：first linesecond secondhadoop 3#注意,如果不加[,]awk '{print $1$2}' a1#则输出firstlinesecondsecondhadoop3#注意,如果不加[$]awk '{print $1 2}' a1#则输出first2second2hadoop2</code></pre><p>3）打印第1列第1+行数据：使用内置的NR变量。（文章结尾会总结下常用的内置变量）</p><pre class=" language-shell"><code class="language-shell">awk 'NR>1{print $1}' a1#结果：secondhadoop</code></pre><p>4）如果需要打印第2+列，就不那么友好了，不过也可以使用循环来完成：<br>打印第2+列的数据</p><pre class=" language-shell"><code class="language-shell">awk '{for(i=1;i<=2;i++)$i="";print}' a1#结果：  3 4  3 4  4 50</code></pre><h4 id="2）分隔符-正则查找"><a href="#2）分隔符-正则查找" class="headerlink" title="2）分隔符/正则查找"></a>2）分隔符/正则查找</h4><p>awk默认是识别空格为列分隔符，不过比如csv使用逗号来进行风格的，显然需要指定分隔符。这里会使用：-F fs。-F就是指定输入文件分隔符，fs可以是一个字符串，也可以是一个正则表达式</p><p>b1内容如下：</p><pre><code>1,2,3,4,5,6a,b,c,d,e,f,ga,s,d,f,g,h,j,ka,a,a,a,a,a,a,ab,b,b,b,b,b,b,b</code></pre><p>1）读取以逗号为分隔符的b1文件，打印第2，3行，第1，2列：</p><pre class=" language-shell"><code class="language-shell">awk -F ',' 'NR>1 && NR<4{print $1,$2}' b1#结果：a ba s</code></pre><p>2）新建c1：文本内容：</p><pre class=" language-shell"><code class="language-shell">a,1,okb,2,noc,3,ioki</code></pre><p>找到ok，并打印第1列：</p><pre class=" language-shell"><code class="language-shell">awk -F ',' '/ok/{print $1}' c1#结果：ac</code></pre><h4 id="awk常用内置变量"><a href="#awk常用内置变量" class="headerlink" title="awk常用内置变量"></a>awk常用内置变量</h4><table><thead><tr><th align="left">变量</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">ARGC</td><td align="left">命令行参数个数</td></tr><tr><td align="left">ARGV</td><td align="left">命令行参数排列</td></tr><tr><td align="left">ENVIRON</td><td align="left">支持队列中系统环境变量的使用</td></tr><tr><td align="left">FILENAME</td><td align="left">awk浏览的文件名</td></tr><tr><td align="left">FNR</td><td align="left">浏览文件的记录数</td></tr><tr><td align="left">FS</td><td align="left">设置输入域分隔符，等价于命令行 -F选项</td></tr><tr><td align="left">NF</td><td align="left">浏览记录的域的个数</td></tr><tr><td align="left">NR</td><td align="left">已读的记录数</td></tr><tr><td align="left">OFS</td><td align="left">输出域分隔符</td></tr><tr><td align="left">ORS</td><td align="left">输出记录分隔符</td></tr><tr><td align="left">RS</td><td align="left">控制记录分隔符</td></tr><tr><td align="left">1）使用：读取文件a1，统计文件名，行号，列数</td><td align="left"></td></tr><tr><td align="left">```shell</td><td align="left"></td></tr><tr><td align="left">awk ‘{print “name:”FILENAME”,line:”NR”,cols:”NF}’ a1</td><td align="left"></td></tr><tr><td align="left">#结果</td><td align="left"></td></tr><tr><td align="left">name:a1,line:1,cols:4</td><td align="left"></td></tr><tr><td align="left">name:a1,line:2,cols:4</td><td align="left"></td></tr><tr><td align="left">name:a1,line:3,cols:4</td><td align="left"></td></tr><tr><td align="left">```</td><td align="left"></td></tr><tr><td align="left">#### 结合其他命令行</td><td align="left"></td></tr><tr><td align="left">awk可以结合其他命令行返回第结果进行处理。通过</td><td align="left">风格。awk第用法和上面将第是一样的。</td></tr><tr><td align="left">1）利用awk统计最近5个登陆的用户的用户名和ip或主机名。（last后面不加指令)</td><td align="left"></td></tr><tr><td align="left">```shell</td><td align="left"></td></tr><tr><td align="left">last</td><td align="left">awk ‘NR&lt;5{print $1,$3}’</td></tr><tr><td align="left">#结果</td><td align="left"></td></tr><tr><td align="left">hadoop 192.168.199.203</td><td align="left"></td></tr><tr><td align="left">hadoop 192.168.199.203</td><td align="left"></td></tr><tr><td align="left">hadoop 192.168.199.203</td><td align="left"></td></tr><tr><td align="left">hadoop 192.168.199.203</td><td align="left"></td></tr><tr><td align="left">hadoop 192.168.199.203</td><td align="left"></td></tr><tr><td align="left">```</td><td align="left"></td></tr></tbody></table><h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed是一种在线编辑器，每次处理一行内容。它将处理的行放入临时缓冲区，然后使用sed命令处理缓冲内容。默认会将处理好的内容发往屏幕，可以通过重定向，将内容输出至文件。<br>sed主要用来自动编辑1～多个文件。<br>特别的，sed是一行一行处理的。</p><h3 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h3><pre class=" language-shell"><code class="language-shell">sed [nefr] [action] [file]</code></pre><p>部分：<br>| nefr | 含义                   |<br>| —- | ———————- |<br>| -f   | 使用file里到sed命令    |<br>| -i   | 直接将结果修改到file里 |</p><p>action：[n1[,n2]] func<br>| func | 含义                   |<br>| —- | ———————- |<br>| a    | 新增行（在该行后插入） |<br>| c    | 修改行                 |<br>| d    | 删除                   |<br>| i    | 插入行（在该行前插入） |<br>| s    | 取代（替换）           |</p><h4 id="简单使用：替换"><a href="#简单使用：替换" class="headerlink" title="简单使用：替换"></a>简单使用：替换</h4><p>1）将hello,hello,world的第一个hello替换为gwkki</p><pre class=" language-shell"><code class="language-shell">#语法：'s/pattern/replacement/flag'echo "hello,hello,world" | sed 's/hello/gwkki/'#结果gwkki,hello,world#特别的，这个替换会作用到每行第一个匹配到的。</code></pre><p>2）所有hello替换为gwkki</p><pre class=" language-shell"><code class="language-shell">echo "hello,hello,world" | sed 's/hello/gwkki/g'#结果gwkki,gwkki,world</code></pre><p>3）替换第二个hello</p><pre class=" language-shell"><code class="language-shell">echo "hello,hello,world" | sed 's/hello/gwkki/2'#结果hello,gwkki,world</code></pre><p>说下flag：<br>数字表示替换第几处匹配到的。<br>g表示全部替换<br>p表示匹配的行会被额外打印。<br>w表示保存到指定文件（读取文件可以用 sed -i command来修改并保存）<br>4)修改c1中的xxx为yyy并保存：</p><pre class=" language-shell"><code class="language-shell">sed -i s/xxx/yyy/ a1</code></pre><h4 id="操作某些行："><a href="#操作某些行：" class="headerlink" title="操作某些行："></a>操作某些行：</h4><pre class=" language-shell"><code class="language-shell">#操作第二行：sed '2s/xxx/yyy/' a1#操作2-4行：包含2，4sed '2,4s/xxx/yyy/' a1#操作3+行：包含3sed ‘3，$s/xxx/yyy/’ a1</code></pre><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><pre class=" language-shell"><code class="language-shell">#删除匹配到k到行：sed '/k/d' c1#删除第3-5行：sed '3,5d' c1</code></pre><h4 id="插入新增"><a href="#插入新增" class="headerlink" title="插入新增"></a>插入新增</h4><pre class=" language-shell"><code class="language-shell">#在"line 1"的下一行新增line2echo "line 1" | sed 'a line 2'#在"line 1"的上一行新增line2echo "line 1" | sed 'i line 2'</code></pre><h4 id="替换特殊字符-如：’"><a href="#替换特殊字符-如：’" class="headerlink" title="替换特殊字符 如：’"></a>替换特殊字符 如：’</h4><p>使用””扩起来即可：“可以用&quot;匹配到</p><pre class=" language-shell"><code class="language-shell">#将'替换成{}sed "s/\'/{}/" a1</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop+Yarn的HA架构解析</title>
      <link href="/2018/01/12/hadoop-yarn-de-ha-jia-gou-jie-xi/"/>
      <url>/2018/01/12/hadoop-yarn-de-ha-jia-gou-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问。依赖其的组件也无法正确运转。重启NameNode是耗时的。导致其之前只适用于离线处理场景。</p><h2 id="HDFS的HA"><a href="#HDFS的HA" class="headerlink" title="HDFS的HA"></a>HDFS的HA</h2><p>hadoop在2.0版本中，解决了HDFS NameNode 和 YARN ResourceManger的单点问题。<br>整个架构可以参考两张图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222852.png" alt="HDFS的HA"></p><p>从图中可以看到，整个HA框架可以分为几个部分：</p><ol><li>Active NameNode 和 Standby NameNode：</li></ol><pre><code>两台 NameNode ，一主一备。一个皇上，一个皇上他儿子。备Name会不断从JournalNode中读取editLog，保持元数据信息与主NameNode一致，并实时与DataNode相互通信，使皇上挂了后，儿子能够正常无缝上位。 值得注意的是，只有主NameNode才能对外提供读写服务。（3.0支持多个standBy NameNodes了）</code></pre><ol start="2"><li>ZKFailoverController(zkfc)：主备切换控制器。</li></ol><pre><code>作为一个独立进程，对 NameNode 的主备切换进行总体控制。zkfc会不断检测到 NameNode 的健康状况，定期向zk集群发送心跳信息。自己被zk选举为active的时候，zkfc进程通过RPC协议调用使NN节点的状态变为active，对外提供实时服务（顺带一提，yarn中，也有类似的zkfc，不过是作为线程存在的。）</code></pre><ol start="3"><li>Zookeeper集群：</li></ol><pre><code>为zkfc的选举切换提供支持。 （条件允许，可将zookepper配置在独立的机器上，防止部署了zk的机器业务过于繁忙，从而导致nameNode无法及时切换。） （由于nameNode是通过zk投票选举，zk部署太多也会影响选举效率）</code></pre><ol start="4"><li><p>DataNode 节点：</p><pre><code>DataNode 会同时向主备 NameNode 发送心跳信息、数据的块信息 。</code></pre></li><li><p>共享存储系统：（本文以JournalNodes为例）</p><pre><code>是实现高可用至关重要的一步。JournalNodes会保存NameNode 在运行过程中所产生的HDFS元数据，主从NameNode 通过JN实现元数据的同步。具体来说：  Active的NameNode的命名空间有任何修改时，会告知大部分的大部分的JournalNodes进程并将EditLog提交到JournalNodes中， 备NameNode会定期从JN中读取修改记录，并在自己这边进行重演，从而保证主从NameNode 的元数据信息相同 另外，对于HA集群，保证只有一个NameNode是Active也是非常重要的，否则两个NameNode的数据状态就会产生分歧，所以，JournalNodes必须确保同一时刻只有一个NameNode可以向自己写数据。</code></pre></li></ol><p><strong>关于JournalNode</strong></p><pre><code>运行的JournalNode进程非常轻量，可以部署在其他的服务器上。注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个。（在active namenode写数据到journalnode上时，必须有半数以上的journalnode写成功的话，才标志写成功。因此需要奇数。）</code></pre><p>参考图：</p><h3 id="主备切换流程概述"><a href="#主备切换流程概述" class="headerlink" title="主备切换流程概述"></a>主备切换流程概述</h3><p>主备切换由：ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现。</p><ol><li>zkfc：<br>作为NameNode上的一个独立进程存在。<br>启动时会创建HealthMonitor和ActiveStandbyElector两个组件。并注册回调方法。</li><li>HealthMonitor：<br>负责检测NameNode的健康情况。</li><li>ActiveStandbyElector：<br>负责完成自动的主备选举。<br>参考图：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222931.png" alt="主备切换流程概述"><h3 id="NameNode的元数据存储概述"><a href="#NameNode的元数据存储概述" class="headerlink" title="NameNode的元数据存储概述"></a>NameNode的元数据存储概述</h3>NameNode 在执行 HDFS 客户端提交写操作（如：创建文件、移动文件等）的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。<br>内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog <strong>仅仅只是在数据恢复的时候起作用</strong>。EditLog 会被切割为很多段，每一段称为一个 Segment。<br>正在写入的EditLog Segment，其文件名形如 edits_inprogress_{start_txid}。<br>写入完成的EditLog Segment，其文件名形如 edits_{start_txid}-{end_txid}。</li></ol><p>NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，<br>NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222638.png" alt=""></p><h3 id="实现主备切换的额外内容"><a href="#实现主备切换的额外内容" class="headerlink" title="实现主备切换的额外内容"></a>实现主备切换的额外内容</h3><p>主备NameNode实际是两个不同的机器。所以对外需要提供一个通用的命名空间服务，才能够在主Name故障时，无感知的完成切换。<br>对于用户或者代码来说，不需要关心主备切换的事情。<br>需要说明的是，nameSpace并<strong>不是一个独立进程</strong>。而仅仅知识core-site/hdfs-site之中的一些配置实现的。<br>通过命名空间的配置，通过</p><pre><code>hdfs://nameNodeService/</code></pre><p>就能够直接访问，而不用去管自己访问的具体是那个NameNode。</p><h2 id="Yarn的HA"><a href="#Yarn的HA" class="headerlink" title="Yarn的HA"></a>Yarn的HA</h2><p>Yarn整体思路和Hdfs是差不多的，并且有相当部分的代码重用。不过也有一些区别，顺带记录下。<br>总体而言，数据是不能丢的,但是作业是可以挂的,挂了重启即可。因此YARN的架构比较轻量级。<br>本例中基于ZKRMStateStore。</p><p>HA整体框架图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223126.png" alt="Yarn的HA"></p><p>从图中可以看到，整个HA框架可以分为几个部分：</p><ol><li>主备ResourceManager（RM）：</li></ol><pre><code>启动时，向zk的目录（如：/hadoop-ha）写入一个lock文件。成功的成为Active节点。失败则是standBy。 StandBy的会去监控这个lock文件，如果不存在，则会尝试创建，以成为Active节点。RM会在Container上启动并监控ApplocationMaster（AM）。</code></pre><p>关于RM的本身用途（放一起方便回忆）： </p><pre><code>ResourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统。NodeManager以心跳的方式向ResourceManager汇报资源使用情况（目前主要是CPU和内存的使用情况）。RM只接受NM的资源回报信息，对于具体的资源处理则交给NM自己处理。</code></pre><ol start="2"><li>NodeManager（NM）：<pre><code>a）运行在所有节点上。b）启动Container容器，用于运行task运算。 c）Yarn的HA中，NM只会与Active的RM进行通信。 d）监控容器并上报资源：将Container的情况报告给RM，将Task的处理情况汇报给AM。</code></pre></li></ol><p>关于Container（放一起方便回忆）：</p><pre><code>Container（容器）是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。 Container 是 一种资源抽象，它封装了某个节点上的多维度资源。从而限定每个任务使用的资源量。一个节点会运行多个Container，但一个Container不会跨节点。 任何一个 job 或 application 必须运行在一个或多个 Container 中。</code></pre><ol start="3"><li><p>AppLocationMaster（AM）：</p><pre><code>a）AM运行在Container上，Container运行在NodeManager之上（The ApplicationMaster is started on a container） b）负责单个Applocation（Job）的task的资源管理和调度。计算Job的资源需求。  c）与调度器协商，并向RM进行资源申请。 d）向NM发出Launch Container请求。并接受NM的task处理状态信息。</code></pre></li><li><p>RMStateStore（本例中指：ZKRMStateStore）</p></li></ol><pre><code>负责数据交换。  active的RM会向zk的目录（如：/rmstore）写入Job信息。active的RM挂掉后，另一个standBy成功转换为Active后，会从该目录读取Job信息。重新构建作业的内存信息，随后启动内部服务。成为Active后，才会开始接受NM的心跳，接受Client的作业请求。</code></pre><ol start="6"><li>ZKFC：</li></ol><pre><code>作为RM中的一个线程存在。 负责自动故障转移</code></pre><h2 id="HDFS与YARN的HA的主要区别"><a href="#HDFS与YARN的HA的主要区别" class="headerlink" title="HDFS与YARN的HA的主要区别"></a>HDFS与YARN的HA的主要区别</h2><table><thead><tr><th>HDFS</th><th>YARN</th></tr></thead><tbody><tr><td>standBy会不断从JN上读取修改数据，并进行复现。保证切换迅速</td><td>在Active节点挂掉后，才会从zk中读取Job信息。恢复不如HDFS那么及时，故更轻量。</td></tr><tr><td>HDFS HA使用JN集群同步数据</td><td>使用存储在Zookeeper中的RMstatestore同步数</td></tr><tr><td>ZKFC为进程</td><td>ZKFC为线程</td></tr><tr><td>Active和Standby都要接受来自NN的心跳等信息</td><td>只有Active才会接受NM的心跳</td></tr></tbody></table><p>参考：</p><ol><li><a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></li><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yarn资源调优</title>
      <link href="/2017/12/01/yarn-zi-yuan-diao-you/"/>
      <url>/2017/12/01/yarn-zi-yuan-diao-you/</url>
      
        <content type="html"><![CDATA[<h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><p>Yarn的资源设置主要是6个参数，目的其实就是资源最大化。同时限制下面任务如某个Spark任务过度调用资源。</p><h2 id="CPU资源调度"><a href="#CPU资源调度" class="headerlink" title="CPU资源调度"></a>CPU资源调度</h2><p>目前的CPU被划分为虚拟CPU（CPU virtual Core）考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，可以通过多配置几个虚拟CPU弥补差异。CPU的相关配置如下：</p><ol><li><p>yarn.nodemanager.resource.cpu-vcores<br>表示该节点服务器上yarn可以使用的虚拟CPU个数，默认是8。</p><p>推荐将值配置与物理核心个数相同。如果CPU的性能好可以调大。</p></li><li><p>yarn.scheduler.minimum-allocation-vcores<br>单个任务最小可申请的虚拟核心数，默认为1。</p></li><li><p>yarn.scheduler.maximum-allocation-vcores<br>单个任务最大可申请的虚拟核心数，默认为4，如果申请资源时，超过这个配置，会抛出InvalidResourceRequestException</p></li></ol><h2 id="Memory资源调度"><a href="#Memory资源调度" class="headerlink" title="Memory资源调度"></a>Memory资源调度</h2><p>yarn一般允许用户配置每个节点上可用的物理资源，集群上除了跑作业，还要留出资源给Hive、HDFS等其他服务。</p><ol><li><p>yarn.nodemanager.resource.memory-mb<br>设置该节点上yarn可使用的内存，默认为8G。</p></li><li><p>yarn.scheduler.minimum-allocation-mb<br>单个任务最小申请物理内存量，默认1024MB。</p></li><li><p>yarn.scheduler.maximum-allocation-mb<br>单个任务最大申请物理内存量，默认为8291MB</p></li></ol><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>如果有一个服务器16核，64G内存，我们应该如何配置上面的6个参数呢（一句话：资源最大化利用）</p><ol><li><p>yarn.nodemanager.resource.cpu-vcores </p><p>虚拟CPU核数<br>根据具体的服务器水平去设置。即虚拟CPU的核数。我们生产设置的是32。</p></li><li><p>yarn.nodemanager.resource.memory-mb </p><p>总内存<br>生产上建议预留15%~20%的内存，所以我们设置的50G</p></li><li><p>yarn.scheduler.minimum-allocation-mb </p><p>单任务最小内存<br>如果设置成2G，就是最多可以跑25个container<br>如果设置成3G，就是最多可以跑16个container</p></li><li><p>yarn.scheduler.minimum-allocation-mb </p><p>单任务最少虚拟Core<br>如果设置vcore = 1，就是最多可以跑32个container，但如果设置成这个，如果上面分配的是2G内存。最多只能跑25个container，就有点浪费了。<br>如果设置vcore = 2，就是最多可以跑16个container，内存就能用3G，比较均匀化</p></li><li><p>yarn.scheduler.maximum-allocation-vcores</p><p>单任务最多虚拟Core<br>一般就设置成4个，cloudera公司做过性能测试。</p></li><li><p>yarn.scheduler.maximum-allocation-mb </p><p>单任务最大内存<br>如果有大任务，需要5-6G内存，那就设置为8G</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typesafe的ConfigFactory的简单使用</title>
      <link href="/2017/11/26/typesafe-de-configfactory-de-jian-dan-shi-yong/"/>
      <url>/2017/11/26/typesafe-de-configfactory-de-jian-dan-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="ConfigFactory"><a href="#ConfigFactory" class="headerlink" title="ConfigFactory"></a>ConfigFactory</h2><p>ConfigFactory能够帮助我们快速的读取配置文件，帮助我们修改一些配置信息，而不用修改代码。<br>例如我们可以将sql的连接信息放入配置文件中，而不是写死在代码里。<br>类似于Java的properties，不过ConfigFactory支持多种文件格式，包括：</p><blockquote><p>The convenience method ConfigFactory.load() loads the following (first-listed are higher priority):<br>a) system properties<br>b) system properties application.conf (all resources on classpath with this name)<br>c) application.json (all resources on classpath with this name)<br>d) application.properties (all resources on classpath with this name)<br>e) reference.conf (all resources on classpath with this name)</p></blockquote><p>其他的优点，用途，用法大家可以参见<a href="https://github.com/lightbend/config" target="_blank" rel="noopener">github项目</a>。<br>里面的<code>README.md</code>进行了详细的说明。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="引入依赖："><a href="#引入依赖：" class="headerlink" title="引入依赖："></a>引入依赖：</h3><p>需要注意：1.2.1及以前的版本是基于Java6开发的，而1.3及之后的版本都需要基于1.8。</p><blockquote><p>Version 1.2.1 and earlier were built for Java 6, while newer versions(1.3.0 and above) will be built for Java 8.</p></blockquote><p>以maven为例：</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.typesafe<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>config<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.3.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><h3 id="获取配置文件"><a href="#获取配置文件" class="headerlink" title="获取配置文件"></a>获取配置文件</h3><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217220011.png" alt="获取配置文件"><br>使用<code>load</code>方法可以加载到实现准备好的配置文件，默认加载<code>classpath</code>下的<code>application.conf</code>，<code>application.json</code>和<code>application.properties</code>文件。当然也可以调用<code>ConfigFactory.load(confName)</code>加载指定的配置文件。<br>获取到之后通过getxxx方法，就能够提取里面的值了。本文主要介绍下简单使用，更复杂的使用请参考:<a href="https://github.com/lightbend/config" target="_blank" rel="noopener">github项目</a>。</p><h3 id="常见配置文件的书写规则："><a href="#常见配置文件的书写规则：" class="headerlink" title="常见配置文件的书写规则："></a>常见配置文件的书写规则：</h3><p>上面提到过，ConfigFactory支持多种配置文件格式，常见的有<code>application.conf</code>，<code>application.json</code>和<code>application.properties</code>文件。</p><h4 id="application-conf"><a href="#application-conf" class="headerlink" title="application.conf"></a>application.conf</h4><p>一般使用k=v的形式即可：</p><pre class=" language-java"><code class="language-java">db<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>url<span class="token operator">=</span><span class="token string">"jdbc:mysql://localhost:3306"</span>db<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>name<span class="token operator">=</span><span class="token string">"dbname"</span>db<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>passwd<span class="token operator">=</span><span class="token number">123456</span><span class="token comment" spellcheck="true">//使用</span>conf<span class="token operator">=</span>ConfigFactory<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>confName<span class="token punctuation">)</span>conf<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.url"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>conf<span class="token punctuation">.</span><span class="token function">getInt</span><span class="token punctuation">(</span><span class="token string">"db.default.passwd"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>也可以使用这种配置形式：</p><pre class=" language-java"><code class="language-java">mysql<span class="token operator">-</span>app <span class="token punctuation">{</span>    url<span class="token operator">=</span><span class="token string">"jdbc:mysql://localhost:3306"</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">//使用</span>conf<span class="token operator">=</span>ConfigFactory<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>confName<span class="token punctuation">)</span>conf<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"mysql-app.url"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="application-json"><a href="#application-json" class="headerlink" title="application.json"></a>application.json</h4><pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"db"</span><span class="token operator">:</span><span class="token punctuation">{</span>    <span class="token property">"url"</span><span class="token operator">:</span><span class="token string">"jdbc:mysql://localhost:3306"</span>    <span class="token property">"dbname"</span><span class="token operator">:</span><span class="token string">"dbname"</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span>//使用conf.getString(<span class="token string">"db.url"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>conf.getString(<span class="token string">"db.dbname"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="application-properties"><a href="#application-properties" class="headerlink" title="application.properties"></a>application.properties</h4><pre class=" language-java"><code class="language-java">db<span class="token punctuation">.</span>url<span class="token operator">=</span>jdbc<span class="token operator">:</span>mysql<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>localhost<span class="token operator">:</span><span class="token number">3306</span><span class="token comment" spellcheck="true">//使用同上。</span></code></pre><h3 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h3><pre class=" language-java"><code class="language-java">object mysqlApp <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//.appName(this.getClass.getSimpleName).master("local[*]")</span>      <span class="token punctuation">.</span><span class="token function">getOrCreate</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>size <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">{</span>      <span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"args error////inputPath///confPath"</span><span class="token punctuation">)</span>      System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    val <span class="token function">Array</span><span class="token punctuation">(</span>intput<span class="token punctuation">,</span>conf<span class="token punctuation">)</span> <span class="token operator">=</span> args<span class="token comment" spellcheck="true">/** * 文件内容： * a,1,A * b,1,B * c,1,C */</span>        val readDF <span class="token operator">=</span> spark<span class="token punctuation">.</span>read<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"text"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>intput<span class="token punctuation">)</span>    <span class="token keyword">import</span> spark<span class="token punctuation">.</span>implicits<span class="token punctuation">.</span>_    val df <span class="token operator">=</span> readDF<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>      val splits <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>      <span class="token punctuation">(</span><span class="token function">splits</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">splits</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">splits</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toDF</span><span class="token punctuation">(</span><span class="token string">"col1"</span><span class="token punctuation">,</span> <span class="token string">"col2"</span><span class="token punctuation">,</span> <span class="token string">"col3"</span><span class="token punctuation">)</span>    df<span class="token punctuation">.</span><span class="token function">printSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    df<span class="token punctuation">.</span>write<span class="token punctuation">.</span><span class="token function">options</span><span class="token punctuation">(</span><span class="token function">getDBConf</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">mode</span><span class="token punctuation">(</span><span class="token string">"overwrite"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"jdbc"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">save</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    def <span class="token function">getDBConf</span><span class="token punctuation">(</span>confPlace<span class="token operator">:</span>String<span class="token operator">=</span><span class="token string">"application.conf"</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token punctuation">{</span>      val config <span class="token operator">=</span> ConfigFactory<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>confPlace<span class="token punctuation">)</span>      val url <span class="token operator">=</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.url"</span><span class="token punctuation">)</span>      val configMap <span class="token operator">=</span> HashMap<span class="token punctuation">[</span>String<span class="token punctuation">,</span> String<span class="token punctuation">]</span><span class="token punctuation">(</span>        <span class="token punctuation">(</span><span class="token string">"url"</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.url"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"db"</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.name"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"dbtable"</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.table"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.user"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"password"</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.password"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">(</span><span class="token string">"driver"</span><span class="token punctuation">,</span>config<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"db.default.driver"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">)</span>      configMap    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Utils </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Utils </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建Harbor</title>
      <link href="/2017/09/16/da-jian-harbor/"/>
      <url>/2017/09/16/da-jian-harbor/</url>
      
        <content type="html"><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>请先安装Docker。本文主要针对离线部署。</p><h3 id="1）Https："><a href="#1）Https：" class="headerlink" title="1）Https："></a>1）Https：</h3><p>本步骤可参考：<a href="https://github.com/goharbor/harbor/blob/master/docs/configure_https.md" target="_blank" rel="noopener">configure_https</a></p><ol><li><p>安装证书：（Getting Certificate Authority）</p><pre><code>openssl genrsa -out ca.key 4096主要修改后三项：openssl req -x509 -new -nodes -sha512 -days 3650 \-subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com&quot; \-key ca.key \-out ca.crt</code></pre></li><li><p>获取服务器证书：（Getting Server Certificate）</p><ol><li><p>生成私钥：</p><pre><code>openssl genrsa -out yourdomain.com.key  4096</code></pre></li><li><p>生成签名请求：</p><pre><code>openssl req -sha512 -new \-subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com&quot; \-key yourdomain.com.key \-out yourdomain.com.csr</code></pre></li><li><p>生成注册表主机证书：（v3.ext）（DNS按需写）</p><pre><code>cat &gt; v3.ext &lt;&lt;-EOFauthorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentextendedKeyUsage = serverAuthsubjectAltName = @alt_names[alt_names]DNS.1=yourdomain.comDNS.2=yourdomainDNS.3=hostnameEOF</code></pre><pre><code>openssl x509 -req -sha512 -days 3650 \-extfile v3.ext \-CA ca.crt -CAkey ca.key -CAcreateserial \-in yourdomain.com.csr \-out yourdomain.com.crt</code></pre></li></ol></li><li><p>配置安装证书：（Configuration and Installation）</p><ol><li><p>为Harbor配置证书（文件夹不存在则创建）</p><pre><code>cp yourdomain.com.crt /data/cert/cp yourdomain.com.key /data/cert/</code></pre></li><li><p>为Docker配置：（文件夹不存在则创建）</p><pre><code>openssl x509 -inform PEM -in yourdomain.com.crt -out yourdomain.com.cert---cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/cp ca.crt /etc/docker/certs.d/yourdomain.com/</code></pre></li><li><p>配置damon.jason：（使用ip登陆必须）</p><ol><li>位置：/etc/docker/daemon.json</li></ol><pre><code>{&quot;registry-mirrors&quot;:[&quot;https://hadoop001&quot;],&quot;insercure-registries&quot;:[&quot;ip&quot;]}</code></pre></li></ol><p>​    </p></li></ol><h3 id="2）配置harbor-yml"><a href="#2）配置harbor-yml" class="headerlink" title="2）配置harbor.yml"></a>2）配置harbor.yml</h3><p>主要配置：</p><ul><li>hostname：hostname或ip</li><li>certificate：/data/cert/<strong>yourdomain.com</strong>.crt</li><li>private_key：/data/cert/<strong>yourdomain.com</strong>.key</li></ul><h3 id="3）-安装Docker-compose"><a href="#3）-安装Docker-compose" class="headerlink" title="3） 安装Docker compose"></a>3） 安装Docker compose</h3><p>参考：<a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener">安装Docker compose</a></p><h3 id="4）-安装Harbor"><a href="#4）-安装Harbor" class="headerlink" title="4） 安装Harbor"></a>4） 安装Harbor</h3><pre><code>sudo ./install.sh</code></pre><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>对Harbor修改配置时，需要重启Docker及Harbor，建议如下操作：</p><ul><li><p><strong>docker-compose需要与harbor.yml同一目录下执行</strong>：</p><table><thead><tr><th>命令</th><th>用途</th></tr></thead><tbody><tr><td>sudo docker-compose down -v</td><td>停止所有容器，并删除容器。并删除挂载卷和volunme的链接（-v）</td></tr><tr><td>vim harbor.yml</td><td>编辑配置</td></tr><tr><td>sudo ./prepare</td><td>执行一个准备脚本。</td></tr><tr><td>sudo docker-compose up -d</td><td>在后台（-d）构建，（重新）创建，启动，链接一个服务相关的容器。</td></tr></tbody></table></li></ul><h3 id="生命周期管理"><a href="#生命周期管理" class="headerlink" title="生命周期管理"></a>生命周期管理</h3><ol><li><p>停止Harbor（会保留数据）：</p><pre><code>sudo docker-compose down -v</code></pre></li><li><p>删除数据：</p><pre><code>rm -r /data/databaserm -r /data/registry</code></pre></li><li><p>后台启动Harbor：</p><pre><code>sudo docker-compose up -d</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka-server-stop显示No kafka server to stop的解决办法</title>
      <link href="/2017/07/07/kafka-server-stop-xian-shi-no-kafka-server-to-stop-de-jie-jue-ban-fa/"/>
      <url>/2017/07/07/kafka-server-stop-xian-shi-no-kafka-server-to-stop-de-jie-jue-ban-fa/</url>
      
        <content type="html"><![CDATA[<p>我们想通过<code>kafka-server-stop.sh</code>友好的关闭kafka时，一般会出现：<code>No kafka server to stop</code>。<br>如图：</p><p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231613.png" alt="示例"></p><p>其实原因很简单，脚本没有找到kafka的PID。<br>解决方案：<br>修改<code>kafka-server-stop.sh</code><br>原来sh脚本应该是这样：（注意第二行 <code>&#39;kafka\.Kafka&#39;</code>）</p><pre class=" language-shell"><code class="language-shell">SIGNAL=${SIGNAL:-TERM}#注意这行！PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')if [ -z "$PIDS" ]; then  echo "No kafka server to stop"  exit 1else  kill -s $SIGNAL $PIDSfi</code></pre><p>第二行改为：（根据自己kafka版本修改）</p><pre class=" language-shell"><code class="language-shell">SIGNAL=${SIGNAL:-TERM}#修改这行PIDS=$(ps ax | grep -i 'kafka_2.11-2.2.1' | grep java | grep -v grep | awk '{print $1}')if [ -z "$PIDS" ]; then  echo "No kafka server to stop"  exit 1else  kill -s $SIGNAL $PIDSfi</code></pre><p>不确定可以把这段命令拷贝到控制台试一下，它会返回kafka进程的pid号。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217231628.png" alt="示例2"><br>简单解释下：<br>2.11是编译kafka的scala版本。<br>2.2.1是kafka版本（是apache的版本，如果是CDK，请查看它底层用的apache的什么版本。）</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用隐式转换为Spark-RDD添加简单的算子</title>
      <link href="/2017/06/14/shi-yong-yin-shi-zhuan-huan-wei-spark-rdd-tian-jia-jian-dan-de-suan-zi/"/>
      <url>/2017/06/14/shi-yong-yin-shi-zhuan-huan-wei-spark-rdd-tian-jia-jian-dan-de-suan-zi/</url>
      
        <content type="html"><![CDATA[<h2 id="干啥用的"><a href="#干啥用的" class="headerlink" title="干啥用的"></a>干啥用的</h2><p>RDD已经提供了非常多的算子供使用，不过人有时候就是想偷懒，或者有些新想法。<br>比如说我想快速的输出rdd的前x个元素（就像DataFrame中的show一样）<br>利用scala的隐式转换，我们可以对RDD进行一些简单的扩展，将一些常用的操作进行封装，或者增加一些新的算子。<br>初学者文章，有错误望批评指正</p><h2 id="说下隐式转换"><a href="#说下隐式转换" class="headerlink" title="说下隐式转换"></a>说下隐式转换</h2><p>实现的基础是利用Scala的隐式转换。如果不了解可以访问官网简单了解下：<a href="https://docs.scala-lang.org/zh-cn/tour/implicit-conversions.html" target="_blank" rel="noopener">隐式转换Doc</a>，网上也有大量的文章帮助了解。<br>本次主要使用隐式类，隐式类的一些注意事项（限制）在此一起记录下：</p><ol><li>构造参数有且只能有一个（参数实际上就是你要把 什么类 转换成该（隐式）类）</li><li>不能是顶层类（必须被定义在类，伴生对象和包对象里）</li><li>不能是case class（case class在定义会自动生成伴生对象与2矛盾）</li><li>作用域内不能有与之相同名称的标示符（就是不能同名）</li></ol><h2 id="实践才是检验真理"><a href="#实践才是检验真理" class="headerlink" title="实践才是检验真理"></a>实践才是检验真理</h2><p>需求：需求很简单，我想打印一个RDD的元素，并且可以自定义展示元素的个数。不用考虑太多其他的优化问题。<br>分析：利用隐式转换，我们需要将：Rdd[Int]转换成自定义的：SimpleRdd[Int]。使Rdd[Int]能够调用SimpleRdd的方法。<br>代码：</p><pre><code>object RddImplicitAspect {  implicit class SimpleRdd[T](rdd:RDD[T]){    def printTopN(printLine:Int = 20) = {      rdd.take(printLine).foreach(println)      println(s&quot;${rdd.getClass.getSimpleName} Output Over&quot;)    }  }}</code></pre><ol><li>首先建立一个工具类，取名<code>RddImplicitAspect</code>。</li><li>弄一个<code>implicit class</code>，类名取为<code>SimpleRdd</code>（<strong>不要重名</strong>），接受的对象是<strong>RDD[T]</strong>。这样T类型的Rdd都能够使用这个方法吧。</li><li>实际上我们应该限制Rdd的类型，比如说<code>[T&lt;:AnyVal]</code>把作用范围缩小。</li><li>之后我们就可以对rdd进行操作了。</li></ol><p>使用：</p><pre><code>object TestApp {  def main(args: Array[String]): Unit = {    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)    val rdd= sc.parallelize(1 to 30)    import com.test.spark.utils.RddImplicitAspect._    rdd.printTopN()    sc.stop()  }}</code></pre><ol><li><strong>不要忘记导入隐式转换。</strong></li><li>直接调用即可。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop fs -ls 相关使用问题的记录</title>
      <link href="/2017/02/14/hadoop-fs-ls-xiang-guan-shi-yong-wen-ti-de-ji-lu/"/>
      <url>/2017/02/14/hadoop-fs-ls-xiang-guan-shi-yong-wen-ti-de-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop-fs-ls-相关使用问题记录"><a href="#hadoop-fs-ls-相关使用问题记录" class="headerlink" title="hadoop fs -ls 相关使用问题记录"></a>hadoop fs -ls 相关使用问题记录</h1><p>新人新学，如有错误，烦请提出指正。</p><h2 id="1）直接使用：hadoop-fs-ls"><a href="#1）直接使用：hadoop-fs-ls" class="headerlink" title="1）直接使用：hadoop fs -ls"></a>1）直接使用：hadoop fs -ls</h2><p>正确的使用方式是：</p><pre><code>hadoop fs [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</code></pre><p>如果将path省去，系统会将：/user/hostname/ 添加。<br>所以实际这条指令等价于：</p><pre><code>hadoop fs -ls /user/hostname/</code></pre><p>实验截图参考：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog1.png" alt=""></p><h2 id="2）使用：hadoop-fs-ls"><a href="#2）使用：hadoop-fs-ls" class="headerlink" title="2）使用：hadoop fs -ls /"></a>2）使用：hadoop fs -ls /</h2><p>实际会默认访问hdfs路径。即实际上，这条指令等价于：</p><pre><code>hadoop fs -ls hdfs:///</code></pre><h2 id="3）使用：hadoop-fs-ls-hdfs-hostname-port"><a href="#3）使用：hadoop-fs-ls-hdfs-hostname-port" class="headerlink" title="3）使用：hadoop fs -ls hdfs://hostname:port/"></a>3）使用：hadoop fs -ls hdfs://hostname:port/</h2><p>访问hdfs的完整路径。即：访问hdfs文件系统上的hostname(ip亦可):port/<br>实际上，可以使用这条指令访问本地的文件系统：</p><pre><code>hdfs dfs -ls file:///home/hadoop/</code></pre><p>参考：<br>[1] <a href="https://stackoverflow.com/questions/28241251/hadoop-fs-ls-results-in-no-such-file-or-directory" target="_blank" rel="noopener">hadoop fs -ls results in “no such file or directory”</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yarn工作流程(Spark cluster)</title>
      <link href="/2017/02/10/yarn-gong-zuo-liu-cheng/"/>
      <url>/2017/02/10/yarn-gong-zuo-liu-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217141526.png" alt="Spark on Yarn-cluster"></p><ol><li>客户端向yarn提交作业，yarn会启动ApplicationsManager。</li><li>RM为作业分配NM，启动ApplicationMaster（AM）</li><li>AM向ApplicationManager注册，并向ResourceScheduler申请资源。</li><li>ResourceScheduler将打包好的资源信息返回给AM，AM通知对应的NodeManager启动Container。</li><li>Container启动成功后，会保持与AM的通信，直到任务完成。</li><li>所有任务完成后，AM向ApplicationManager申请注销资源。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
