<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用隐式转换为Spark-RDD添加简单的算子</title>
      <link href="/2017/06/14/shi-yong-yin-shi-zhuan-huan-wei-spark-rdd-tian-jia-jian-dan-de-suan-zi/"/>
      <url>/2017/06/14/shi-yong-yin-shi-zhuan-huan-wei-spark-rdd-tian-jia-jian-dan-de-suan-zi/</url>
      
        <content type="html"><![CDATA[<h2 id="干啥用的"><a href="#干啥用的" class="headerlink" title="干啥用的"></a>干啥用的</h2><p>RDD已经提供了非常多的算子供使用，不过人有时候就是想偷懒，或者有些新想法。<br>比如说我想快速的输出rdd的前x个元素（就像DataFrame中的show一样）<br>利用scala的隐式转换，我们可以对RDD进行一些简单的扩展，将一些常用的操作进行封装，或者增加一些新的算子。<br>初学者文章，有错误望批评指正</p><h2 id="说下隐式转换"><a href="#说下隐式转换" class="headerlink" title="说下隐式转换"></a>说下隐式转换</h2><p>实现的基础是利用Scala的隐式转换。如果不了解可以访问官网简单了解下：<a href="https://docs.scala-lang.org/zh-cn/tour/implicit-conversions.html" target="_blank" rel="noopener">隐式转换Doc</a>，网上也有大量的文章帮助了解。<br>本次主要使用隐式类，隐式类的一些注意事项（限制）在此一起记录下：</p><ol><li>构造参数有且只能有一个（参数实际上就是你要把 什么类 转换成该（隐式）类）</li><li>不能是顶层类（必须被定义在类，伴生对象和包对象里）</li><li>不能是case class（case class在定义会自动生成伴生对象与2矛盾）</li><li>作用域内不能有与之相同名称的标示符（就是不能同名）</li></ol><h2 id="实践才是检验真理"><a href="#实践才是检验真理" class="headerlink" title="实践才是检验真理"></a>实践才是检验真理</h2><p>需求：需求很简单，我想打印一个RDD的元素，并且可以自定义展示元素的个数。不用考虑太多其他的优化问题。<br>分析：利用隐式转换，我们需要将：Rdd[Int]转换成自定义的：SimpleRdd[Int]。使Rdd[Int]能够调用SimpleRdd的方法。<br>代码：</p><pre><code>object RddImplicitAspect {  implicit class SimpleRdd[T](rdd:RDD[T]){    def printTopN(printLine:Int = 20) = {      rdd.take(printLine).foreach(println)      println(s&quot;${rdd.getClass.getSimpleName} Output Over&quot;)    }  }}</code></pre><ol><li>首先建立一个工具类，取名<code>RddImplicitAspect</code>。</li><li>弄一个<code>implicit class</code>，类名取为<code>SimpleRdd</code>（<strong>不要重名</strong>），接受的对象是<strong>RDD[T]</strong>。这样T类型的Rdd都能够使用这个方法吧。</li><li>实际上我们应该限制Rdd的类型，比如说<code>[T&lt;:AnyVal]</code>把作用范围缩小。</li><li>之后我们就可以对rdd进行操作了。</li></ol><p>使用：</p><pre><code>object TestApp {  def main(args: Array[String]): Unit = {    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)    val rdd= sc.parallelize(1 to 30)    import com.test.spark.utils.RddImplicitAspect._    rdd.printTopN()    sc.stop()  }}</code></pre><ol><li><strong>不要忘记导入隐式转换。</strong></li><li>直接调用即可。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop fs -ls 相关使用问题的记录</title>
      <link href="/2017/02/14/hadoop-fs-ls-xiang-guan-shi-yong-wen-ti-de-ji-lu/"/>
      <url>/2017/02/14/hadoop-fs-ls-xiang-guan-shi-yong-wen-ti-de-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop-fs-ls-相关使用问题记录"><a href="#hadoop-fs-ls-相关使用问题记录" class="headerlink" title="hadoop fs -ls 相关使用问题记录"></a>hadoop fs -ls 相关使用问题记录</h1><p>新人新学，如有错误，烦请提出指正。</p><h2 id="1）直接使用：hadoop-fs-ls"><a href="#1）直接使用：hadoop-fs-ls" class="headerlink" title="1）直接使用：hadoop fs -ls"></a>1）直接使用：hadoop fs -ls</h2><p>正确的使用方式是：</p><pre><code>hadoop fs [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</code></pre><p>如果将path省去，系统会将：/user/hostname/ 添加。<br>所以实际这条指令等价于：</p><pre><code>hadoop fs -ls /user/hostname/</code></pre><p>实验截图参考：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog1.png" alt=""></p><h2 id="2）使用：hadoop-fs-ls"><a href="#2）使用：hadoop-fs-ls" class="headerlink" title="2）使用：hadoop fs -ls /"></a>2）使用：hadoop fs -ls /</h2><p>实际会默认访问hdfs路径。即实际上，这条指令等价于：</p><pre><code>hadoop fs -ls hdfs:///</code></pre><h2 id="3）使用：hadoop-fs-ls-hdfs-hostname-port"><a href="#3）使用：hadoop-fs-ls-hdfs-hostname-port" class="headerlink" title="3）使用：hadoop fs -ls hdfs://hostname:port/"></a>3）使用：hadoop fs -ls hdfs://hostname:port/</h2><p>访问hdfs的完整路径。即：访问hdfs文件系统上的hostname(ip亦可):port/<br>实际上，可以使用这条指令访问本地的文件系统：</p><pre><code>hdfs dfs -ls file:///home/hadoop/</code></pre><p>参考：<br>[1] <a href="https://stackoverflow.com/questions/28241251/hadoop-fs-ls-results-in-no-such-file-or-directory" target="_blank" rel="noopener">hadoop fs -ls results in “no such file or directory”</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
