<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.ibuer.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Ssiu Blog">
<meta property="og:url" content="http://blog.ibuer.fun/page/10/index.html">
<meta property="og:site_name" content="Ssiu Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ssiu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://blog.ibuer.fun/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Ssiu Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ssiu Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2018/05/17/hadoop-shi-yong-lzo-ya-suo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/17/hadoop-shi-yong-lzo-ya-suo/" class="post-title-link" itemprop="url">Hadoop使用LZO压缩</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-05-17 12:00:30" itemprop="dateCreated datePublished" datetime="2018-05-17T12:00:30+08:00">2018-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-17 19:34:31" itemprop="dateModified" datetime="2020-02-17T19:34:31+08:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="为什么要使用LZO压缩。"><a href="#为什么要使用LZO压缩。" class="headerlink" title="为什么要使用LZO压缩。"></a>为什么要使用LZO压缩。</h3><p>Hadoop目前支持的压缩中，用得多的主要有：Snappy，Bzip2，LZO。</p>
<p>其中：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>Bzip2</td>
<td>压缩率高，可切分。</td>
</tr>
<tr>
<td>LZO</td>
<td>压缩率中，速度快，可建索引来完成分片。</td>
</tr>
<tr>
<td>Snappy</td>
<td>压缩率中，速度快。不可切分。</td>
</tr>
</tbody></table>
<h3 id="为啥使用了lzo仍然不能分片"><a href="#为啥使用了lzo仍然不能分片" class="headerlink" title="为啥使用了lzo仍然不能分片"></a>为啥使用了lzo仍然不能分片</h3><p>在hdfs.xml中，有这样的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.blocksize&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;134217728&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>这个配置设置了块大小为128M，在MapReduce/Spark的过程中，inputformat执行完毕之后，默认就会根据该配置，对文件进行切块(split)，进而根据块的数量来决定map task的数量。</p>
<p>除了textFile之外，压缩格式中的lzo，bzip2也可以进行文件的切块操作。</p>
<p>但是从一般情况，lzo本身是无法进行切块的——如果直接将大于128M的data.lzo文件作为map的输入时，默认blocksize为128M的情况下，number of splits的值仍然为1，即data.lzo仍然被当为一块直接输入map task。</p>
<p>所以为了实现lzo的切块，需要为lzo的压缩文件生成一个索引文件data.lzo.index。</p>
<h3 id="如何生成lzo文件"><a href="#如何生成lzo文件" class="headerlink" title="如何生成lzo文件"></a>如何生成lzo文件</h3><p><code>lzop -v data</code>，就会生成data.lzo文件</p>
<h3 id="给data-lzo配置索引文件"><a href="#给data-lzo配置索引文件" class="headerlink" title="给data.lzo配置索引文件"></a>给data.lzo配置索引文件</h3><p>需要准备hadoop-lzo-0.4.21-SNAPSHOT.jar，如果没有的话就需要编译生成一下。</p>
<p>1.安装编译所需文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install lzo-devel zlib-devel gcc autoconf automake libtool</span><br></pre></td></tr></table></figure>

<p>2.下载，解压</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure>

<p>3.修改pom.xml，将其中的hadoop.current.version改为自己的hadoop版本<br>4.编译</p>
<p>在hadoop-lzo-master/下执行<code>mvn clean package -Dmaven.test.skip=true</code>进行编译，编译好的jar包在hadoop-lzo-master/target/</p>
<p>5.修改hadoop的配置文件：core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6.重启hadoop集群，将data.lzo丢到hdfs里。</p>
<p>7.创建index文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 使用mapreduce创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/data.lzo</span><br><span class="line"></span><br><span class="line"># 使用本地程序创建索引</span><br><span class="line">hadoop jar /home/hadoop/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /input/data.lzo</span><br></pre></td></tr></table></figure>

<p>8.执行自己的Spark程序的时候，输入路径为/input而非/input/data.lzo，这样就能实现lzo的分片操作了。</p>
<h3 id="什么时候选择LZO？"><a href="#什么时候选择LZO？" class="headerlink" title="什么时候选择LZO？"></a>什么时候选择LZO？</h3><p>这个主要看离线任务的粒度。因为创建索引需要耗时的。数据越大，耗时也越长。</p>
<p>如果是以天为单位，一般是具有创建索引时间，而创建索引后，Spark就能直接并行读取这批数据。而我们不用去关注文件的大小是否合适</p>
<p>如果以小时甚至分钟为单位，一般不具备创建索引的时间，这时候就需要控制单个文件的大小，具体选择需要进行测试选择一个能接受的大小，或者也可以让单个文件&lt; Block（我们测试后认为0.7*Block是一个比较合适的值），这样能提供和切片后一样的性能，但元数据也会有明显的负担。这种粒度选择Snappy与LZO区别并不是很大。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2018/04/15/spark-nei-cun-guan-li/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/15/spark-nei-cun-guan-li/" class="post-title-link" itemprop="url">Spark内存管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-04-15 17:57:46" itemprop="dateCreated datePublished" datetime="2018-04-15T17:57:46+08:00">2018-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-17 23:21:27" itemprop="dateModified" datetime="2020-02-17T23:21:27+08:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Spark的内存管理"><a href="#Spark的内存管理" class="headerlink" title="Spark的内存管理"></a>Spark的内存管理</h2><p>Spark的内存管理一共分为了：UnifiedMemoryManager和StaticMemoryManager。1.6以后Spark默认会使用前者。本文主要讨论在yarn上运行的情况。大体上是一样的。</p>
<blockquote>
<p>Version ：2.2.x</p>
</blockquote>
<h2 id="StaticMemoryManager："><a href="#StaticMemoryManager：" class="headerlink" title="StaticMemoryManager："></a>StaticMemoryManager：</h2><p>由于已经不被推荐使用，也不过多介绍：<br>在静态内存管理里，其实也是分了堆外内存和堆内内存，在Yarn的模式下，布局如下图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581761503848.png" alt="Executor布局"></p>
<ol>
<li><p>堆外内存：</p>
<p> 堆外内存主要由参数<code>spark.yarn.executor.memoryOverhead</code>指定，此部分为用户代码及Spark 不可操作的内存，具体了解可以看下官网介绍：</p>
<blockquote>
<p>The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</p>
</blockquote>
<p> 部分博客把这个参数和统一内存管理里的<code>spark.memory.offheap.size</code>混为一谈，这完全是两个版本里的参数。</p>
</li>
<li><p>堆内内存：<br> 这部分是主要需要关注的内存，静态内存主要分为执行内存和存储内存，默认情况下：<br>  2.1 执行内存 = SystemMemory * execution.memory.Fraction(0.2) * executor.safetyFraction(0.8)<br>  2.2 存储内存 = SystemMemory * storage.memory.Fraction(0.2) * storage.safetyFraction(0.8)<br>  2.3 其中SystemMemory可以近似小于executor-memory，对于不同的JVM会略有区别。</p>
</li>
</ol>
<h2 id="UnifiedMemoryManager："><a href="#UnifiedMemoryManager：" class="headerlink" title="UnifiedMemoryManager："></a>UnifiedMemoryManager：</h2><p>1.6后默认使用UnifiedMemoryManager，简单来说就是执行内存和存储内存能够相互借用。对于统一的内存管理，也分为堆内内存和堆外内存。</p>
<h3 id="堆内内存："><a href="#堆内内存：" class="headerlink" title="堆内内存："></a>堆内内存：</h3><p>堆内内存主要还是分为执行内存和存储内存，另外还有两块空间：UserMemory，Reserved Memory。具体图示：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762131167.png" alt="统一内存管理"></p>
<table>
<thead>
<tr>
<th>空间</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>execution</td>
<td>执行内存，计算、shuffle时使用</td>
</tr>
<tr>
<td>storage</td>
<td>存储内存，用来Cache数据</td>
</tr>
<tr>
<td>user memory</td>
<td>存储转换时的依赖信息。</td>
</tr>
<tr>
<td>reserved memory</td>
<td>用来存储Spark内部对象。</td>
</tr>
</tbody></table>
<p>通过观察UnifiedMemoryManager的源码，我们就能够得到这几块内存区域的计算公式：</p>
<table>
<thead>
<tr>
<th>区域</th>
<th>计算来源</th>
</tr>
</thead>
<tbody><tr>
<td>系统内存（SystemMemory）</td>
<td>近似小于executor-memory</td>
</tr>
<tr>
<td>预留内存（reservedMemory）</td>
<td>testing.reservedMemory（默认300M）</td>
</tr>
<tr>
<td>可用最大内存（MaxMemory）</td>
<td>（SystemMemory-reservedMemory）*memory.fraction(默认0.6)</td>
</tr>
<tr>
<td>用户内存</td>
<td>（SystemMemory-reservedMemory）*（1-memory.fraction)</td>
</tr>
<tr>
<td>执行内存</td>
<td>MaxMemory * 0.5</td>
</tr>
<tr>
<td>存储内存</td>
<td>MaxMemory * 0.5</td>
</tr>
</tbody></table>
<h3 id="堆外内存："><a href="#堆外内存：" class="headerlink" title="堆外内存："></a>堆外内存：</h3><p>堆外内存只分：执行内存和存储内存。功能上与堆内内存互补。两者的比例也是通过参数 <code>spark.memory.storageFraction</code> 控制。</p>
<p>相关参数：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>参数</th>
</tr>
</thead>
<tbody><tr>
<td>开启</td>
<td>spark.memory.offHeap.enabled</td>
</tr>
<tr>
<td>总大小</td>
<td>spark.memory.offHeap.size（只由这个参数控制，submit是那个参数只控制堆内内存）</td>
</tr>
<tr>
<td>比例</td>
<td>spark.memory.storageFraction</td>
</tr>
</tbody></table>
<h3 id="借用规则"><a href="#借用规则" class="headerlink" title="借用规则"></a>借用规则</h3><p>统一内存管理的有点就在于能够相互借用，具体规则其实很简单：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1581762641262.png" alt="借用规则"></p>
<ol>
<li>Executor和Storage都能够相互借用对方的内存，但以executor优先。</li>
<li>Storage需要借用时，只能借用executor释放后的内存。</li>
<li>Execution需要借用时，则可以主动要求storage释放占用的己方内存。</li>
<li>这种借用关系存在于堆外和堆内，但堆外不能借堆内，反之亦然。</li>
</ol>
<h4 id="从Spark的源码看两者的借用逻辑："><a href="#从Spark的源码看两者的借用逻辑：" class="headerlink" title="从Spark的源码看两者的借用逻辑："></a>从Spark的源码看两者的借用逻辑：</h4><ol>
<li>acquireStorageMemory</li>
</ol>
<p>核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">  // There is not enough free memory in the storage pool, so try to borrow free memory from</span><br><span class="line">  // the execution pool.</span><br><span class="line">  val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree,</span><br><span class="line">    numBytes - storagePool.memoryFree)</span><br><span class="line">  executionPool.decrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">  storagePool.incrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里就能够看到，但己方内存不够用时，Storage最终的借用内存。<code>Math.min(executionPool.memoryFree,      numBytes - storagePool.memoryFree)</code>，前者为Execution池中的空闲内存。</p>
<ol start="2">
<li>acquireExecutionMemory</li>
</ol>
<p>核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = &#123;</span><br><span class="line">  if (extraMemoryNeeded &gt; 0) &#123;</span><br><span class="line">    ...</span><br><span class="line">    if (memoryReclaimableFromStorage &gt; 0) &#123;</span><br><span class="line">      val spaceToReclaim = storagePool.freeSpaceToShrinkPool(</span><br><span class="line">        math.min(extraMemoryNeeded, memoryReclaimableFromStorage))</span><br><span class="line">      storagePool.decrementPoolSize(spaceToReclaim)</span><br><span class="line">      executionPool.incrementPoolSize(spaceToReclaim)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">def computeMaxExecutionPoolSize(): Long = &#123;</span><br><span class="line">  maxMemory - math.min(storagePool.memoryUsed, storageRegionSize)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">while (true) &#123;</span><br><span class="line">      ...</span><br><span class="line">      maybeGrowPool(numBytes - memoryFree)</span><br><span class="line">      val maxPoolSize = computeMaxPoolSize()</span><br><span class="line">      ...</span><br><span class="line">      val toGrant = math.min(maxToGrant, memoryFree)</span><br><span class="line">      if (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;</span><br><span class="line">        ...</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        memoryForTask(taskAttemptId) += toGrant</span><br><span class="line">        return toGrant</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这段具体的计算公式我们可以不过多关注，其实官方注释一句话就把这段逻辑解释清楚了，通过：</p>
<blockquote>
<p>The size the execution pool would have after evicting storage memory.</p>
</blockquote>
<p>上述代码在计算好待释放内存后，通过<code>storagePool.freeSpaceToShrinkPool</code> -&gt; <code>memoryStore.evictBlocksToFreeSpace</code>来完成内存释放。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2018/03/31/ben-di-spark-idea-fang-wen-yun-zhu-ji-shang-de-hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/31/ben-di-spark-idea-fang-wen-yun-zhu-ji-shang-de-hive/" class="post-title-link" itemprop="url">本地spark+idea访问云主机上的hive</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-31 21:50:00" itemprop="dateCreated datePublished" datetime="2018-03-31T21:50:00+08:00">2018-03-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-18 11:10:30" itemprop="dateModified" datetime="2020-02-18T11:10:30+08:00">2020-02-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录下本人使用本地spark+idea访问云主机遇到过的一些小问题。<br>仅供参考。欢迎讨论。</p>
<blockquote>
<p>spark版本：2.1.1 </p>
</blockquote>
<h2 id="使用sparkSQL-idea访问云主机HIVE"><a href="#使用sparkSQL-idea访问云主机HIVE" class="headerlink" title="使用sparkSQL+idea访问云主机HIVE"></a>使用sparkSQL+idea访问云主机HIVE</h2><h3 id="准备hive-site-xml"><a href="#准备hive-site-xml" class="headerlink" title="准备hive-site.xml"></a>准备hive-site.xml</h3><p>官网是这么描述的：</p>
<blockquote>
<p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.</p>
</blockquote>
<p>实际上，一般只需要准备hive-site.xml这个文件即可，因为spark访问Hive主要是通过访问hive的原数据库来完成的：<br><img src="https://img-blog.csdnimg.cn/20191014173213417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Ntb2tlcml1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以最重要的就是让spark程序能够访问到原数据库。本例是使用mysql。<br>贴一下hive-site.xml供参考。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hadoop换成云主机ip就好了，不过mysql需要注意开启远程访问权限 --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--客户端显示当前查询表的头信息 --&gt;</span> </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--客户端显示当前数据库名称信息 --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><p>主要是三个依赖：sparksql、mysql、spark-hive。<br>请根据需要进行更改。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="获取sparkSession"><a href="#获取sparkSession" class="headerlink" title="获取sparkSession"></a>获取sparkSession</h3><p>新版的spark，通过sparkSession来连接hive，不过有几个参数需要注意，先贴代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;hiveApp&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .config(<span class="string">&quot;dfs.client.use.datanode.hostname&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>
<p>需要<code>enableHiveSupport</code>来对hive进行支持。<br>访问云主机，需要<code>.config(&quot;dfs.client.use.datanode.hostname&quot;, &quot;true&quot;)</code>，因为hadoop返回对ip是内网地址，通过这个设置让hadoop返回主机名，这样我们配置下hosts就能够访问了。</p>
<h3 id="读取hive表"><a href="#读取hive表" class="headerlink" title="读取hive表"></a>读取hive表</h3><p>拿到正确的sparksession，就能对hive进行各种操作了。<br>这里贴一下完整代码方便查阅。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object hiveApp &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder().master(<span class="string">&quot;local[*]&quot;</span>).appName(<span class="string">&quot;hiveApp&quot;</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .config(<span class="string">&quot;dfs.client.use.datanode.hostname&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val df = spark.table(<span class="string">&quot;spark.person&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2018/02/17/linux-zhong-awk-yu-sed-zhi-ling-de-shi-yong-bi-ji/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/17/linux-zhong-awk-yu-sed-zhi-ling-de-shi-yong-bi-ji/" class="post-title-link" itemprop="url">Linux中awk与sed指令的使用笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-02-17 21:42:22" itemprop="dateCreated datePublished" datetime="2018-02-17T21:42:22+08:00">2018-02-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-17 23:21:27" itemprop="dateModified" datetime="2020-02-17T23:21:27+08:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近工作要使用到这对命令进行脚本编写，使用感觉特别容易忘，故记录整理下。</p>
<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>简单来说，awk是把文本逐行读入，默认以空格为列分隔符进行切片。对切好对数据再进行处理。<br>本文会再学习实验的同时，简单介绍一些常用的awk命令。</p>
<h3 id="使用方法："><a href="#使用方法：" class="headerlink" title="使用方法："></a>使用方法：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;&#123;pattern + action&#125;&#x27; &#123;filename&#125;</span><br></pre></td></tr></table></figure>
<p>pattern表示awk在数据中查找对内容。<br>action表示找到后执行什么命令。<br>{}用来对一系列指令进行分组。</p>
<h3 id="调用途径："><a href="#调用途径：" class="headerlink" title="调用途径："></a>调用途径：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1)命令行</span></span><br><span class="line">awk [-F field-separator] &#x27;commands&#x27; input-files</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">2)shell脚本</span></span><br><span class="line"><span class="meta">#</span><span class="bash">awk命令解释器作为脚本首行，相当于将<span class="comment">#!/bin/bash =&gt; #!/bin/awk</span></span></span><br></pre></td></tr></table></figure>
<h3 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h3><p>首先，准备一个文本格式对文件（a1）。以空格分开。<br>再准备一个文件b1，以【,】作为分隔符<br>比如说a1对内容：第三行会多处一列。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217214320.png" alt="awk"></p>
<h4 id="1）区分行与列，打印数据"><a href="#1）区分行与列，打印数据" class="headerlink" title="1）区分行与列，打印数据"></a>1）区分行与列，打印数据</h4><p>1）打印a1所有数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;&#123;print&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">输出：</span></span><br><span class="line">first line 3 4</span><br><span class="line">second second 3 4</span><br><span class="line">hadoop 3 4 50</span><br></pre></td></tr></table></figure>
<p>2）打印a1第一列，第二列数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;&#123;print $1,$2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">输出：</span></span><br><span class="line">first line</span><br><span class="line">second second</span><br><span class="line">hadoop 3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意,如果不加[,]</span></span><br><span class="line">awk &#x27;&#123;print $1$2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">则输出</span></span><br><span class="line">firstline</span><br><span class="line">secondsecond</span><br><span class="line">hadoop3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意,如果不加[$]</span></span><br><span class="line">awk &#x27;&#123;print $1 2&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">则输出</span></span><br><span class="line">first2</span><br><span class="line">second2</span><br><span class="line">hadoop2</span><br></pre></td></tr></table></figure>
<p>3）打印第1列第1+行数据：使用内置的NR变量。（文章结尾会总结下常用的内置变量）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;NR&gt;1&#123;print $1&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">second</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure>
<p>4）如果需要打印第2+列，就不那么友好了，不过也可以使用循环来完成：<br>打印第2+列的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;&#123;for(i=1;i&lt;=2;i++)$i=&quot;&quot;;print&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">  3 4</span><br><span class="line">  3 4</span><br><span class="line">  4 50</span><br></pre></td></tr></table></figure>
<h4 id="2）分隔符-正则查找"><a href="#2）分隔符-正则查找" class="headerlink" title="2）分隔符/正则查找"></a>2）分隔符/正则查找</h4><p>awk默认是识别空格为列分隔符，不过比如csv使用逗号来进行风格的，显然需要指定分隔符。这里会使用：-F fs。-F就是指定输入文件分隔符，fs可以是一个字符串，也可以是一个正则表达式</p>
<p>b1内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1,2,3,4,5,6</span><br><span class="line">a,b,c,d,e,f,g</span><br><span class="line">a,s,d,f,g,h,j,k</span><br><span class="line">a,a,a,a,a,a,a,a</span><br><span class="line">b,b,b,b,b,b,b,b</span><br></pre></td></tr></table></figure>

<p>1）读取以逗号为分隔符的b1文件，打印第2，3行，第1，2列：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk -F &#x27;,&#x27; &#x27;NR&gt;1 &amp;&amp; NR&lt;4&#123;print $1,$2&#125;&#x27; b1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">a b</span><br><span class="line">a s</span><br></pre></td></tr></table></figure>
<p>2）新建c1：文本内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a,1,ok</span><br><span class="line">b,2,no</span><br><span class="line">c,3,ioki</span><br></pre></td></tr></table></figure>
<p>找到ok，并打印第1列：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk -F &#x27;,&#x27; &#x27;/ok/&#123;print $1&#125;&#x27; c1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果：</span></span><br><span class="line">a</span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<h4 id="awk常用内置变量"><a href="#awk常用内置变量" class="headerlink" title="awk常用内置变量"></a>awk常用内置变量</h4><table>
<thead>
<tr>
<th align="left">变量</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ARGC</td>
<td align="left">命令行参数个数</td>
</tr>
<tr>
<td align="left">ARGV</td>
<td align="left">命令行参数排列</td>
</tr>
<tr>
<td align="left">ENVIRON</td>
<td align="left">支持队列中系统环境变量的使用</td>
</tr>
<tr>
<td align="left">FILENAME</td>
<td align="left">awk浏览的文件名</td>
</tr>
<tr>
<td align="left">FNR</td>
<td align="left">浏览文件的记录数</td>
</tr>
<tr>
<td align="left">FS</td>
<td align="left">设置输入域分隔符，等价于命令行 -F选项</td>
</tr>
<tr>
<td align="left">NF</td>
<td align="left">浏览记录的域的个数</td>
</tr>
<tr>
<td align="left">NR</td>
<td align="left">已读的记录数</td>
</tr>
<tr>
<td align="left">OFS</td>
<td align="left">输出域分隔符</td>
</tr>
<tr>
<td align="left">ORS</td>
<td align="left">输出记录分隔符</td>
</tr>
<tr>
<td align="left">RS</td>
<td align="left">控制记录分隔符</td>
</tr>
<tr>
<td align="left">1）使用：读取文件a1，统计文件名，行号，列数</td>
<td align="left"></td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">awk &#x27;&#123;print &quot;name:&quot;FILENAME&quot;,line:&quot;NR&quot;,cols:&quot;NF&#125;&#x27; a1</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">name:a1,line:1,cols:4</span><br><span class="line">name:a1,line:2,cols:4</span><br><span class="line">name:a1,line:3,cols:4</span><br></pre></td></tr></table></figure>
<h4 id="结合其他命令行"><a href="#结合其他命令行" class="headerlink" title="结合其他命令行"></a>结合其他命令行</h4><p>awk可以结合其他命令行返回第结果进行处理。通过|风格。awk第用法和上面将第是一样的。<br>1）利用awk统计最近5个登陆的用户的用户名和ip或主机名。（last后面不加指令)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">last | awk &#x27;NR&lt;5&#123;print $1,$3&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br><span class="line">hadoop 192.168.199.203</span><br></pre></td></tr></table></figure>

<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed是一种在线编辑器，每次处理一行内容。它将处理的行放入临时缓冲区，然后使用sed命令处理缓冲内容。默认会将处理好的内容发往屏幕，可以通过重定向，将内容输出至文件。<br>sed主要用来自动编辑1～多个文件。<br>特别的，sed是一行一行处理的。</p>
<h3 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed [nefr] [action] [file]</span><br></pre></td></tr></table></figure>
<p>部分：<br>| nefr | 含义                   |<br>| —- | ———————- |<br>| -f   | 使用file里到sed命令    |<br>| -i   | 直接将结果修改到file里 |</p>
<p>action：[n1[,n2]] func<br>| func | 含义                   |<br>| —- | ———————- |<br>| a    | 新增行（在该行后插入） |<br>| c    | 修改行                 |<br>| d    | 删除                   |<br>| i    | 插入行（在该行前插入） |<br>| s    | 取代（替换）           |</p>
<h4 id="简单使用：替换"><a href="#简单使用：替换" class="headerlink" title="简单使用：替换"></a>简单使用：替换</h4><p>1）将hello,hello,world的第一个hello替换为gwkki</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">语法：<span class="string">&#x27;s/pattern/replacement/flag&#x27;</span></span></span><br><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">gwkki,hello,world</span><br><span class="line"><span class="meta">#</span><span class="bash">特别的，这个替换会作用到每行第一个匹配到的。</span></span><br></pre></td></tr></table></figure>
<p>2）所有hello替换为gwkki</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/g&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">gwkki,gwkki,world</span><br></pre></td></tr></table></figure>
<p>3）替换第二个hello</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;hello,hello,world&quot; | sed &#x27;s/hello/gwkki/2&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">结果</span></span><br><span class="line">hello,gwkki,world</span><br></pre></td></tr></table></figure>
<p>说下flag：<br>数字表示替换第几处匹配到的。<br>g表示全部替换<br>p表示匹配的行会被额外打印。<br>w表示保存到指定文件（读取文件可以用 sed -i command来修改并保存）<br>4)修改c1中的xxx为yyy并保存：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i s/xxx/yyy/ a1</span><br></pre></td></tr></table></figure>

<h4 id="操作某些行："><a href="#操作某些行：" class="headerlink" title="操作某些行："></a>操作某些行：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">操作第二行：</span></span><br><span class="line">sed &#x27;2s/xxx/yyy/&#x27; a1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">操作2-4行：包含2，4</span></span><br><span class="line">sed &#x27;2,4s/xxx/yyy/&#x27; a1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">操作3+行：包含3</span></span><br><span class="line">sed ‘3，$s/xxx/yyy/’ a1</span><br></pre></td></tr></table></figure>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">删除匹配到k到行：</span></span><br><span class="line">sed &#x27;/k/d&#x27; c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">删除第3-5行：</span></span><br><span class="line">sed &#x27;3,5d&#x27; c1</span><br></pre></td></tr></table></figure>
<h4 id="插入新增"><a href="#插入新增" class="headerlink" title="插入新增"></a>插入新增</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">在<span class="string">&quot;line 1&quot;</span>的下一行新增line2</span></span><br><span class="line">echo &quot;line 1&quot; | sed &#x27;a line 2&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">在<span class="string">&quot;line 1&quot;</span>的上一行新增line2</span></span><br><span class="line">echo &quot;line 1&quot; | sed &#x27;i line 2&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="替换特殊字符-如：’"><a href="#替换特殊字符-如：’" class="headerlink" title="替换特殊字符 如：’"></a>替换特殊字符 如：’</h4><p>使用””扩起来即可：“可以用&quot;匹配到</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将<span class="string">&#x27;替换成&#123;&#125;</span></span></span><br><span class="line">sed &quot;s/\&#x27;/&#123;&#125;/&quot; a1</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2018/01/12/hadoop-yarn-de-ha-jia-gou-jie-xi/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/12/hadoop-yarn-de-ha-jia-gou-jie-xi/" class="post-title-link" itemprop="url">Hadoop+Yarn的HA架构解析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-01-12 00:25:40" itemprop="dateCreated datePublished" datetime="2018-01-12T00:25:40+08:00">2018-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-17 23:21:27" itemprop="dateModified" datetime="2020-02-17T23:21:27+08:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问。依赖其的组件也无法正确运转。重启NameNode是耗时的。导致其之前只适用于离线处理场景。</p>
<h2 id="HDFS的HA"><a href="#HDFS的HA" class="headerlink" title="HDFS的HA"></a>HDFS的HA</h2><p>hadoop在2.0版本中，解决了HDFS NameNode 和 YARN ResourceManger的单点问题。<br>整个架构可以参考两张图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222852.png" alt="HDFS的HA"></p>
<p>从图中可以看到，整个HA框架可以分为几个部分：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">两台 NameNode ，一主一备。一个皇上，一个皇上他儿子。</span><br><span class="line">备Name会不断从JournalNode中读取editLog，保持元数据信息与主NameNode一致，并实时与DataNode相互通信，使皇上挂了后，儿子能够正常无缝上位。 </span><br><span class="line">值得注意的是，只有主NameNode才能对外提供读写服务。（3.0支持多个standBy NameNodes了）</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>ZKFailoverController(zkfc)：主备切换控制器。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作为一个独立进程，对 NameNode 的主备切换进行总体控制。</span><br><span class="line">zkfc会不断检测到 NameNode 的健康状况，定期向zk集群发送心跳信息。</span><br><span class="line">自己被zk选举为active的时候，zkfc进程通过RPC协议调用使NN节点的状态变为active，对外提供实时服务</span><br><span class="line">（顺带一提，yarn中，也有类似的zkfc，不过是作为线程存在的。）</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Zookeeper集群：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为zkfc的选举切换提供支持。 </span><br><span class="line">（条件允许，可将zookepper配置在独立的机器上，防止部署了zk的机器业务过于繁忙，从而导致nameNode无法及时切换。） </span><br><span class="line">（由于nameNode是通过zk投票选举，zk部署太多也会影响选举效率）</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>DataNode 节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataNode 会同时向主备 NameNode 发送心跳信息、数据的块信息 。</span><br></pre></td></tr></table></figure></li>
<li><p>共享存储系统：（本文以JournalNodes为例）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">是实现高可用至关重要的一步。</span><br><span class="line">JournalNodes会保存NameNode 在运行过程中所产生的HDFS元数据，主从NameNode 通过JN实现元数据的同步。</span><br><span class="line">具体来说： </span><br><span class="line">	Active的NameNode的命名空间有任何修改时，会告知大部分的大部分的JournalNodes进程并将EditLog提交到JournalNodes中，</span><br><span class="line">	备NameNode会定期从JN中读取修改记录，并在自己这边进行重演，从而保证主从NameNode 的元数据信息相同</span><br><span class="line">	另外，对于HA集群，保证只有一个NameNode是Active也是非常重要的，否则两个NameNode的数据状态就会产生分歧，所以，JournalNodes必须确保同一时刻只有一个NameNode可以向自己写数据。</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>关于JournalNode</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行的JournalNode进程非常轻量，可以部署在其他的服务器上。</span><br><span class="line">注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个。</span><br><span class="line">（在active namenode写数据到journalnode上时，必须有半数以上的journalnode写成功的话，才标志写成功。因此需要奇数。）</span><br></pre></td></tr></table></figure>

<p>参考图：</p>
<h3 id="主备切换流程概述"><a href="#主备切换流程概述" class="headerlink" title="主备切换流程概述"></a>主备切换流程概述</h3><p>主备切换由：ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现。</p>
<ol>
<li>zkfc：<br>作为NameNode上的一个独立进程存在。<br>启动时会创建HealthMonitor和ActiveStandbyElector两个组件。并注册回调方法。</li>
<li>HealthMonitor：<br>负责检测NameNode的健康情况。</li>
<li>ActiveStandbyElector：<br>负责完成自动的主备选举。<br>参考图：<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222931.png" alt="主备切换流程概述"><h3 id="NameNode的元数据存储概述"><a href="#NameNode的元数据存储概述" class="headerlink" title="NameNode的元数据存储概述"></a>NameNode的元数据存储概述</h3>NameNode 在执行 HDFS 客户端提交写操作（如：创建文件、移动文件等）的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。<br>内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog <strong>仅仅只是在数据恢复的时候起作用</strong>。EditLog 会被切割为很多段，每一段称为一个 Segment。<br>正在写入的EditLog Segment，其文件名形如 edits_inprogress_{start_txid}。<br>写入完成的EditLog Segment，其文件名形如 edits_{start_txid}-{end_txid}。</li>
</ol>
<p>NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，<br>NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。<br><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217222638.png"></p>
<h3 id="实现主备切换的额外内容"><a href="#实现主备切换的额外内容" class="headerlink" title="实现主备切换的额外内容"></a>实现主备切换的额外内容</h3><p>主备NameNode实际是两个不同的机器。所以对外需要提供一个通用的命名空间服务，才能够在主Name故障时，无感知的完成切换。<br>对于用户或者代码来说，不需要关心主备切换的事情。<br>需要说明的是，nameSpace并<strong>不是一个独立进程</strong>。而仅仅知识core-site/hdfs-site之中的一些配置实现的。<br>通过命名空间的配置，通过</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs://nameNodeService/</span><br></pre></td></tr></table></figure>
<p>就能够直接访问，而不用去管自己访问的具体是那个NameNode。</p>
<h2 id="Yarn的HA"><a href="#Yarn的HA" class="headerlink" title="Yarn的HA"></a>Yarn的HA</h2><p>Yarn整体思路和Hdfs是差不多的，并且有相当部分的代码重用。不过也有一些区别，顺带记录下。<br>总体而言，数据是不能丢的,但是作业是可以挂的,挂了重启即可。因此YARN的架构比较轻量级。<br>本例中基于ZKRMStateStore。</p>
<p>HA整体框架图：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/win/20200217223126.png" alt="Yarn的HA"></p>
<p>从图中可以看到，整个HA框架可以分为几个部分：</p>
<ol>
<li>主备ResourceManager（RM）：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动时，向zk的目录（如：/hadoop-ha）写入一个lock文件。成功的成为Active节点。失败则是standBy。 </span><br><span class="line">StandBy的会去监控这个lock文件，如果不存在，则会尝试创建，以成为Active节点。</span><br><span class="line">RM会在Container上启动并监控ApplocationMaster（AM）。</span><br></pre></td></tr></table></figure>

<p>关于RM的本身用途（放一起方便回忆）： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ResourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统。</span><br><span class="line">NodeManager以心跳的方式向ResourceManager汇报资源使用情况（目前主要是CPU和内存的使用情况）。</span><br><span class="line">RM只接受NM的资源回报信息，对于具体的资源处理则交给NM自己处理。</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>NodeManager（NM）：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a）运行在所有节点上。</span><br><span class="line">b）启动Container容器，用于运行task运算。 </span><br><span class="line">c）Yarn的HA中，NM只会与Active的RM进行通信。 </span><br><span class="line">d）监控容器并上报资源：将Container的情况报告给RM，将Task的处理情况汇报给AM。</span><br></pre></td></tr></table></figure></li>
</ol>
<p>关于Container（放一起方便回忆）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Container（容器）是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。 </span><br><span class="line">Container 是 一种资源抽象，它封装了某个节点上的多维度资源。从而限定每个任务使用的资源量。</span><br><span class="line">一个节点会运行多个Container，但一个Container不会跨节点。 </span><br><span class="line">任何一个 job 或 application 必须运行在一个或多个 Container 中。</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>AppLocationMaster（AM）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a）AM运行在Container上，Container运行在NodeManager之上（The ApplicationMaster is started on a container） </span><br><span class="line">b）负责单个Applocation（Job）的task的资源管理和调度。计算Job的资源需求。  </span><br><span class="line">c）与调度器协商，并向RM进行资源申请。 </span><br><span class="line">d）向NM发出Launch Container请求。并接受NM的task处理状态信息。</span><br></pre></td></tr></table></figure></li>
<li><p>RMStateStore（本例中指：ZKRMStateStore）</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">负责数据交换。  </span><br><span class="line">active的RM会向zk的目录（如：/rmstore）写入Job信息。</span><br><span class="line">active的RM挂掉后，另一个standBy成功转换为Active后，会从该目录读取Job信息。重新构建作业的内存信息，随后启动内部服务。</span><br><span class="line">成为Active后，才会开始接受NM的心跳，接受Client的作业请求。</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>ZKFC：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">作为RM中的一个线程存在。 </span><br><span class="line">负责自动故障转移</span><br></pre></td></tr></table></figure>

<h2 id="HDFS与YARN的HA的主要区别"><a href="#HDFS与YARN的HA的主要区别" class="headerlink" title="HDFS与YARN的HA的主要区别"></a>HDFS与YARN的HA的主要区别</h2><table>
<thead>
<tr>
<th>HDFS</th>
<th>YARN</th>
</tr>
</thead>
<tbody><tr>
<td>standBy会不断从JN上读取修改数据，并进行复现。保证切换迅速</td>
<td>在Active节点挂掉后，才会从zk中读取Job信息。恢复不如HDFS那么及时，故更轻量。</td>
</tr>
<tr>
<td>HDFS HA使用JN集群同步数据</td>
<td>使用存储在Zookeeper中的RMstatestore同步数</td>
</tr>
<tr>
<td>ZKFC为进程</td>
<td>ZKFC为线程</td>
</tr>
<tr>
<td>Active和Standby都要接受来自NN的心跳等信息</td>
<td>只有Active才会接受NM的心跳</td>
</tr>
</tbody></table>
<p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager High Availability</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2017/12/01/yarn-zi-yuan-diao-you/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/12/01/yarn-zi-yuan-diao-you/" class="post-title-link" itemprop="url">Yarn资源调优</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-12-01 00:01:43" itemprop="dateCreated datePublished" datetime="2017-12-01T00:01:43+08:00">2017-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-17 23:21:27" itemprop="dateModified" datetime="2020-02-17T23:21:27+08:00">2020-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><p>Yarn的资源设置主要是6个参数，目的其实就是资源最大化。同时限制下面任务如某个Spark任务过度调用资源。</p>
<h2 id="CPU资源调度"><a href="#CPU资源调度" class="headerlink" title="CPU资源调度"></a>CPU资源调度</h2><p>目前的CPU被划分为虚拟CPU（CPU virtual Core）考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，可以通过多配置几个虚拟CPU弥补差异。CPU的相关配置如下：</p>
<ol>
<li><p>yarn.nodemanager.resource.cpu-vcores<br>表示该节点服务器上yarn可以使用的虚拟CPU个数，默认是8。</p>
<p>推荐将值配置与物理核心个数相同。如果CPU的性能好可以调大。</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-vcores<br>单个任务最小可申请的虚拟核心数，默认为1。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-vcores<br>单个任务最大可申请的虚拟核心数，默认为4，如果申请资源时，超过这个配置，会抛出InvalidResourceRequestException</p>
</li>
</ol>
<h2 id="Memory资源调度"><a href="#Memory资源调度" class="headerlink" title="Memory资源调度"></a>Memory资源调度</h2><p>yarn一般允许用户配置每个节点上可用的物理资源，集群上除了跑作业，还要留出资源给Hive、HDFS等其他服务。</p>
<ol>
<li><p>yarn.nodemanager.resource.memory-mb<br>设置该节点上yarn可使用的内存，默认为8G。</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb<br>单个任务最小申请物理内存量，默认1024MB。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-mb<br>单个任务最大申请物理内存量，默认为8291MB</p>
</li>
</ol>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>如果有一个服务器16核，64G内存，我们应该如何配置上面的6个参数呢（一句话：资源最大化利用）</p>
<ol>
<li><p>yarn.nodemanager.resource.cpu-vcores </p>
<p>虚拟CPU核数<br>根据具体的服务器水平去设置。即虚拟CPU的核数。我们生产设置的是32。</p>
</li>
<li><p>yarn.nodemanager.resource.memory-mb </p>
<p>总内存<br>生产上建议预留15%~20%的内存，所以我们设置的50G</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb </p>
<p>单任务最小内存<br>如果设置成2G，就是最多可以跑25个container<br>如果设置成3G，就是最多可以跑16个container</p>
</li>
<li><p>yarn.scheduler.minimum-allocation-mb </p>
<p>单任务最少虚拟Core<br>如果设置vcore = 1，就是最多可以跑32个container，但如果设置成这个，如果上面分配的是2G内存。最多只能跑25个container，就有点浪费了。<br>如果设置vcore = 2，就是最多可以跑16个container，内存就能用3G，比较均匀化</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-vcores</p>
<p>单任务最多虚拟Core<br>一般就设置成4个，cloudera公司做过性能测试。</p>
</li>
<li><p>yarn.scheduler.maximum-allocation-mb </p>
<p>单任务最大内存<br>如果有大任务，需要5-6G内存，那就设置为8G</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ssiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ssiu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false}});</script></body>
</html>
