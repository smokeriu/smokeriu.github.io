<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.ibuer.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Ssiu Blog">
<meta property="og:url" content="http://blog.ibuer.fun/page/5/index.html">
<meta property="og:site_name" content="Ssiu Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ssiu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://blog.ibuer.fun/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Ssiu Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ssiu Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2021/01/17/flink-de-shi-jian-yu-yi-ji-jian-dan-shi-yong/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/17/flink-de-shi-jian-yu-yi-ji-jian-dan-shi-yong/" class="post-title-link" itemprop="url">Flink的时间语义及简单使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-01-17 12:54:01" itemprop="dateCreated datePublished" datetime="2021-01-17T12:54:01+08:00">2021-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-25 11:53:07" itemprop="dateModified" datetime="2021-07-25T11:53:07+08:00">2021-07-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flink/" itemprop="url" rel="index"><span itemprop="name">Flink</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>本文Flink API版本：java-1.12</p>
</blockquote>
<blockquote>
<p>注1 ： Flink在1.11后，通过WatermarkStrategy来指定水位线及事件时间。将AssignerWithPeriodicWatermarks<code>和</code>AssignerWithPunctuatedWatermarks标记为过时。本文也主要是记录新接口的使用方法。</p>
<p>注2 ： Flink在1.12（大概）后，默认的时间策略为事件时间。</p>
<p>注3 ： 本文主要以Kafka作为输入元为例。仅简单介绍用法，共后续使用时的资料翻阅。</p>
<p>注4 ： 为了前后的连贯性，本文将不讨论在1.12中已被标记为过时的方法。若需使用早先的API，可翻阅其他资料。</p>
</blockquote>
<h2 id="分配时间戳和生成水位线"><a href="#分配时间戳和生成水位线" class="headerlink" title="分配时间戳和生成水位线"></a>分配时间戳和生成水位线</h2><p>使用事件时间，需要提供数据中<strong>代表时间的字段</strong>，并指定水位线。在新的FlinkAPI中，提供了常见的默认实现，来将时间戳的分配简单化：</p>
<h3 id="默认实现"><a href="#默认实现" class="headerlink" title="默认实现"></a>默认实现</h3><p>简单使用：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> WatermarkStrategy&lt;ObjectNode&gt; watermarkStrategy = WatermarkStrategy.</span><br><span class="line">				&lt;ObjectNode&gt;forBoundedOutOfOrderness(Duration.ofMillis(<span class="number">10</span>*<span class="number">1000</span>))</span><br><span class="line">				.withTimestampAssigner((SerializableTimestampAssigner&lt;ObjectNode&gt;) (element, recordTimestamp) -&gt; element.get(<span class="string">&quot;value&quot;</span>).get(<span class="string">&quot;log_time&quot;</span>).longValue());</span><br></pre></td></tr></table></figure>

<h4 id="水位线分配"><a href="#水位线分配" class="headerlink" title="水位线分配"></a>水位线分配</h4><p>通过调用WatermarkStrategy接口的默认方法，可以直接使用内置的通用策略。分别是</p>
<ul>
<li><p><code>forMonotonousTimestamps()</code>：单调递增的水位线分配器。</p>
<ul>
<li><p>当前时间的时间戳就充当watermark，如果后续数据时间戳更小，则认为是过期数据。</p>
</li>
<li><p>对于并行数据源。则是每个分区独立计算。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a watermark strategy for situations with monotonously ascending timestamps.<br>The watermarks are generated periodically and tightly follow the latest timestamp in the data. The delay introduced by this strategy is mainly the periodic interval in which the watermarks are generated.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>forBoundedOutOfOrderness(Duration.ofSeconds(10))</code></p>
<ul>
<li><p>参数表示数据之间允许的最大延迟。</p>
</li>
<li><p>如果新数据的时间戳 比 <strong>水位线-最大延迟 还要小</strong>。则认为是过期数据</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a watermark strategy for situations where records are out of order, but you can place an upper bound on how far the events are out of order. An out-of-order bound B means that once the an event with timestamp T was encountered, no events older than T - B will follow any more.<br>The watermarks are generated periodically. The delay introduced by this watermark strategy is the periodic interval length, plus the out of orderness bound.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>默认实现的水位线是周期性分配的。</p>
<ul>
<li>分配的周期取决于取决于<code>ExecutionConfig.getAutoWatermarkInterval()</code>。</li>
<li>该间隔可通过<code>env.getConfig().setAutoWatermarkInterval(long interval_ms)</code>设置。</li>
</ul>
</li>
</ul>
<h4 id="分配事件时间戳"><a href="#分配事件时间戳" class="headerlink" title="分配事件时间戳"></a>分配事件时间戳</h4><p>通过*.withTimestampAssigner*,可指定如何将数据的某个字段分配为时间戳，对于Kafka来说，若不指定则是将Kafka的record-time作为事件时间。</p>
<p>Flink提供两种方式来指定：</p>
<ul>
<li><p>TimestampAssignerSupplier<T>：为时间分配器提供更多上下文。</p>
<ul>
<li><p>实现方法需要返回一个TimestampAssigner。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>You can use this when a TimestampAssigner needs additional context, for example access to the metrics system.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>SerializableTimestampAssigner<T> ：指定如何将数据的某个字段分配为时间戳</p>
<ul>
<li><p>实现extractTimestamp方法，用来指定时间戳</p>
</li>
<li><p>SerializableTimestampAssigner实现的就是TimestampAssigner，并增加了序列化。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>You can use this in case you want to specify a TimestampAssigner via a lambda function.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="空闲数据"><a href="#空闲数据" class="headerlink" title="空闲数据"></a>空闲数据</h4><blockquote>
<p>摘自官网：</p>
<p>如果数据源中的某一个分区/分片在一段时间内未发送事件数据，则意味着 <code>WatermarkGenerator</code> 也不会获得任何新数据去生成 watermark。在这种情况下，当某些其他分区仍然发送事件数据的时候就会出现问题。由于下游算子 watermark 的计算方式是取所有不同的上游并行数据源 watermark 的最小值，则其 watermark 将不会发生变化。</p>
<p>故可以使用 <code>WatermarkStrategy</code> 来检测空闲输入并将其标记为空闲状态。</p>
</blockquote>
<p>WatermarkStrategy接口提供了<code>withIdleness(Duration idleTimeout)</code>默认方法。</p>
<p>即当某一个分区超过<em>多长时间</em>没有数据时，则标记为空闲。就算该分区的水位线没有变化，也不会影响整体水位线的变化。</p>
<ul>
<li><blockquote>
<p>注释原文：</p>
<p>Creates a new enriched WatermarkStrategy that also does idleness detection in the created WatermarkGenerator.<br>Add an idle timeout to the watermark strategy. If no records flow in a partition of a stream for that amount of time, then that partition is considered “idle” and will not hold back the progress of watermarks in downstream operators.<br>Idleness can be important if some partitions have little data and might not have events during some periods. Without idleness, these streams can stall the overall event time progress of the application</p>
</blockquote>
</li>
</ul>
<h4 id="注意点："><a href="#注意点：" class="headerlink" title="注意点："></a>注意点：</h4><ul>
<li>需<code>&lt;ObjectNode&gt;forBoundedOutOfOrderness</code>这般显示指定数据类型。</li>
</ul>
<h3 id="自定义实现"><a href="#自定义实现" class="headerlink" title="自定义实现"></a>自定义实现</h3><p>可以实现WatermarkStrategy接口，来自定义水位线策略：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> WatermarkStrategy&lt;ObjectNode&gt;() &#123;</span><br><span class="line">	<span class="comment">// 必须实现</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> WatermarkGenerator&lt;ObjectNode&gt; <span class="title">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> WatermarkGenerator&lt;ObjectNode&gt;() &#123;</span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(ObjectNode event, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;;</span><br><span class="line">	&#125;;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 选择实现</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> TimestampAssigner&lt;ObjectNode&gt; <span class="title">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h4><ul>
<li><p><code>createWatermarkGenerator</code>：必须实现</p>
<ul>
<li><p>方法需要返回一个<code>WatermarkGenerator</code>。可以通过实现对应接口自定义。</p>
<ul>
<li><code>onEvent(T event, long eventTimestamp, WatermarkOutput output)</code><ul>
<li>每条记录都会调用。根据记录记住事件时间戳，或更新水位线。</li>
</ul>
</li>
<li><code>onPeriodicEmit(WatermarkOutput output)</code><ul>
<li>定期调用。可能生成新的时间戳（也可能不会）生成间隔取决于ExecutionConfig.getAutoWatermarkInterval()</li>
</ul>
</li>
</ul>
</li>
<li><p>watermark 的生成方式本质上是有两种：<em>周期性生成</em> 和 <em>标记生成</em>。</p>
</li>
<li><p><strong>周期性</strong>生成器通常的实现逻辑：</p>
<ul>
<li><p>通过 <code>onEvent()</code>更新事件时间戳。</p>
</li>
<li><p>通过<code>onPeriodicEmit()</code>发出水位线。注意：<em>onPeriodicEmit方法的调用是定时调用的</em></p>
</li>
<li><blockquote>
<p> 官网示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessGenerator</span> <span class="keyword">implements</span> <span class="title">WatermarkGenerator</span>&lt;<span class="title">MyEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness = <span class="number">3500</span>; <span class="comment">// 3.5 秒</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(MyEvent event, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 发出的 watermark = 当前最大时间戳 - 最大乱序时间</span></span><br><span class="line">        output.emitWatermark(<span class="keyword">new</span> Watermark(currentMaxTimestamp - maxOutOfOrderness - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</blockquote>
</li>
</ul>
</li>
<li><p><strong>标记</strong>生成器通常的实现逻辑：</p>
<ul>
<li><p>通过 <code>onEvent()</code>判断是否需要更新（如数据中明确的字段值），如果需要则直接更新。</p>
</li>
<li><blockquote>
<p>官网示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class PunctuatedAssigner implements WatermarkGenerator&lt;MyEvent&gt; &#123;</span><br><span class="line"></span><br><span class="line"> @Override</span><br><span class="line"> public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) &#123;</span><br><span class="line">     if (event.hasWatermarkMarker()) &#123;</span><br><span class="line">         output.emitWatermark(new Watermark(event.getWatermarkTimestamp()));</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> @Override</span><br><span class="line"> public void onPeriodicEmit(WatermarkOutput output) &#123;</span><br><span class="line">     // onEvent 中已经实现</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>createTimestampAssigner</code>：选择实现</p>
<ul>
<li>参考：[分配事件时间戳](# 分配事件时间戳)</li>
</ul>
</li>
</ul>
<h3 id="为数据源指定事件时间"><a href="#为数据源指定事件时间" class="headerlink" title="为数据源指定事件时间"></a>为数据源指定事件时间</h3><p>通过上述过程，我们很容易得到一个<strong>WatermarkStrategy</strong>，通过这个，我们就能够用来指定数据中的事件时间戳和水位线了。</p>
<p>以Kafka为例，如果要修改默认的水位线策略，只需通过<code>.assignTimestampsAndWatermarks()</code>方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myConsumer.assignTimestampsAndWatermarks(watermarkStrategy);</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>.assignTimestampsAndWatermarks</code>方法同样是DataStream中提供的方法。</p>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="使用事件时间及水位线"><a href="#使用事件时间及水位线" class="headerlink" title="使用事件时间及水位线"></a>使用事件时间及水位线</h2><p>todo 概述</p>
<h3 id="处理函数"><a href="#处理函数" class="headerlink" title="处理函数"></a>处理函数</h3><p>上述内容中得到的时间戳和水位线存放于程序的上下文<code>(Context ctx)</code>中，一般而言可以通过Process及相关的函数进行访问。</p>
<blockquote>
<p>处理函数都实现了RichFunction接口</p>
</blockquote>
<p>对于ProcessFunction，一般提供下述两个方法：</p>
<ul>
<li><code>processElement(I value, Context ctx, Collector&lt;O&gt; out)</code>：必须实现<ul>
<li>每条记录调用一次。</li>
<li>通过<code> Context ctx</code>访问<strong>时间戳</strong>。（也可以访问一些其他的上下文信息，如TimerService，将结果发往副结果）</li>
</ul>
</li>
<li><code>onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</code>：选择实现<ul>
<li>会在计时器触发时调用。<strong>故一般计时器会在<code>processElement</code>中设置</strong></li>
<li><code>timestamp</code>参数指的是触发计时器的时间戳。</li>
<li>用在<code>KeyedProcessFunction</code>中，用来在某些键值不再使用后，清除分区状态/或实现一些基于时间的自定义窗口逻辑。</li>
</ul>
</li>
<li>计时器：通过ctx的<code>timerService</code>使用<ul>
<li><strong>只能在键值流中使用计时器</strong>（*Setting timers is only supported on a keyed streams.*）</li>
<li>每个时间戳可以拥有一个计时器。一个key可以有多个计时器（key不同时，计时器的时间戳允许重复）</li>
<li><strong>不应该注册过多的计时器。建议在每个processFunction通过维护状态来管理计时器（注册+删除）。</strong></li>
<li>计时器触发后，就自动删除了？（todo）</li>
<li><code>timerService</code>：<ul>
<li><code>registerEventTimeTimer(long time)</code>：注册一个事件时间计时器。<ul>
<li>参数就相当于计时器id及出发条件。</li>
<li>当水位线时间戳 大于或等于计时器时间戳，则触发。</li>
</ul>
</li>
<li><code>deleteEventTimeTimer(long time)</code>：删除一个事件时间计时器</li>
<li><code>currentProcessingTime()</code>返回当前的处理时间</li>
<li><code>currentWatermark()</code>返回当前水位线</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="窗口算子"><a href="#窗口算子" class="headerlink" title="窗口算子"></a>窗口算子</h3><blockquote>
<p>1、本文只考虑基于时间的窗口。</p>
<p>2、窗口可用于键值与非键值数据流。本文主要以键值数据流为例进行说明。</p>
<p>3、对于键值数据流，窗口算子为并行计算。对于非键值，考虑到需要处理整个窗口的所有元素，需要单线程处理。</p>
</blockquote>
<p>实际使用窗口算子时，必需要指定两个窗口组件：</p>
<ul>
<li>窗口分配器<code>WindowAssigner</code>，得到<code>WindowedStream</code>：决定了元素如何划分到不同的窗口中。</li>
<li>作用于<code>WindowedStream</code>上的处理函数：如何处理单个窗口中的数据。</li>
</ul>
<p>如果需要自定义某些实现逻辑，还可以指定其他窗口组件：</p>
<ul>
<li>触发器<code>trigger</code>：定义了窗口何时准备好执行计算、何时需要清除自身内容。</li>
<li>移除器<code>evictor</code>：用来从窗口中删除已收集元素。可以在ProcessWindowFunc前/后使用。</li>
</ul>
<p>简单示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">baseInput.keyBy(((KeySelector&lt;Tuple4&lt;Integer, Long, Long, String&gt;, Integer&gt;) value -&gt; value.f0))</span><br><span class="line">				.window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>)))</span><br><span class="line">				.process(<span class="keyword">new</span> ProcessWindowFunction&lt;Tuple4&lt;Integer, Long, Long, String&gt;, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">					<span class="meta">@Override</span></span><br><span class="line">					<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Integer integer, Context context, Iterable&lt;Tuple4&lt;Integer, Long, Long, String&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">						StringBuffer buffer = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">						buffer.append(integer).append(<span class="string">&quot;!&quot;</span>);</span><br><span class="line">						elements.forEach(ele -&gt; buffer.append(ele.f3).append(<span class="string">&quot;-&quot;</span>));</span><br><span class="line">						out.collect(buffer.toString());</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;);</span><br></pre></td></tr></table></figure>

<h4 id="窗口分配器"><a href="#窗口分配器" class="headerlink" title="窗口分配器"></a>窗口分配器</h4><h5 id="内置窗口分配器"><a href="#内置窗口分配器" class="headerlink" title="内置窗口分配器"></a>内置窗口分配器</h5><ul>
<li><p>TumblingWindow（滚动窗口）：下一个窗口的开始为：这个窗口的结束时间。</p>
<ul>
<li><p>提供两个内置实现（事件时间和处理时间各一个）：</p>
<ul>
<li><p><code>TumblingEventTimeWindows</code>：基于事件时间的滚动窗口。</p>
</li>
<li><p><code>of</code>方法接收参数：</p>
<ul>
<li><p>size : 窗口大小</p>
</li>
<li><p>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</p>
</li>
<li><p>windowStagger：交错策略。实际源码中表现为会影响offset的效果。</p>
<ul>
<li><blockquote>
<p>The utility that produces staggering offset in runtime. </p>
<p>有三类：</p>
<p>​    ALIGNED：默认。不产生任何影响。所有分区的窗口开始时间都是一样的</p>
<p>​    RANDOM：加入一个随机值作为交错参数。即每个分区的第一个窗口的开始时间会随机开始。</p>
<p>​    NATURAL：根据每个线程得到的第一个元素，来作为该分区的第一个窗口的开始时间。</p>
<p>（待验证）</p>
<p>Flink会在WindowOperator中的processElement方法中，对每个接收到的流元素进行窗口画风。而这个参数会影响到最后生成的elementWindows，windowStagger主要就是通过影响offset，来影响elementWindows的生成。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>窗口大小决定了多久会生成一个新的窗口。</p>
</li>
</ul>
</li>
<li><p>SlidingTimeWindow（滑动窗口）：下一个窗口开始时间为：这个窗口开始时间+滑动距离。</p>
<ul>
<li>提供两个内置实现（事件时间和处理时间各一个）：<ul>
<li><code>SlidingEventTimeWindows</code>：基于事件时间的滑动窗口。</li>
<li><code>of</code>方法接收参数：<ul>
<li>size : 窗口大小。</li>
<li>slide：滑动距离。</li>
<li>offset : 偏移量。相比于UTC±00:00。否则第一个滚动窗口开始时间一定是0点。</li>
</ul>
</li>
</ul>
</li>
<li>滑动距离决定了多久会生成一个新的窗口。</li>
</ul>
</li>
<li><p>sessionWindow（会话窗口）：下一个窗口开始时间为：新元素到达 且距离上一个元素到达时间超过阈值。</p>
<ul>
<li><p>提供两类(动态和固定)，供四个（事件时间和处理时间各两个）内置实现：</p>
<ul>
<li><p><code>EventTimeSessionWindows</code>：基于事件时间的会话窗口。</p>
<ul>
<li>方法：<ul>
<li><code>withGap(Time size)</code>：间隔阈值。</li>
</ul>
</li>
</ul>
</li>
<li><p><code>DynamicEventTimeSessionWindows</code>：基于事件时间的动态会话窗口。</p>
<ul>
<li><p>方法：</p>
<ul>
<li><p><code>withDynamicGap(SessionWindowTimeGapExtractor&lt;T&gt; sessionWindowTimeGapExtractor)</code>：</p>
<ul>
<li><blockquote>
<p>SessionWindowTimeGapExtractor能够从数据中提取超时间隔。</p>
<p>即数据中可以有一个字段，来作为这条数据距离上一条数据多久。<code>SessionWindowTimeGapExtractor</code>接口需要手动实现。且只有一个方法，从数据中拿到这他的间隔<code>long extract(T element);</code></p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>两者的主要区别：</p>
<ul>
<li>非动态：超时间隔是固定的。</li>
<li>动态：有数据自己决定是否划分新窗口。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>globalWindow（全局窗口）: 只会产生一个窗口</p>
<ul>
<li>不会自动触发，必须手动指定<em>触发器</em>。</li>
<li>必须指定<em>移除器</em>来移除窗口中的元素</li>
</ul>
</li>
</ul>
<h5 id="自定义窗口分配器"><a href="#自定义窗口分配器" class="headerlink" title="自定义窗口分配器"></a>自定义窗口分配器</h5><p><code>WindowAssigner</code>接口提供了四个方法需要实现：</p>
<ul>
<li><code>assignWindows(T element, long timestamp, WindowAssignerContext context)</code>：返回元素归属的窗口集合（这条记录归属哪些窗口）<ul>
<li>由于一般都是针对时间设计，所以一般都是返回TimeWindow。基于数量的窗口可以使用<code>countWindow</code></li>
</ul>
</li>
<li><code>getDefaultTrigger(StreamExecutionEnvironment env)</code>：返回该分配器的默认触发器。<ul>
<li>请参考自定义触发器，或者Trigger子类的内部实现。</li>
</ul>
</li>
<li><code>getWindowSerializer(ExecutionConfig executionConfig)</code>：返回窗口的TypeSerializer<ul>
<li>对于TimeWindow而言，则返回<code>new TimeWindow.Serializer()</code>即可。</li>
</ul>
</li>
<li><code>isEventTime()</code>：是否是事件时间窗口</li>
</ul>
<h4 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h4><p>触发器决定何时对窗口进行计算并发出结果。每次调用触发器都会生成一个<code>TriggerResult</code>，用于决定窗口接下来的行为：</p>
<ul>
<li>CONTINUE：什么都不做。</li>
<li>FIRE：调用<code>ProcessFunction</code>（如果有使用），并发出结果。如果包含增量聚合函数，则直接发出结果。</li>
<li>PURGE：清空窗口内容，并删除窗口、及窗口元数据。并调用<code>ProcessWindowFunction.clear()</code>方法。</li>
<li>FIRE_AND_PURGE：先FIRE，再PURGE。</li>
</ul>
<h5 id="内置触发器"><a href="#内置触发器" class="headerlink" title="内置触发器"></a>内置触发器</h5><p>Flink提供一些内置的触发器，包括（只说明事件时间的，处理时间一样的）：</p>
<ul>
<li><p><code>EventTimeTrigger</code>：水位线大于窗口结束时间，则触发。</p>
</li>
<li><p><code>CountTrigger</code>：当窗口内元素数量大于阈值时则触发。</p>
</li>
<li><p><code>DeltaTrigger</code>：初始化是需要给定一个<code>DeltaFunction</code>，根据接入数据计算出来的Delta指标是否超过指定的Threshold去判断是否触发窗口计算。</p>
<ul>
<li>计算当前元素与上一个触发计算元素的Delta值来与阈值进行比较。</li>
</ul>
</li>
<li><p><code>ContinuousEventTimeTrigger</code>:</p>
<ul>
<li><p>水位线大于窗口结束时间，则触发。</p>
</li>
<li><p>定期设置处理时间计时器。（计时器触发时会调用触发器的<code>onEventTime</code>方法。详见自定义）</p>
<ul>
<li><blockquote>
<p>1、当第一个元素达到时，会根据时间间隔注册一个计时器。触发时间约为开始时间+时间间隔（<code>start = timestamp - (timestamp % interval)+interval</code>）</p>
<p>2、当计时器触发时（），进行判断。</p>
<p>​    2.1、如果触发事件 等于 窗口结束时间，则触发触发器。</p>
<p>​    2.2、其他情况，则判断计时器是否有效，若有效则触发触发器，并根据设定的间隔注册下一个计时器。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>PurgingTrigger</code>：是一个trigger的包装类。具体作用为：如果被包装的trigger触发返回FIRE，则PurgingTrigger将返回修改为FIRE_AND_PURGE，其他的返回值不做处理。</p>
<ul>
<li>通过<code>of(Trigger&lt;T, W&gt; nestedTrigger)</code>来包装其他<code>Trigger</code></li>
</ul>
</li>
<li><p><code>ProcessingTimeoutTrigger</code>是一个trigger的包装类，作用于处理时间。具体作用为：为元素配置处理时间超时。当到达超时时间时，清除触发器状态</p>
</li>
</ul>
<h5 id="自定义触发器"><a href="#自定义触发器" class="headerlink" title="自定义触发器"></a>自定义触发器</h5><p>自定义触发器需要实现触发器API的下述方法：</p>
<ul>
<li><code>onElement(T element, long timestamp, W window, TriggerContext ctx)</code>：每有元素添加到窗口时就调用。<ul>
<li>入参<code>long timestamp</code>为事件的时间属性。</li>
<li>入参<code>W window</code>为时间所属的窗口对象。（对于一个元素所属多个窗口时，会对应有多个触发器，每个触发器只负责一个窗口）</li>
</ul>
</li>
<li><code>onProcessingTime(long time, W window, TriggerContext ctx)</code>：处理时间计时器触发时调用。计时器通过ctx注册/清除。</li>
<li><code>onEventTime(long time, W window, TriggerContext ctx)</code>事件时间计时器触发时调用。计时器通过ctx注册/清除。<ul>
<li>入参<code>long time</code>为触发计时器的时间。等于计时器注册时间。</li>
</ul>
</li>
<li><code>canMerge()</code>：该触发器是否支持合并。</li>
<li><code>onMerge(W window, OnMergeContext ctx)</code>：合并逻辑（将多个窗口合并为一个窗口），需要合并时则调用该方法。<ul>
<li>在触发器与<code>MergingWindowAssigner</code>一起使用时，需要实现该方法。</li>
<li>触发器的自定义状态同样需要合并。</li>
<li>入参<code>W window</code>为合并后的窗口。</li>
</ul>
</li>
<li><code>clear(W window, TriggerContext ctx)</code>：在触发器中清除那些为给定窗口保存的状态。</li>
</ul>
<h4 id="移除器"><a href="#移除器" class="headerlink" title="移除器"></a>移除器</h4><p>用于在窗口执行计算前或计算后删除窗口中的元素。是一个可选组件。</p>
<h5 id="内置移除器"><a href="#内置移除器" class="headerlink" title="内置移除器"></a>内置移除器</h5><p>Flink提供了三类内置的<code>Evictor</code>：</p>
<ul>
<li><code>TimeEvictor</code>：以时间为判断标准，决定元素是否会被移除。<ul>
<li>该移除器初始化时需要赋予一个时间size大小。移除比 最大时间-size 还小的数据。</li>
</ul>
</li>
<li><code>CountEvictor</code>：以元素计数为标准，决定元素是否会被移除。<ul>
<li>该移除器初始化时需要赋予一个数量size大小。移除多余的元素。从前向后移除。</li>
</ul>
</li>
<li><code>DeltaEvictor</code>：DeltaEvictor通过计算DeltaFunction的值（依次传入每个元素和最后一个元素），并将其与threshold进行对比，如果函数计算结果大于等于threshold，则该元素会被移除。</li>
</ul>
<h5 id="自定义移除器"><a href="#自定义移除器" class="headerlink" title="自定义移除器"></a>自定义移除器</h5><p>自定义移除器需要实现移除器API的下述方法：</p>
<ul>
<li><code>evictBefore(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, int size, W window, EvictorContext ctx)</code>：窗口函数作用于窗口内容前调用。</li>
<li><code>evictAfter(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, int size, W window, EvictorContext ctx)</code>：窗口函数作用于窗口内容后调用。<ul>
<li>入参<code>(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements</code>：窗口中的元素集合。移除操作主要对这个集合处理。满足条件时<code>iterator.remove();</code></li>
<li>入参<code>int size</code>：当前的元素个数。</li>
<li>入参<code>W window</code>：本次处理的窗口。</li>
</ul>
</li>
</ul>
<h2 id="处理迟到数据"><a href="#处理迟到数据" class="headerlink" title="处理迟到数据"></a>处理迟到数据</h2><p>即便设置了水位线，也不可避免的会产生迟到数据。</p>
<blockquote>
<p>迟到数据一般是针对window而言，因为window需要聚合一段时间内的数据触发计算。</p>
<p>不需要window的情况下，在处理过程中其实对迟到数据不需要特殊处理。</p>
</blockquote>
<p>Flink提供了三种处理方案：</p>
<ul>
<li>直接丢弃。默认方案</li>
<li>基于迟到事件更新结果。</li>
<li>旁路输出。</li>
</ul>
<h3 id="基于迟到事件更新结果"><a href="#基于迟到事件更新结果" class="headerlink" title="基于迟到事件更新结果"></a>基于迟到事件更新结果</h3><p>Flink的<code>WindowedStream</code>提供了<code>allowedLateness(Time lateness)</code>方法。入参为允许延迟的时间（默认为0）。这个方法的作用和实现方式：</p>
<ul>
<li>当设置lateness不为0时，窗口被删除的时间会在原本的基础上推迟lateness。</li>
<li>当水位线触发计算后，每当有满足要求的迟到数据达到时，都会再次触发计算。</li>
</ul>
<h3 id="旁路输出"><a href="#旁路输出" class="headerlink" title="旁路输出"></a>旁路输出</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/06/24/spark-3-datasourcev2-01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/24/spark-3-datasourcev2-01/" class="post-title-link" itemprop="url">spark-3 DataSourceV2-01</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-24 16:19:08 / 修改时间：17:58:54" itemprop="dateCreated datePublished" datetime="2020-06-24T16:19:08+08:00">2020-06-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark3/" itemprop="url" rel="index"><span itemprop="name">Spark3</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Java-Interfaces"><a href="#Java-Interfaces" class="headerlink" title="Java Interfaces"></a>Java Interfaces</h2><p>Spark3中，DataSourceV2采用Java进行编写，而非Scala。这样做的主要目的是为了更好的和Java交互。</p>
<p>大部分接口可以在下面的包中找到：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.connector.catalog</span><br></pre></td></tr></table></figure>

<p>后文是DataSrouceV2的关键接口。</p>
<h2 id="TableProvider"><a href="#TableProvider" class="headerlink" title="TableProvider"></a>TableProvider</h2><p>这是一个提供读写的数据源，他提供结构化数据。需要实现如下方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">StructType <span class="title">inferSchema</span><span class="params">(CaseInsensitiveStringMap options)</span></span>;</span><br><span class="line"><span class="function">Table <span class="title">getTable</span><span class="params">(StructType schema, Transform[] partitioning, Map&lt;String, String&gt; properties)</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>inferSchema：</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Infer the schema of the table identified by the given options.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> options an immutable case-insensitive string-to-string map that can identify a table,</span></span><br><span class="line"><span class="comment"> *                e.g. file path, Kafka topic name, etc.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过给定的options，推断表结构。从而实现结构化数据。</p>
<ol start="2">
<li>getTable</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a &#123;<span class="doctag">@link</span> Table&#125; instance with the specified table schema, partitioning and properties</span></span><br><span class="line"><span class="comment"> * to do read/write. The returned table should report the same schema and partitioning with the</span></span><br><span class="line"><span class="comment"> * specified ones, or Spark may fail the operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> schema The specified table schema.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partitioning The specified table partitioning.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> properties The specified table properties. It&#x27;s case preserving (contains exactly what</span></span><br><span class="line"><span class="comment"> *                   users specified) and implementations are free to use it case sensitively or</span></span><br><span class="line"><span class="comment"> *                   insensitively. It should be able to identify a table, e.g. file path, Kafka</span></span><br><span class="line"><span class="comment"> *                   topic name, etc.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过用户给定的信息去load数据。</p>
<h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><p>Table是逻辑结构化数据集的接口，需要实现下述3种方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">String <span class="title">name</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">StructType <span class="title">schema</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Set&lt;TableCapability&gt; <span class="title">capabilities</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>name</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A name to identify this table. Implementations should provide a meaningful name, like the</span></span><br><span class="line"><span class="comment"> * database and table name from catalog, or the location of files for this table.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>用于标示表的名称。</p>
<ol start="2">
<li>schema</li>
</ol>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the schema of this table. If the table is not readable and doesn&#x27;t have a schema, an</span><br><span class="line"> * empty schema can be returned here.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回表的schema，如果表不可读，或者没有schema时，将返回空的schema</p>
<ol start="3">
<li>capabilities</li>
</ol>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the set of capabilities for this table.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回表的性能。这是Spark3新的功能，允许指定支持哪种类型的操作表。如BATCH_READ、BATCH_WRITE。这有助于Spark在尝试运行操作之前验证这些信息。</p>
<h2 id="SupportsRead"><a href="#SupportsRead" class="headerlink" title="SupportsRead"></a>SupportsRead</h2><p>该接口表示数据源是支持读的，需要实现下述方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ScanBuilder <span class="title">newScanBuilder</span><span class="params">(CaseInsensitiveStringMap options)</span></span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a &#123;<span class="doctag">@link</span> ScanBuilder&#125; which can be used to build a &#123;<span class="doctag">@link</span> Scan&#125;. Spark will call this</span></span><br><span class="line"><span class="comment"> * method to configure each data source scan.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> options The options for reading, which is an immutable case-insensitive</span></span><br><span class="line"><span class="comment"> *                string-to-string map.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回一个ScanBuilder用来创建一个Scan（Scan会用来读取表，后文会介绍）。Spark会在配置Souce Scan时调用这个方法。</p>
<h2 id="ScanBuilder"><a href="#ScanBuilder" class="headerlink" title="ScanBuilder"></a>ScanBuilder</h2><p>该接口会用来创建Scan，这个接口可以配合谓词下推（SupportsPushDownXYZ），帮助Scan读取所需数据。需要实现下述方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scan build();</span><br></pre></td></tr></table></figure>

<h2 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h2><p>数据源扫描的逻辑表现形式。此接口用于提供逻辑信息，如实际的读取模式。这是所有不同扫描模式的通用接口（批、微批等）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">StructType <span class="title">readSchema</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">default</span> Batch <span class="title">toBatch</span><span class="params">()</span></span>&#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>readSchema</p>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the actual schema of this data source scan, which may be different from the physical</span></span><br><span class="line"><span class="comment"> * schema of the underlying storage, as column pruning or other optimizations may happen.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>数据源的实际结构，这个看起来似乎和Table接口有重复。这是因为，在发生列裁剪或其他优化后，结构可能发生改变。这个方法返回的是最初的schema。</p>
<ol start="2">
<li>toBatch</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the physical representation of this scan for batch query. By default this method throws</span></span><br><span class="line"><span class="comment"> * exception, data sources must overwrite this method to provide an implementation, if the</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> Table&#125; that creates this scan returns &#123;<span class="doctag">@link</span> TableCapability#BATCH_READ&#125; support in its</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> Table#capabilities()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> UnsupportedOperationException</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>在批处理时，这个方法需要被override。</p>
<h2 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h2><p>进行批处理时的数据源的物理表现，这个接口用于提供一些物理信息，如：扫描的数据有多少分区，如何去读取分区数据。</p>
<p>需要实现如下方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InputPartition[] planInputPartitions();</span><br><span class="line"><span class="function">PartitionReaderFactory <span class="title">createReaderFactory</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<ol>
<li>planInputPartitions</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a list of &#123;<span class="doctag">@link</span> InputPartition input partitions&#125;. Each &#123;<span class="doctag">@link</span> InputPartition&#125;</span></span><br><span class="line"><span class="comment"> * represents a data split that can be processed by one Spark task. The number of input</span></span><br><span class="line"><span class="comment"> * partitions returned here is the same as the number of RDD partitions this scan outputs.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * If the &#123;<span class="doctag">@link</span> Scan&#125; supports filter pushdown, this Batch is likely configured with a filter</span></span><br><span class="line"><span class="comment"> * and is responsible for creating splits for that filter, which is not a full scan.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * This method will be called only once during a data source scan, to launch one Spark job.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回输入分区列表，这个方法决定了数据的分区数。</p>
<ol start="2">
<li>createReaderFactory</li>
</ol>
<blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a factory to create a &#123;<span class="doctag">@link</span> PartitionReader&#125; for each &#123;<span class="doctag">@link</span> InputPartition&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>返回一个用于创建PartitionReader的工厂，PartitionReader和InputPartition是一一对应的。</p>
<h2 id="PartitionReaderFactory"><a href="#PartitionReaderFactory" class="headerlink" title="PartitionReaderFactory"></a>PartitionReaderFactory</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/" class="post-title-link" itemprop="url">spark-3.0 Hadoop2.6-CDH编译的问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-22 21:27:58 / 修改时间：21:46:23" itemprop="dateCreated datePublished" datetime="2020-06-22T21:27:58+08:00">2020-06-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark3/" itemprop="url" rel="index"><span itemprop="name">Spark3</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>使用cdh5版本编译Spark3时，会出现如下问题：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; [ERROR] [Error]</span><br><span class="line">&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:298:</span><br><span class="line">&gt; value setRolledLogsIncludePattern is not a member of</span><br><span class="line">&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext</span><br><span class="line">&gt; [ERROR] [Error]</span><br><span class="line">&gt; /Users/xxx/Documents/codes/xxx/spark-3.0/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:300:</span><br><span class="line">&gt; value setRolledLogsExcludePattern is not a member of</span><br><span class="line">&gt; org.apache.hadoop.yarn.api.records.LogAggregationContext</span><br></pre></td></tr></table></figure>

<p>这个问题起始官方曾经修复过，不过后来可能改过这部分代码，导致编译报错：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/spark/pull/16884">Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p>
<p>这是实际解决的连接：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/spark/pull/16884/commits/d7c7e81bafa229bb6083ed5b29789b3f9cb78bf7">[SPARK-19545][YARN]Fix compile issue for Spark on Yarn when building against Hadoop 2.6.0~2.6.3</a></p>
<p>我们仔细对比下Spark3中的代码，这段代码亦和修复前的代码一样（只是spark3中多了try-catch）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> logAggregationContext = <span class="type">Records</span>.newRecord(classOf[<span class="type">LogAggregationContext</span>])</span><br><span class="line">logAggregationContext.setRolledLogsIncludePattern(includePattern)</span><br><span class="line">sparkConf.get(<span class="type">ROLLED_LOG_EXCLUDE_PATTERN</span>).foreach &#123; excludePattern =&gt;</span><br><span class="line">  logAggregationContext.setRolledLogsExcludePattern(excludePattern)</span><br><span class="line">&#125;</span><br><span class="line">appContext.setLogAggregationContext(logAggregationContext)</span><br></pre></td></tr></table></figure>

<p>然后是当初修复后的代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> logAggregationContext = <span class="type">Records</span>.newRecord(classOf[<span class="type">LogAggregationContext</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// These two methods were added in Hadoop 2.6.4, so we still need to use reflection to</span></span><br><span class="line"><span class="comment">// avoid compile error when building against Hadoop 2.6.0 ~ 2.6.3.</span></span><br><span class="line"><span class="keyword">val</span> setRolledLogsIncludePatternMethod =</span><br><span class="line">  logAggregationContext.getClass.getMethod(<span class="string">&quot;setRolledLogsIncludePattern&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">setRolledLogsIncludePatternMethod.invoke(logAggregationContext, includePattern)</span><br><span class="line"></span><br><span class="line">sparkConf.get(<span class="type">ROLLED_LOG_EXCLUDE_PATTERN</span>).foreach &#123; excludePattern =&gt;</span><br><span class="line">  <span class="keyword">val</span> setRolledLogsExcludePatternMethod =</span><br><span class="line">    logAggregationContext.getClass.getMethod(<span class="string">&quot;setRolledLogsExcludePattern&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">  setRolledLogsExcludePatternMethod.invoke(logAggregationContext, excludePattern)</span><br><span class="line">&#125;</span><br><span class="line">appContext.setLogAggregationContext(logAggregationContext)</span><br></pre></td></tr></table></figure>

<p>所以只要把源代码改动下就ok了。</p>
<p>Spark在<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/SPARK-25016">SPARK-25016</a>中移除了对Hadoop2.6的支持，代码改动也是原来那个时候。</p>
<p>可能新的代码在hadoop2.7中更加高效？如果仍然需要使用hadoop2.6的集群。就只能自己修改源码然后编译了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/" class="post-title-link" itemprop="url">spark-3.0 自适应查询</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-22 14:55:11 / 修改时间：21:28:20" itemprop="dateCreated datePublished" datetime="2020-06-22T14:55:11+08:00">2020-06-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark3/" itemprop="url" rel="index"><span itemprop="name">Spark3</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译"><a href="#Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译" class="headerlink" title="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译"></a>Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译</h1><p>原文链接：<a target="_blank" rel="noopener" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></p>
<blockquote>
<p>Over the years, there’s been an extensive and continuous effort to improve Spark SQL’s query optimizer and planner in order to generate high-quality query execution plans. One of the biggest improvements is the cost-based optimization framework that collects and leverages a variety of data statistics (e.g., row count, number of distinct values, NULL values, max/min values, etc.) to help Spark choose better plans. Examples of these cost-based optimization techniques include choosing the right join type (broadcast hash join vs. sort merge join), selecting the correct build side in a hash-join, or adjusting the join order in a multi-way join. However, outdated statistics and imperfect cardinality estimates can lead to suboptimal query plans. Adaptive Query Execution, new in the upcoming Apache SparkTM 3.0 release and available in the Databricks Runtime 7.0, now looks to tackle such issues by reoptimizing and adjusting query plans based on runtime statistics collected in the process of query execution.</p>
</blockquote>
<p>多年来，为了提供高质量的查询计划，Spark-SQL的query optimizer 和query planner进行了连续且大量的更新，其中最大的进步之一便是基于成本的优化框架，它收集并利用各种数据统计信息(例如，行计数、不同值的数量、空值、最大/最小值等)，来帮助Spark选择更好的计划。</p>
<p>这些基于成本的框架示例包括有：选择right join的类型（hash广播join vs 排序合并join），选择合适的hash-join端，调整join计划的链接顺序等。然而，过时的统计数据和不完美的基数估计可能导致不佳(次优)的查询计划。</p>
<p>自适应查询执行是即将发布的Apache SparkTM 3.0版本中的新增功能，该功能通过基于查询执行<strong>过程中收集的运行时统计信息</strong>，来优化并调整查询计划来解决这些问题。</p>
<h2 id="The-Adaptive-Query-Execution-AQE-framework"><a href="#The-Adaptive-Query-Execution-AQE-framework" class="headerlink" title="The Adaptive Query Execution (AQE) framework"></a>The Adaptive Query Execution (AQE) framework</h2><blockquote>
<p>One of the most important questions for Adaptive Query Execution is when to reoptimize. Spark operators are often pipelined and executed in parallel processes. However, a shuffle or broadcast exchange breaks this pipeline. We call them materialization points and use the term “query stages” to denote subsections bounded by these materialization points in a query. Each query stage materializes its intermediate result and the following stage can only proceed if all the parallel processes running the materialization have completed. This provides a natural opportunity for reoptimization, for it is when data statistics on all partitions are available and successive operations have not started yet.</p>
</blockquote>
<p>AQE中最重要的便是何时去重新优化，Spark任务通常是链式并行计算(pipelined and executed in parallel processes)，不过shuffle和广播会中断上述的链式计算，我们称之为物化点，后续文章中将使用“查询阶段”来表示查询中由这些物化点限定的子节。</p>
<p>每个查询阶段都需要物化其中间结果，只有在运行物化的所有并行进程都已完成时，下一个阶段才能继续。这个节点是天然的能够用来重新优化的节点，因为它有所有之前分区的统计数据，并且后续计划还未开始。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png" alt="优化逻辑"></p>
<blockquote>
<p>When the query starts, the Adaptive Query Execution framework first kicks off all the leaf stages — the stages that do not depend on any other stages. As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.</p>
</blockquote>
<p>当查询开始时，AQE框架会启动所有‘叶’阶段——不需要依赖其他阶段的阶段。一旦这些阶段完成了物化，框架就在物理查询计划中将它们标记为完成，并使用从<strong>完成阶段检索到的运行时统计信息</strong>，来相应地更新逻辑查询计划。</p>
<p>基于这些统计信息，框架会运行优化器(具有选定的逻辑优化规则列表)，物理规划器，物理优化规则。物理优化规则包括常规物理规则和特定于自适应执行的规则，例如合并分区、join时的数据倾斜处理等。</p>
<p>现在我们已经获得了一个新优化的查询计划，其中包含一些已完成的阶段。AQE框架将搜索并执行那些子阶段已经完成物化的新的查询阶段，然后重复上面的执行-重新优化-执行过程，直到完成整个查询。</p>
<blockquote>
<p>In Spark 3.0, the AQE framework is shipped with three features:</p>
<ul>
<li><p>Dynamically coalescing shuffle partitions</p>
</li>
<li><p>Dynamically switching join strategies</p>
</li>
<li><p>Dynamically optimizing skew joins</p>
</li>
</ul>
<p>The following sections will talk about these three features in detail.</p>
</blockquote>
<p>Spark3.0中，AQE附带了下面三个功能：</p>
<ul>
<li>动态合并shuffle分区。简化甚至避免调整 shuffle 分区的数量。</li>
<li>动态调整连接策略。部分避免了由于缺少统计信息或错误估计大小而导致执行次计划的情况</li>
<li>动态优化数据倾斜连接。解决数据倾斜的问题。</li>
</ul>
<p>接下来的章节会详细讨论这些功能：</p>
<h2 id="Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）"><a href="#Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）" class="headerlink" title="Dynamically coalescing shuffle partitions（动态合并shuffle分区）"></a>Dynamically coalescing shuffle partitions（动态合并shuffle分区）</h2><blockquote>
<p>When running queries in Spark to deal with very large data, shuffle usually has a very important impact on query performance among many other things. Shuffle is an expensive operator as it needs to move data across the network, so that data is redistributed in a way required by downstream operators.</p>
<p>One key property of shuffle is the number of partitions. The best number of partitions is data dependent, yet data sizes may differ vastly from stage to stage, query to query, making this number hard to tune:</p>
<ol>
<li>If there are too few partitions, then the data size of each partition may be very large, and the tasks to process these large partitions may need to spill data to disk (e.g., when sort or aggregate is involved) and, as a result, slow down the query.</li>
<li>If there are too many partitions, then the data size of each partition may be very small, and there will be a lot of small network data fetches to read the shuffle blocks, which can also slow down the query because of the inefficient I/O pattern. Having a large number of tasks also puts more burden on the Spark task scheduler.</li>
</ol>
</blockquote>
<p>使用Spark处理大数据时，shuffle是影响性能中最重要的一项。shuffle是一个昂贵的操作，因为需要经过网络传输数据，从而以后续算子需要的样子重新分配分区。</p>
<p>shuffle的关键属性之一是分区数量，分区的最佳数量取决于数据，但是不同阶段、不同查询的数据大小可能会有很大差异，这使得该数量很难调优。</p>
<ol>
<li>如果分区数量过小，单个分区中的数据会很大。处理这些大分区的任务时，可能需要将数据溢出到磁盘。(例如，当涉及排序或聚合时)，因此减慢了查询速度。</li>
<li>如果分区数量过多，单个分区中的数据会很小。这会导致将有大量小型网络数据提取来读取shuffle块，会因为低效的I/O模式而减慢查询速度。并且过多数量的任务也会给Spark任务调度带来更多负担。</li>
</ol>
<blockquote>
<p>To solve this problem, we can set a relatively large number of shuffle partitions at the beginning, then combine adjacent small partitions into bigger partitions at runtime by looking at the shuffle file statistics.</p>
<p>For example, let’s say we are running the query SELECT max(i)FROM tbl GROUP BY j. The input data tbl is rather small so there are only two partitions before grouping. The initial shuffle partition number is set to five, so after local grouping, the partially grouped data is shuffled into five partitions. Without AQE, Spark will start five tasks to do the final aggregation. However, there are three very small partitions here, and it would be a waste to start a separate task for each of them.</p>
<p>Instead, AQE coalesces these three small partitions into one and, as a result, the final aggregation now only needs to perform three tasks rather than five.</p>
</blockquote>
<p>为了解决这些问题。我们可以在任务开始时设置大数量的分区。然后在运行时通过查看统计数据，将相邻的小分区组合成更大的分区。</p>
<p>例如，当我们执行查询下述时：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">max</span>(i)<span class="keyword">FROM</span> tbl <span class="keyword">GROUP</span> <span class="keyword">BY</span> j</span><br></pre></td></tr></table></figure>

<p>输入数据tbl非常小，所以在grouping前只有2个分区。如果初始的shuffle分区数量设置为5，在local grouping后，部分分区的数据会被shuffle到5个分区中。没有AQE时，Spark会启动5个任务来执行最后的聚合计算。但是这里有三个非常小的分区，为每个分区启动单独的任务将是一种浪费：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140533.png" alt="Without AQE"></p>
<p>不过当启用AQE时，AQE会合并这三个小的分区，所以最后的聚合只需要3个任务，而不是5个。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140648.png" alt="enable AQE"></p>
<h2 id="Dynamically-switching-join-strategies（动态调整连接策略）"><a href="#Dynamically-switching-join-strategies（动态调整连接策略）" class="headerlink" title="Dynamically switching join strategies（动态调整连接策略）"></a>Dynamically switching join strategies（动态调整连接策略）</h2><blockquote>
<p>Spark supports a number of join strategies, among which broadcast hash join is usually the most performant if one side of the join can fit well in memory. And for this reason, Spark plans a broadcast hash join if the estimated size of a join relation is lower than the broadcast-size threshold. But a number of things can make this size estimation go wrong — such as the presence of a very selective filter — or the join relation being a series of complex operators other than just a scan.</p>
</blockquote>
<p>Spark支持多种连接策略，例如：如果join的一侧可以放入内存中，则广播hash-join(broadcast hash join)通常是性能最高的。基于上述原因，Spark会在join一侧的预估数据量小于广播阈值（broadcast-size threshold）时，Spark会采用broadcast hash join。不过，很多事情可能会导致这种大小估计出错——例如，存在一个非常有选择性的过滤器、或者连接侧存在一系列复杂的运算符，而不是只有一次扫描（scan）。</p>
<blockquote>
<p>简单来说，就是Spark估算Join两侧数据量大小不准确时导致的，AQE能够动态基于之前物化的数据来调整连接策略，也就减少了这种统计出错带来的影响。</p>
</blockquote>
<blockquote>
<p>To solve this problem, AQE now replans the join strategy at runtime based on the most accurate join relation size. As can be seen in the following example, the right side of the join is found to be way smaller than the estimate and also small enough to be broadcast, so after the AQE reoptimization the statically planned sort merge join is now converted to a broadcast hash join.</p>
<p>For the broadcast hash join converted at runtime, we may further optimize the regular shuffle to a localized shuffle (i.e., shuffle that reads on a per mapper basis instead of a per reducer basis) to reduce the network traffic.</p>
</blockquote>
<p>为了解决这个问题，AQE会根据最准确的连接关系大小，在运行时重新规划join策略。下面的示例中展示了这种情况。</p>
<p>参与join的右侧的数据的实际数据量(8M)，小于它的预估值(15M)。实际的数据量起始能够通过广播写入内存中，所以在AQE的重新调整后，计划从sort merge join调整为了 broadcast hash join。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622141835.png" alt="Dynamically switching join strategies"></p>
<p>对于运行时转换的broadcast hash join，我们能够进一步的将常规的shuffle调整为局部shuffle来减少网络传输（例如：基于mapper来读取shuffle，而不是基于reducer）</p>
<h2 id="Dynamically-optimizing-skew-joins"><a href="#Dynamically-optimizing-skew-joins" class="headerlink" title="Dynamically optimizing skew joins"></a>Dynamically optimizing skew joins</h2><blockquote>
<p>Data skew occurs when data is unevenly distributed among partitions in the cluster. Severe skew can significantly downgrade query performance, especially with joins. AQE skew join optimization detects such skew automatically from shuffle file statistics. It then splits the skewed partitions into smaller subpartitions, which will be joined to the corresponding partition from the other side respectively.</p>
</blockquote>
<p>当数据在集群中分布不均时就会出现数据倾斜（比如某个key的数据特别多时）。严重的偏斜可能会显著降低查询性能——特别是使用join时。AQE能够自动从shuffle 文件的统计中自动检测此类倾斜。之后，它会将倾斜的分区拆分成更小的子分区，这些子分区会连接上另一侧的与其匹配的分区。</p>
<blockquote>
<p>Let’s take this example of table A join table B, in which table A has a partition A0 significantly bigger than its other partitions.</p>
<p>The skew join optimization will thus split partition A0 into two subpartitions and join each of them to the corresponding partition B0 of table B.</p>
<p>Without this optimization, there would be four tasks running the sort merge join with one task taking a much longer time. After this optimization, there will be five tasks running the join, but each task will take roughly the same amount of time, resulting in an overall better performance.</p>
</blockquote>
<p>下述示例是TableA join TableB，其中，TableA中的分区A0明显大于其他分区。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143417.png" alt="优化前"></p>
<p>因此，AQE将分区A0拆分成两个子分区，并将它们中的每一个子分区，连接到表B的相应分区B0。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143433.png" alt="优化后"></p>
<p>如果没有优化器，这个Join会启动4个Task来执行，并且其中一个Task会耗费非常长的时间(A0对应的任务)。</p>
<p>在优化之后，这个Join会启动5个Task来执行，但每个任务都会耗费几乎一样的时间。最终的结果表现会更好。</p>
<h2 id="TPC-DS-performance-gains-from-AQE（性能提升）"><a href="#TPC-DS-performance-gains-from-AQE（性能提升）" class="headerlink" title="TPC-DS performance gains from AQE（性能提升）"></a>TPC-DS performance gains from AQE（性能提升）</h2><blockquote>
<p>In our experiments using TPC-DS data and queries, Adaptive Query Execution yielded up to an 8x speedup in query performance and 32 queries had more than 1.1x speedup Below is a chart of the 10 TPC-DS queries having the most performance improvement by AQE.</p>
<p>Most of these improvements have come from dynamic partition coalescing and dynamic join strategy switching since randomly generated TPC-DS data do not have skew. Yet we’ve seen even greater improvements in production workload in which all three features of AQE are leveraged.</p>
</blockquote>
<p>在我们使用TPC-DS数据和查询的实验中，AQE在查询性能方面获得了高达8倍的提升，32个查询的提升超过了1.1倍。下面是通过AQE获得最大性能提升的10个TPC-DS查询的图表。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143919.png" alt="性能提升"></p>
<p>这些改进大多来自动态分区合并（Dynamically coalescing shuffle partitions）和动态连接策略切换（Dynamically switching join strategies），因为随机生成的TPC-DS数据一般不存在数据倾斜的问题。</p>
<p>然而，我们已经看到，在利用了AQE的所有三个特性之后，生产工作得到了很大的改善。</p>
<h2 id="Enabling-AQE（启用AQE）"><a href="#Enabling-AQE（启用AQE）" class="headerlink" title="Enabling AQE（启用AQE）"></a>Enabling AQE（启用AQE）</h2><blockquote>
<p>AQE can be enabled by setting SQL config spark.sql.adaptive.enabled to true (default false in Spark 3.0), and applies if the query meets the following criteria:</p>
<ul>
<li>It is not a streaming query</li>
<li>It contains at least one exchange (usually when there’s a join, aggregate or window operator) or one subquery</li>
</ul>
</blockquote>
<p>通过将配置<code>spark.sql.adaptive.enabled</code>设置为<code>true</code>，AQE能在Spark3.0中启用，（在Spark3.0中默认是关闭的）。AQE有如下限制：</p>
<ul>
<li>不能是流式查询</li>
<li>至少需要一次数据交换（比如使用join、聚合、窗口函数时会出现）、或者存在一次子查询。</li>
</ul>
<blockquote>
<p>By making query optimization less dependent on static statistics, AQE has solved one of the greatest struggles of Spark cost-based optimization — the balance between the stats collection overhead and the estimation accuracy. To achieve the best estimation accuracy and planning outcome, it is usually required to maintain detailed, up-to-date statistics and some of them are expensive to collect, such as column histograms, which can be used to improve selectivity and cardinality estimation or to detect data skew. AQE has largely eliminated the need for such statistics as well as for the manual tuning effort. On top of that, AQE has also made SQL query optimization more resilient to the presence of arbitrary UDFs and unpredictable data set changes, e.g., sudden increase or decrease in data size, frequent and random data skew, etc. There’s no need to “know” your data in advance any more. AQE will figure out the data and improve the query plan as the query runs, increasing query performance for faster analytics and system performance.</p>
</blockquote>
<p>通过减少查询优化对静态统计数据的依赖，AQE解决了基于Spark Cost的优化的最大难题之一 — 统计数据的开销和评估精度之间的平衡。为了达到最佳的估计精度和规划结果，通常需要维护详细的最新统计数据，其中一些统计数据的收集成本很高，就例如柱状图，其可用于提高选择性、基数估计、检测数据倾斜。</p>
<p>AQE在很大程度上消除了对此类信息的统计，以及手动调优工作的需要。最重要的是，AQE还使SQL查询优化对任意UDF的不可预测的数据集更改具有更强的弹性，例如：数据大小的突然增加或减少、频繁和随机的数据歪斜等。</p>
<p>现在不再需要事先“了解”您的数据。AQE将在查询运行时找出数据并改进查询计划，从而提高查询性能以实现更快的分析和系统性能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/" class="post-title-link" itemprop="url">spark-3.0 动态分区裁剪</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-22 11:16:28 / 修改时间：21:28:19" itemprop="dateCreated datePublished" datetime="2020-06-22T11:16:28+08:00">2020-06-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark3/" itemprop="url" rel="index"><span itemprop="name">Spark3</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Spark中的静态分区裁剪"><a href="#Spark中的静态分区裁剪" class="headerlink" title="Spark中的静态分区裁剪"></a>Spark中的静态分区裁剪</h2><p>用过Spark的都知道，其实简单来说就是谓词下推。在Spark执行下述查询时，能够尽可能将谓词下推至扫描文件的阶段。从而减少读取的数据量，实现处理速度的提升：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Sales <span class="keyword">WHERE</span> day_of_week <span class="operator">=</span> ‘Mon’</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150635.png" alt="分区裁剪"></p>
<h2 id="Spark3-0中的动态分区裁剪"><a href="#Spark3-0中的动态分区裁剪" class="headerlink" title="Spark3.0中的动态分区裁剪"></a>Spark3.0中的动态分区裁剪</h2><p>所谓的<strong>动态分区裁剪</strong>就是基于运行时（run time）推断出来的信息来进一步进行分区裁剪。</p>
<p>在Join时，如果我们只需要一部分DIM Table中的数据，静态分区裁剪能够将这部分裁剪下推下去。</p>
<img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622150941.png" style="zoom:50%;" />

<p>动态分区裁剪则更进一步，会将这部分裁剪作用到FACT Table前，然后再进行Join。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622151112.png" alt="动态分区裁剪"></p>
<h2 id="开启动态分区裁剪"><a href="#开启动态分区裁剪" class="headerlink" title="开启动态分区裁剪"></a>开启动态分区裁剪</h2><p>开启动态分区裁剪：</p>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.enabled</code>设置为<code>true</code>(默认)</p>
<p>其他参数：</p>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.useStats</code>，<code>true</code>(默认)：</p>
<ul>
<li><p>在动态分区修剪后，将使用DISTINCT COUNT统计信息计算分区表的数据大小，以便在广播重用不适用的情况下，评估是否值得添加额外的子查询作为裁剪筛选器。</p>
</li>
<li><blockquote>
<p>When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p>
</blockquote>
</li>
</ul>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio</code>，<code>0.5</code>(默认)：</p>
<ul>
<li><p>当统计数据不可用或配置没有使用时，此配置将用作计算动态分区修剪后分区表数据大小的后备筛选器比率，以便在广播重用不适用的情况下评估是否值得添加额外的子查询作为裁剪筛选器。</p>
</li>
<li><blockquote>
<p>When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p>
</blockquote>
</li>
</ul>
<p><code>spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcast</code>，true(默认)：</p>
<ul>
<li><p>动态分区裁剪将寻求重用来自broadcast hash join操作的广播结果。</p>
</li>
<li><blockquote>
<p>When true, dynamic partition pruning will seek to reuse the broadcast results from a broadcast hash join operation.</p>
</blockquote>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/05/28/cdh-zhong-jin-yong-kerberos/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/28/cdh-zhong-jin-yong-kerberos/" class="post-title-link" itemprop="url">CDH中禁用kerberos</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-28 09:22:16 / 修改时间：10:13:32" itemprop="dateCreated datePublished" datetime="2020-05-28T09:22:16+08:00">2020-05-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CDH/" itemprop="url" rel="index"><span itemprop="name">CDH</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>免费版的CDH关闭kerberos会相对复杂。</p>
<ol>
<li>关闭所有组件。</li>
</ol>
<h2 id="一、操作zookeeper"><a href="#一、操作zookeeper" class="headerlink" title="一、操作zookeeper"></a>一、操作zookeeper</h2><ol>
<li>关闭zk的认证配置：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095701.png"></p>
<ol start="2">
<li>找到zk的数据目录：<br>[][1]</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528094904.png"></p>
<ol start="3">
<li>删除目录下的内容：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095524.png"></p>
<ol start="4">
<li>初始化zk：</li>
</ol>
<img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528095741.png" style="zoom:50%;" />

<ol start="5">
<li>确认初始化成功：</li>
</ol>
<h2 id="HDFS篇"><a href="#HDFS篇" class="headerlink" title="HDFS篇"></a>HDFS篇</h2><ol>
<li><p>关闭认证相关配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop.security.authentication</span><br><span class="line">hadoop.security.authorization</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100134.png"></p>
</li>
<li><p>修改权限及其他配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dfs.datanode.data.dir.perm =&gt; 755</span><br><span class="line">dfs.datanode.address =&gt; 50010</span><br><span class="line">dfs.datanode.http.address =&gt; 50070</span><br></pre></td></tr></table></figure>

<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528100400.png"></p>
</li>
</ol>
<h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><ol>
<li>搜索auth选项，关闭即可。一般来说还需要关HBase</li>
</ol>
<h2 id="重启、核查："><a href="#重启、核查：" class="headerlink" title="重启、核查："></a>重启、核查：</h2><p>重启，</p>
<ol>
<li>核查可发现kerberos已关闭：</li>
</ol>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101020.png"></p>
<ol start="2">
<li><p>核查hdfs上数据，发现都在：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200528101222.png"></p>
</li>
<li><p>核查zk数据，kafka数据，一切正常：</p>
<p>![image-20200528101328031](/Users/liushengwei/Library/Application Support/typora-user-images/image-20200528101328031.png)</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ssiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ssiu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false}});</script></body>
</html>
