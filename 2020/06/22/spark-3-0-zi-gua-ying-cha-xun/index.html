<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.ibuer.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译原文链接：Adaptive Query Execution: Speeding Up Spark SQL at Runtime  Over the years, there’s been an extensive and continuous effort to improve">
<meta property="og:type" content="article">
<meta property="og:title" content="spark-3.0 自适应查询">
<meta property="og:url" content="http://blog.ibuer.fun/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/index.html">
<meta property="og:site_name" content="Ssiu Blog">
<meta property="og:description" content="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译原文链接：Adaptive Query Execution: Speeding Up Spark SQL at Runtime  Over the years, there’s been an extensive and continuous effort to improve">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140533.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140648.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622141835.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143417.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143433.png">
<meta property="og:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143919.png">
<meta property="article:published_time" content="2020-06-22T06:55:11.000Z">
<meta property="article:modified_time" content="2020-06-22T13:28:20.909Z">
<meta property="article:author" content="Ssiu">
<meta property="article:tag" content="Spark3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png">

<link rel="canonical" href="http://blog.ibuer.fun/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>spark-3.0 自适应查询 | Ssiu Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ssiu Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.ibuer.fun/2020/06/22/spark-3-0-zi-gua-ying-cha-xun/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ssiu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ssiu Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark-3.0 自适应查询
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-22 14:55:11 / 修改时间：21:28:20" itemprop="dateCreated datePublished" datetime="2020-06-22T14:55:11+08:00">2020-06-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark3/" itemprop="url" rel="index"><span itemprop="name">Spark3</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译"><a href="#Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime翻译" class="headerlink" title="Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译"></a>Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译</h1><p>原文链接：<a target="_blank" rel="noopener" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></p>
<blockquote>
<p>Over the years, there’s been an extensive and continuous effort to improve Spark SQL’s query optimizer and planner in order to generate high-quality query execution plans. One of the biggest improvements is the cost-based optimization framework that collects and leverages a variety of data statistics (e.g., row count, number of distinct values, NULL values, max/min values, etc.) to help Spark choose better plans. Examples of these cost-based optimization techniques include choosing the right join type (broadcast hash join vs. sort merge join), selecting the correct build side in a hash-join, or adjusting the join order in a multi-way join. However, outdated statistics and imperfect cardinality estimates can lead to suboptimal query plans. Adaptive Query Execution, new in the upcoming Apache SparkTM 3.0 release and available in the Databricks Runtime 7.0, now looks to tackle such issues by reoptimizing and adjusting query plans based on runtime statistics collected in the process of query execution.</p>
</blockquote>
<p>多年来，为了提供高质量的查询计划，Spark-SQL的query optimizer 和query planner进行了连续且大量的更新，其中最大的进步之一便是基于成本的优化框架，它收集并利用各种数据统计信息(例如，行计数、不同值的数量、空值、最大/最小值等)，来帮助Spark选择更好的计划。</p>
<p>这些基于成本的框架示例包括有：选择right join的类型（hash广播join vs 排序合并join），选择合适的hash-join端，调整join计划的链接顺序等。然而，过时的统计数据和不完美的基数估计可能导致不佳(次优)的查询计划。</p>
<p>自适应查询执行是即将发布的Apache SparkTM 3.0版本中的新增功能，该功能通过基于查询执行<strong>过程中收集的运行时统计信息</strong>，来优化并调整查询计划来解决这些问题。</p>
<h2 id="The-Adaptive-Query-Execution-AQE-framework"><a href="#The-Adaptive-Query-Execution-AQE-framework" class="headerlink" title="The Adaptive Query Execution (AQE) framework"></a>The Adaptive Query Execution (AQE) framework</h2><blockquote>
<p>One of the most important questions for Adaptive Query Execution is when to reoptimize. Spark operators are often pipelined and executed in parallel processes. However, a shuffle or broadcast exchange breaks this pipeline. We call them materialization points and use the term “query stages” to denote subsections bounded by these materialization points in a query. Each query stage materializes its intermediate result and the following stage can only proceed if all the parallel processes running the materialization have completed. This provides a natural opportunity for reoptimization, for it is when data statistics on all partitions are available and successive operations have not started yet.</p>
</blockquote>
<p>AQE中最重要的便是何时去重新优化，Spark任务通常是链式并行计算(pipelined and executed in parallel processes)，不过shuffle和广播会中断上述的链式计算，我们称之为物化点，后续文章中将使用“查询阶段”来表示查询中由这些物化点限定的子节。</p>
<p>每个查询阶段都需要物化其中间结果，只有在运行物化的所有并行进程都已完成时，下一个阶段才能继续。这个节点是天然的能够用来重新优化的节点，因为它有所有之前分区的统计数据，并且后续计划还未开始。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622113507.png" alt="优化逻辑"></p>
<blockquote>
<p>When the query starts, the Adaptive Query Execution framework first kicks off all the leaf stages — the stages that do not depend on any other stages. As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.</p>
</blockquote>
<p>当查询开始时，AQE框架会启动所有‘叶’阶段——不需要依赖其他阶段的阶段。一旦这些阶段完成了物化，框架就在物理查询计划中将它们标记为完成，并使用从<strong>完成阶段检索到的运行时统计信息</strong>，来相应地更新逻辑查询计划。</p>
<p>基于这些统计信息，框架会运行优化器(具有选定的逻辑优化规则列表)，物理规划器，物理优化规则。物理优化规则包括常规物理规则和特定于自适应执行的规则，例如合并分区、join时的数据倾斜处理等。</p>
<p>现在我们已经获得了一个新优化的查询计划，其中包含一些已完成的阶段。AQE框架将搜索并执行那些子阶段已经完成物化的新的查询阶段，然后重复上面的执行-重新优化-执行过程，直到完成整个查询。</p>
<blockquote>
<p>In Spark 3.0, the AQE framework is shipped with three features:</p>
<ul>
<li><p>Dynamically coalescing shuffle partitions</p>
</li>
<li><p>Dynamically switching join strategies</p>
</li>
<li><p>Dynamically optimizing skew joins</p>
</li>
</ul>
<p>The following sections will talk about these three features in detail.</p>
</blockquote>
<p>Spark3.0中，AQE附带了下面三个功能：</p>
<ul>
<li>动态合并shuffle分区。简化甚至避免调整 shuffle 分区的数量。</li>
<li>动态调整连接策略。部分避免了由于缺少统计信息或错误估计大小而导致执行次计划的情况</li>
<li>动态优化数据倾斜连接。解决数据倾斜的问题。</li>
</ul>
<p>接下来的章节会详细讨论这些功能：</p>
<h2 id="Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）"><a href="#Dynamically-coalescing-shuffle-partitions（动态合并shuffle分区）" class="headerlink" title="Dynamically coalescing shuffle partitions（动态合并shuffle分区）"></a>Dynamically coalescing shuffle partitions（动态合并shuffle分区）</h2><blockquote>
<p>When running queries in Spark to deal with very large data, shuffle usually has a very important impact on query performance among many other things. Shuffle is an expensive operator as it needs to move data across the network, so that data is redistributed in a way required by downstream operators.</p>
<p>One key property of shuffle is the number of partitions. The best number of partitions is data dependent, yet data sizes may differ vastly from stage to stage, query to query, making this number hard to tune:</p>
<ol>
<li>If there are too few partitions, then the data size of each partition may be very large, and the tasks to process these large partitions may need to spill data to disk (e.g., when sort or aggregate is involved) and, as a result, slow down the query.</li>
<li>If there are too many partitions, then the data size of each partition may be very small, and there will be a lot of small network data fetches to read the shuffle blocks, which can also slow down the query because of the inefficient I/O pattern. Having a large number of tasks also puts more burden on the Spark task scheduler.</li>
</ol>
</blockquote>
<p>使用Spark处理大数据时，shuffle是影响性能中最重要的一项。shuffle是一个昂贵的操作，因为需要经过网络传输数据，从而以后续算子需要的样子重新分配分区。</p>
<p>shuffle的关键属性之一是分区数量，分区的最佳数量取决于数据，但是不同阶段、不同查询的数据大小可能会有很大差异，这使得该数量很难调优。</p>
<ol>
<li>如果分区数量过小，单个分区中的数据会很大。处理这些大分区的任务时，可能需要将数据溢出到磁盘。(例如，当涉及排序或聚合时)，因此减慢了查询速度。</li>
<li>如果分区数量过多，单个分区中的数据会很小。这会导致将有大量小型网络数据提取来读取shuffle块，会因为低效的I/O模式而减慢查询速度。并且过多数量的任务也会给Spark任务调度带来更多负担。</li>
</ol>
<blockquote>
<p>To solve this problem, we can set a relatively large number of shuffle partitions at the beginning, then combine adjacent small partitions into bigger partitions at runtime by looking at the shuffle file statistics.</p>
<p>For example, let’s say we are running the query SELECT max(i)FROM tbl GROUP BY j. The input data tbl is rather small so there are only two partitions before grouping. The initial shuffle partition number is set to five, so after local grouping, the partially grouped data is shuffled into five partitions. Without AQE, Spark will start five tasks to do the final aggregation. However, there are three very small partitions here, and it would be a waste to start a separate task for each of them.</p>
<p>Instead, AQE coalesces these three small partitions into one and, as a result, the final aggregation now only needs to perform three tasks rather than five.</p>
</blockquote>
<p>为了解决这些问题。我们可以在任务开始时设置大数量的分区。然后在运行时通过查看统计数据，将相邻的小分区组合成更大的分区。</p>
<p>例如，当我们执行查询下述时：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">max</span>(i)<span class="keyword">FROM</span> tbl <span class="keyword">GROUP</span> <span class="keyword">BY</span> j</span><br></pre></td></tr></table></figure>

<p>输入数据tbl非常小，所以在grouping前只有2个分区。如果初始的shuffle分区数量设置为5，在local grouping后，部分分区的数据会被shuffle到5个分区中。没有AQE时，Spark会启动5个任务来执行最后的聚合计算。但是这里有三个非常小的分区，为每个分区启动单独的任务将是一种浪费：</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140533.png" alt="Without AQE"></p>
<p>不过当启用AQE时，AQE会合并这三个小的分区，所以最后的聚合只需要3个任务，而不是5个。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622140648.png" alt="enable AQE"></p>
<h2 id="Dynamically-switching-join-strategies（动态调整连接策略）"><a href="#Dynamically-switching-join-strategies（动态调整连接策略）" class="headerlink" title="Dynamically switching join strategies（动态调整连接策略）"></a>Dynamically switching join strategies（动态调整连接策略）</h2><blockquote>
<p>Spark supports a number of join strategies, among which broadcast hash join is usually the most performant if one side of the join can fit well in memory. And for this reason, Spark plans a broadcast hash join if the estimated size of a join relation is lower than the broadcast-size threshold. But a number of things can make this size estimation go wrong — such as the presence of a very selective filter — or the join relation being a series of complex operators other than just a scan.</p>
</blockquote>
<p>Spark支持多种连接策略，例如：如果join的一侧可以放入内存中，则广播hash-join(broadcast hash join)通常是性能最高的。基于上述原因，Spark会在join一侧的预估数据量小于广播阈值（broadcast-size threshold）时，Spark会采用broadcast hash join。不过，很多事情可能会导致这种大小估计出错——例如，存在一个非常有选择性的过滤器、或者连接侧存在一系列复杂的运算符，而不是只有一次扫描（scan）。</p>
<blockquote>
<p>简单来说，就是Spark估算Join两侧数据量大小不准确时导致的，AQE能够动态基于之前物化的数据来调整连接策略，也就减少了这种统计出错带来的影响。</p>
</blockquote>
<blockquote>
<p>To solve this problem, AQE now replans the join strategy at runtime based on the most accurate join relation size. As can be seen in the following example, the right side of the join is found to be way smaller than the estimate and also small enough to be broadcast, so after the AQE reoptimization the statically planned sort merge join is now converted to a broadcast hash join.</p>
<p>For the broadcast hash join converted at runtime, we may further optimize the regular shuffle to a localized shuffle (i.e., shuffle that reads on a per mapper basis instead of a per reducer basis) to reduce the network traffic.</p>
</blockquote>
<p>为了解决这个问题，AQE会根据最准确的连接关系大小，在运行时重新规划join策略。下面的示例中展示了这种情况。</p>
<p>参与join的右侧的数据的实际数据量(8M)，小于它的预估值(15M)。实际的数据量起始能够通过广播写入内存中，所以在AQE的重新调整后，计划从sort merge join调整为了 broadcast hash join。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622141835.png" alt="Dynamically switching join strategies"></p>
<p>对于运行时转换的broadcast hash join，我们能够进一步的将常规的shuffle调整为局部shuffle来减少网络传输（例如：基于mapper来读取shuffle，而不是基于reducer）</p>
<h2 id="Dynamically-optimizing-skew-joins"><a href="#Dynamically-optimizing-skew-joins" class="headerlink" title="Dynamically optimizing skew joins"></a>Dynamically optimizing skew joins</h2><blockquote>
<p>Data skew occurs when data is unevenly distributed among partitions in the cluster. Severe skew can significantly downgrade query performance, especially with joins. AQE skew join optimization detects such skew automatically from shuffle file statistics. It then splits the skewed partitions into smaller subpartitions, which will be joined to the corresponding partition from the other side respectively.</p>
</blockquote>
<p>当数据在集群中分布不均时就会出现数据倾斜（比如某个key的数据特别多时）。严重的偏斜可能会显著降低查询性能——特别是使用join时。AQE能够自动从shuffle 文件的统计中自动检测此类倾斜。之后，它会将倾斜的分区拆分成更小的子分区，这些子分区会连接上另一侧的与其匹配的分区。</p>
<blockquote>
<p>Let’s take this example of table A join table B, in which table A has a partition A0 significantly bigger than its other partitions.</p>
<p>The skew join optimization will thus split partition A0 into two subpartitions and join each of them to the corresponding partition B0 of table B.</p>
<p>Without this optimization, there would be four tasks running the sort merge join with one task taking a much longer time. After this optimization, there will be five tasks running the join, but each task will take roughly the same amount of time, resulting in an overall better performance.</p>
</blockquote>
<p>下述示例是TableA join TableB，其中，TableA中的分区A0明显大于其他分区。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143417.png" alt="优化前"></p>
<p>因此，AQE将分区A0拆分成两个子分区，并将它们中的每一个子分区，连接到表B的相应分区B0。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143433.png" alt="优化后"></p>
<p>如果没有优化器，这个Join会启动4个Task来执行，并且其中一个Task会耗费非常长的时间(A0对应的任务)。</p>
<p>在优化之后，这个Join会启动5个Task来执行，但每个任务都会耗费几乎一样的时间。最终的结果表现会更好。</p>
<h2 id="TPC-DS-performance-gains-from-AQE（性能提升）"><a href="#TPC-DS-performance-gains-from-AQE（性能提升）" class="headerlink" title="TPC-DS performance gains from AQE（性能提升）"></a>TPC-DS performance gains from AQE（性能提升）</h2><blockquote>
<p>In our experiments using TPC-DS data and queries, Adaptive Query Execution yielded up to an 8x speedup in query performance and 32 queries had more than 1.1x speedup Below is a chart of the 10 TPC-DS queries having the most performance improvement by AQE.</p>
<p>Most of these improvements have come from dynamic partition coalescing and dynamic join strategy switching since randomly generated TPC-DS data do not have skew. Yet we’ve seen even greater improvements in production workload in which all three features of AQE are leveraged.</p>
</blockquote>
<p>在我们使用TPC-DS数据和查询的实验中，AQE在查询性能方面获得了高达8倍的提升，32个查询的提升超过了1.1倍。下面是通过AQE获得最大性能提升的10个TPC-DS查询的图表。</p>
<p><img src="https://my-blog-1259467560.cos.ap-chengdu.myqcloud.com/blog/20200622143919.png" alt="性能提升"></p>
<p>这些改进大多来自动态分区合并（Dynamically coalescing shuffle partitions）和动态连接策略切换（Dynamically switching join strategies），因为随机生成的TPC-DS数据一般不存在数据倾斜的问题。</p>
<p>然而，我们已经看到，在利用了AQE的所有三个特性之后，生产工作得到了很大的改善。</p>
<h2 id="Enabling-AQE（启用AQE）"><a href="#Enabling-AQE（启用AQE）" class="headerlink" title="Enabling AQE（启用AQE）"></a>Enabling AQE（启用AQE）</h2><blockquote>
<p>AQE can be enabled by setting SQL config spark.sql.adaptive.enabled to true (default false in Spark 3.0), and applies if the query meets the following criteria:</p>
<ul>
<li>It is not a streaming query</li>
<li>It contains at least one exchange (usually when there’s a join, aggregate or window operator) or one subquery</li>
</ul>
</blockquote>
<p>通过将配置<code>spark.sql.adaptive.enabled</code>设置为<code>true</code>，AQE能在Spark3.0中启用，（在Spark3.0中默认是关闭的）。AQE有如下限制：</p>
<ul>
<li>不能是流式查询</li>
<li>至少需要一次数据交换（比如使用join、聚合、窗口函数时会出现）、或者存在一次子查询。</li>
</ul>
<blockquote>
<p>By making query optimization less dependent on static statistics, AQE has solved one of the greatest struggles of Spark cost-based optimization — the balance between the stats collection overhead and the estimation accuracy. To achieve the best estimation accuracy and planning outcome, it is usually required to maintain detailed, up-to-date statistics and some of them are expensive to collect, such as column histograms, which can be used to improve selectivity and cardinality estimation or to detect data skew. AQE has largely eliminated the need for such statistics as well as for the manual tuning effort. On top of that, AQE has also made SQL query optimization more resilient to the presence of arbitrary UDFs and unpredictable data set changes, e.g., sudden increase or decrease in data size, frequent and random data skew, etc. There’s no need to “know” your data in advance any more. AQE will figure out the data and improve the query plan as the query runs, increasing query performance for faster analytics and system performance.</p>
</blockquote>
<p>通过减少查询优化对静态统计数据的依赖，AQE解决了基于Spark Cost的优化的最大难题之一 — 统计数据的开销和评估精度之间的平衡。为了达到最佳的估计精度和规划结果，通常需要维护详细的最新统计数据，其中一些统计数据的收集成本很高，就例如柱状图，其可用于提高选择性、基数估计、检测数据倾斜。</p>
<p>AQE在很大程度上消除了对此类信息的统计，以及手动调优工作的需要。最重要的是，AQE还使SQL查询优化对任意UDF的不可预测的数据集更改具有更强的弹性，例如：数据大小的突然增加或减少、频繁和随机的数据歪斜等。</p>
<p>现在不再需要事先“了解”您的数据。AQE将在查询运行时找出数据并改进查询计划，从而提高查询性能以实现更快的分析和系统性能。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark3/" rel="tag"># Spark3</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/22/spark-3-0-dong-tai-fen-qu-cai-jian/" rel="prev" title="spark-3.0 动态分区裁剪">
      <i class="fa fa-chevron-left"></i> spark-3.0 动态分区裁剪
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/22/spark-3-0-hadoop2-6-cdh-bian-yi-de-wen-ti/" rel="next" title="spark-3.0 Hadoop2.6-CDH编译的问题">
      spark-3.0 Hadoop2.6-CDH编译的问题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Adaptive-Query-Execution-Speeding-Up-Spark-SQL-at-Runtime%E7%BF%BB%E8%AF%91"><span class="nav-number">1.</span> <span class="nav-text">Adaptive Query Execution: Speeding Up Spark SQL at Runtime翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Adaptive-Query-Execution-AQE-framework"><span class="nav-number">1.1.</span> <span class="nav-text">The Adaptive Query Execution (AQE) framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dynamically-coalescing-shuffle-partitions%EF%BC%88%E5%8A%A8%E6%80%81%E5%90%88%E5%B9%B6shuffle%E5%88%86%E5%8C%BA%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">Dynamically coalescing shuffle partitions（动态合并shuffle分区）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dynamically-switching-join-strategies%EF%BC%88%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E8%BF%9E%E6%8E%A5%E7%AD%96%E7%95%A5%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">Dynamically switching join strategies（动态调整连接策略）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dynamically-optimizing-skew-joins"><span class="nav-number">1.4.</span> <span class="nav-text">Dynamically optimizing skew joins</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TPC-DS-performance-gains-from-AQE%EF%BC%88%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%EF%BC%89"><span class="nav-number">1.5.</span> <span class="nav-text">TPC-DS performance gains from AQE（性能提升）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Enabling-AQE%EF%BC%88%E5%90%AF%E7%94%A8AQE%EF%BC%89"><span class="nav-number">1.6.</span> <span class="nav-text">Enabling AQE（启用AQE）</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ssiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ssiu</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false}});</script></body>
</html>
